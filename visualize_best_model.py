#!/usr/bin/env python3
"""
å¯è§†åŒ–best.pthæ¨¡å‹çš„é¢„æµ‹ç»“æœ

ä½¿ç”¨å·¥ä½œç©ºé—´ç°æœ‰çš„PDEBenchVisualizeråˆ›å»ºå…¨é¢çš„å¯è§†åŒ–ï¼š
- æ¨¡å‹é¢„æµ‹ç»“æœå¯¹æ¯”ï¼ˆGT vs Pred vs Errorï¼‰
- å¤šä¸ªæµ‹è¯•æ ·æœ¬çš„å¯è§†åŒ–å±•ç¤º
- é¢„æµ‹è´¨é‡åˆ†æå›¾
- åŠŸç‡è°±åˆ†æ
- è¯¯å·®åˆ†å¸ƒç»Ÿè®¡å›¾
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from omegaconf import OmegaConf
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from utils.visualization import PDEBenchVisualizer
from models.swin_unet import SwinUNet
from datasets.pdebench import PDEBenchDataModule
from ops.degradation import apply_degradation_operator
from ops.metrics import compute_all_metrics

def load_model_and_config(checkpoint_path):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
    print(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    config = checkpoint['config']
    
    # åˆ›å»ºæ¨¡å‹
    model_config = config['model']
    model = SwinUNet(
        in_channels=model_config['params']['in_channels'],
        out_channels=model_config['params']['out_channels'],
        img_size=model_config['params']['img_size'],
        **{k: v for k, v in model_config['params'].items() 
           if k not in ['in_channels', 'out_channels', 'img_size']}
    )
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ - Epoch: {checkpoint['epoch']}")
    print(f"âœ“ æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']:.6f}")
    
    return model, config, checkpoint

def create_data_loader(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = PDEBenchDataModule(config['data'])
    data_module.setup()
    
    # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_loader = data_module.test_dataloader()
    
    print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ - æµ‹è¯•æ ·æœ¬æ•°: {len(test_loader.dataset)}")
    
    return test_loader, data_module

def generate_predictions(model, test_loader, num_samples=5):
    """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
    print(f"ğŸ”„ ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ...")
    
    results = []
    device = next(model.parameters()).device
    
    with torch.no_grad():
        for i, batch in enumerate(test_loader):
            if i >= num_samples:
                break
                
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            input_data = batch['observation'].to(device)  # ä½¿ç”¨observationä½œä¸ºè¾“å…¥ï¼ˆä¸Šé‡‡æ ·åçš„ï¼‰
            target = batch['target'].to(device)
            
            # è·å–çœŸå®çš„ä½åˆ†è¾¨ç‡è§‚æµ‹æ•°æ®
            if 'lr_observation' in batch:
                lr_observation = batch['lr_observation']  # çœŸå®çš„32x32è§‚æµ‹æ•°æ®
            elif 'original_observation' in batch:
                lr_observation = batch['original_observation']  # çœŸå®çš„è§‚æµ‹æ•°æ®
            else:
                # å¦‚æœæ²¡æœ‰åŸå§‹è§‚æµ‹æ•°æ®ï¼Œä½¿ç”¨observation
                lr_observation = batch['observation']
            
            # æ¨¡å‹é¢„æµ‹
            pred = model(input_data)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = compute_all_metrics(pred, target)
            
            # è½¬æ¢ä¸ºnumpyç”¨äºå¯è§†åŒ–
            lr_observation_np = lr_observation.cpu().numpy()  # çœŸå®çš„ä½åˆ†è¾¨ç‡è§‚æµ‹
            input_np = input_data.cpu().numpy()  # ä¸Šé‡‡æ ·åçš„è§‚æµ‹ï¼ˆç”¨äºæ¨¡å‹è¾“å…¥ï¼‰
            target_np = target.cpu().numpy()
            pred_np = pred.cpu().numpy()
            
            results.append({
                'lr_observation': lr_observation_np,  # çœŸå®çš„32x32è§‚æµ‹æ•°æ®
                'observation': input_np,  # ä¸Šé‡‡æ ·åçš„è§‚æµ‹æ•°æ®
                'target': target_np,
                'prediction': pred_np,
                'metrics': metrics,
                'sample_id': i + 1
            })
            
            rel_l2_value = metrics['rel_l2'].mean() if hasattr(metrics['rel_l2'], 'mean') else metrics['rel_l2']
            print(f"  æ ·æœ¬ {i+1}: Rel-L2 = {rel_l2_value:.4f}")
    
    print(f"âœ“ é¢„æµ‹ç»“æœç”Ÿæˆå®Œæˆ")
    return results

def create_comprehensive_visualizations(results, config, checkpoint, output_dir):
    """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–"""
    print("ğŸ”„ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–å¯è§†åŒ–å™¨
    visualizer = PDEBenchVisualizer(str(output_path))
    
    # 1. ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºçœŸå®è§‚æµ‹ vs GT vs Pred vs Errorå¯¹æ¯”å›¾
    print("  ğŸ“Š åˆ›å»ºé¢„æµ‹ç»“æœå¯¹æ¯”å›¾ï¼ˆä½¿ç”¨çœŸå®32x32è§‚æµ‹æ•°æ®ï¼‰...")
    for result in results:
        sample_id = result['sample_id']
        
        # å–ç¬¬ä¸€ä¸ªbatchå’Œç¬¬ä¸€ä¸ªé€šé“
        gt = result['target'][0, 0]  # [H, W] - 128x128çœŸå€¼
        pred = result['prediction'][0, 0]  # [H, W] - 128x128é¢„æµ‹
        lr_observed = result['lr_observation'][0, 0]  # çœŸå®çš„32x32è§‚æµ‹æ•°æ®
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"DEBUG: gt shape: {gt.shape}")
        print(f"DEBUG: pred shape: {pred.shape}")
        print(f"DEBUG: lr_observed shape: {lr_observed.shape}")
        
        # åˆ›å»ºå››è”å›¾å¯¹æ¯”ï¼š32x32è§‚æµ‹ vs 128x128çœŸå€¼ vs 128x128é¢„æµ‹ vs 128x128è¯¯å·®
        rel_l2_value = result['metrics']['rel_l2'].mean() if hasattr(result['metrics']['rel_l2'], 'mean') else result['metrics']['rel_l2']
        visualizer.create_quadruplet_visualization(
            observed=torch.tensor(lr_observed),  # ä½¿ç”¨çœŸå®çš„32x32è§‚æµ‹æ•°æ®
            gt=torch.tensor(gt),
            pred=torch.tensor(pred),
            save_name=f"true_observation_comparison_sample_{sample_id}",
            title=f"Sample {sample_id} - True 32x32 Observation - Rel-L2: {rel_l2_value:.4f}"
        )
    
    # 2. åˆ›å»ºåŠŸç‡è°±åˆ†æå›¾
    print("  ğŸ“Š åˆ›å»ºåŠŸç‡è°±åˆ†æå›¾...")
    for result in results:
        sample_id = result['sample_id']
        gt = result['target'][0, 0]
        pred = result['prediction'][0, 0]
        
        # GTåŠŸç‡è°±
        visualizer.create_power_spectrum_plot(
            field=torch.tensor(gt),
            save_name=f"power_spectrum_gt_sample_{sample_id}"
        )
        
        # é¢„æµ‹åŠŸç‡è°±
        visualizer.create_power_spectrum_plot(
            field=torch.tensor(pred),
            save_name=f"power_spectrum_pred_sample_{sample_id}"
        )
    
    # 3. åˆ›å»ºåŠŸç‡è°±å¯¹æ¯”å›¾ï¼ˆè·³è¿‡ï¼Œå› ä¸ºæ–¹æ³•ä¸å­˜åœ¨ï¼‰
    print("  ğŸ“Š è·³è¿‡åŠŸç‡è°±å¯¹æ¯”å›¾ï¼ˆæ–¹æ³•ä¸å­˜åœ¨ï¼‰...")
    
    # 4. è·³è¿‡è¾¹ç•Œæ•ˆåº”åˆ†æï¼ˆæ–¹æ³•ä¸å­˜åœ¨ï¼‰
    print("  ğŸ“Š è·³è¿‡è¾¹ç•Œæ•ˆåº”åˆ†æï¼ˆæ–¹æ³•ä¸å­˜åœ¨ï¼‰...")
    
    # 5. åˆ›å»ºæŒ‡æ ‡æ±‡æ€»å›¾
    print("  ğŸ“Š åˆ›å»ºæŒ‡æ ‡æ±‡æ€»å›¾...")
    all_metrics = {}
    for result in results:
        for metric, value in result['metrics'].items():
            if metric not in all_metrics:
                all_metrics[metric] = []
            all_metrics[metric].append(value.mean() if hasattr(value, 'mean') else value)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªä¿ç•™å‡å€¼ï¼‰
    stats = {}
    for metric, values in all_metrics.items():
        stats[metric] = np.mean(values)
    
    visualizer.create_metrics_summary_plot(
        {'Model': stats},
        save_name="metrics_summary"
    )
    
    # 6. åˆ›å»ºè¾¹ç•Œåˆ†æå›¾
    print("  ğŸ“Š åˆ›å»ºè¾¹ç•Œåˆ†æå›¾...")
    for result in results:
        sample_id = result['sample_id']
        gt = result['target'][0, 0]
        pred = result['prediction'][0, 0]
        
        visualizer.create_boundary_analysis(
            gt=torch.tensor(gt),
            pred=torch.tensor(pred),
            boundary_width=16,
            save_name=f"boundary_analysis_sample_{sample_id}"
        )
    
    # 7. ä¿å­˜è¯¦ç»†ç»“æœ
    print("  ğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœ...")
    results_summary = {
        'model_info': {
            'epoch': checkpoint['epoch'],
            'best_val_loss': checkpoint['best_val_loss'],
            'config': config
        },
        'metrics_summary': {
            key: {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            }
            for key, values in all_metrics.items()
        },
        'sample_metrics': [
            {
                'sample_id': result['sample_id'],
                'metrics': {k: v.mean().item() if isinstance(v, torch.Tensor) else v 
                          for k, v in result['metrics'].items()}
            }
            for result in results
        ]
    }
    
    # ä¿å­˜JSONç»“æœ
    with open(output_path / "results_summary.json", 'w') as f:
        import json
        json.dump(results_summary, f, indent=2, default=str)
    
    print(f"âœ“ å¯è§†åŒ–å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_path}")
    return output_path

def main():
    """ä¸»å‡½æ•°"""
    checkpoint_path = "f:/Zhaoyang/Sparse2Full/runs/checkpoints/best.pth"
    output_dir = "f:/Zhaoyang/Sparse2Full/runs/true_observation_visualization"
    
    print("ğŸš€ å¼€å§‹å¯è§†åŒ–best.pthæ¨¡å‹...")
    
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œé…ç½®
        model, config, checkpoint = load_model_and_config(checkpoint_path)
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader, data_module = create_data_loader(config)
        
        # 3. ç”Ÿæˆé¢„æµ‹ç»“æœ
        results = generate_predictions(model, test_loader, num_samples=5)
        
        # 4. åˆ›å»ºå¯è§†åŒ–
        output_path = create_comprehensive_visualizations(
            results, config, checkpoint, output_dir
        )
        
        print("\nğŸ‰ å¯è§†åŒ–å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
        print("\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
        for file_path in sorted(output_path.glob("*.png")):
            print(f"  - {file_path.name}")
        
        # æ˜¾ç¤ºæŒ‡æ ‡æ±‡æ€»
        print("\nğŸ“ˆ æŒ‡æ ‡æ±‡æ€»:")
        with open(output_path / "results_summary.json", 'r') as f:
            import json
            summary = json.load(f)
            
        for metric, value in summary['metrics_summary'].items():
            if isinstance(value, (int, float)):
                print(f"  {metric}: {value:.4f}")
            else:
                print(f"  {metric}: {value}")
            
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()