#!/usr/bin/env python3
"""
éªŒè¯train.yamlé…ç½®æ–‡ä»¶ä¸­çš„è®­ç»ƒæ•°æ®æ˜¯å¦ä¸ºçœŸå®çš„PDEBenchæ•°æ®
"""

import h5py
import numpy as np
import os
import yaml
from pathlib import Path

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def check_hdf5_structure(file_path):
    """æ£€æŸ¥HDF5æ–‡ä»¶ç»“æ„å’Œå†…å®¹"""
    print(f"\n=== æ£€æŸ¥HDF5æ–‡ä»¶ç»“æ„ ===")
    print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
    
    try:
        with h5py.File(file_path, 'r') as f:
            print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / (1024**3):.2f} GB")
            
            # æ‰“å°æ‰€æœ‰é”®
            print(f"\næ–‡ä»¶ä¸­çš„é”®: {list(f.keys())}")
            
            # æ£€æŸ¥tensoræ•°æ®
            if 'tensor' in f:
                tensor_data = f['tensor']
                print(f"\ntensoræ•°æ®ä¿¡æ¯:")
                print(f"  å½¢çŠ¶: {tensor_data.shape}")
                print(f"  æ•°æ®ç±»å‹: {tensor_data.dtype}")
                print(f"  æœ€å°å€¼: {np.min(tensor_data[:]):.6f}")
                print(f"  æœ€å¤§å€¼: {np.max(tensor_data[:]):.6f}")
                print(f"  å‡å€¼: {np.mean(tensor_data[:]):.6f}")
                print(f"  æ ‡å‡†å·®: {np.std(tensor_data[:]):.6f}")
                
                # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬
                print(f"\nå‰3ä¸ªæ ·æœ¬çš„å½¢çŠ¶å’Œæ•°å€¼èŒƒå›´:")
                for i in range(min(3, tensor_data.shape[0])):
                    sample = tensor_data[i]
                    print(f"  æ ·æœ¬ {i}: å½¢çŠ¶={sample.shape}, èŒƒå›´=[{np.min(sample):.6f}, {np.max(sample):.6f}]")
            
            # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„é”®
            for key in ['nu', 'x', 'y', 'coords']:
                if key in f:
                    data = f[key]
                    print(f"\n{key}æ•°æ®ä¿¡æ¯:")
                    print(f"  å½¢çŠ¶: {data.shape}")
                    print(f"  æ•°æ®ç±»å‹: {data.dtype}")
                    if data.size < 100:  # åªæ‰“å°å°æ•°ç»„çš„å€¼
                        print(f"  å€¼: {data[:]}")
                    else:
                        print(f"  èŒƒå›´: [{np.min(data[:]):.6f}, {np.max(data[:]):.6f}]")
                        
    except Exception as e:
        print(f"è¯»å–HDF5æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False
    
    return True

def validate_pdebench_data(file_path, config):
    """éªŒè¯æ˜¯å¦ä¸ºçœŸå®çš„PDEBench DarcyFlowæ•°æ®"""
    print(f"\n=== éªŒè¯PDEBenchæ•°æ®çœŸå®æ€§ ===")
    
    try:
        with h5py.File(file_path, 'r') as f:
            # æ£€æŸ¥æ•°æ®é›†åç§°
            dataset_name = config['data']['dataset_name']
            print(f"é…ç½®ä¸­çš„æ•°æ®é›†åç§°: {dataset_name}")
            
            # æ£€æŸ¥æ•°æ®é”®
            expected_keys = config['data']['keys']
            print(f"é…ç½®ä¸­çš„æ•°æ®é”®: {expected_keys}")
            
            # éªŒè¯æ–‡ä»¶åæ˜¯å¦ç¬¦åˆPDEBenchå‘½åè§„èŒƒ
            filename = os.path.basename(file_path)
            print(f"æ–‡ä»¶å: {filename}")
            
            if "DarcyFlow" in filename and "beta1.0" in filename:
                print("âœ“ æ–‡ä»¶åç¬¦åˆPDEBench DarcyFlowå®˜æ–¹å‘½åè§„èŒƒ")
            else:
                print("âš  æ–‡ä»¶åä¸ç¬¦åˆPDEBenchæ ‡å‡†å‘½åè§„èŒƒ")
            
            # æ£€æŸ¥æ•°æ®ç»´åº¦æ˜¯å¦åˆç†
            if 'tensor' in f:
                tensor_shape = f['tensor'].shape
                print(f"æ•°æ®å½¢çŠ¶: {tensor_shape}")
                
                # PDEBench DarcyFlowé€šå¸¸æ˜¯4D: (samples, channels, height, width)
                if len(tensor_shape) == 4:
                    samples, channels, height, width = tensor_shape
                    print(f"âœ“ æ•°æ®ç»´åº¦æ­£ç¡®: {samples}æ ·æœ¬, {channels}é€šé“, {height}x{width}åˆ†è¾¨ç‡")
                    
                    # æ£€æŸ¥åˆ†è¾¨ç‡æ˜¯å¦åˆç†
                    if height == width and height in [32, 64, 128, 256, 512]:
                        print(f"âœ“ ç©ºé—´åˆ†è¾¨ç‡åˆç†: {height}x{width}")
                    else:
                        print(f"âš  ç©ºé—´åˆ†è¾¨ç‡å¼‚å¸¸: {height}x{width}")
                        
                    # æ£€æŸ¥æ ·æœ¬æ•°é‡æ˜¯å¦åˆç†
                    if samples >= 100:
                        print(f"âœ“ æ ·æœ¬æ•°é‡åˆç†: {samples}")
                    else:
                        print(f"âš  æ ·æœ¬æ•°é‡è¾ƒå°‘: {samples}")
                        
                else:
                    print(f"âš  æ•°æ®ç»´åº¦å¼‚å¸¸: {len(tensor_shape)}D")
            
            # æ£€æŸ¥ç‰©ç†åœºæ•°å€¼èŒƒå›´
            if 'tensor' in f:
                data = f['tensor'][:]
                data_min, data_max = np.min(data), np.max(data)
                data_mean, data_std = np.mean(data), np.std(data)
                
                print(f"\nç‰©ç†åœºæ•°å€¼ç‰¹å¾:")
                print(f"  æ•°å€¼èŒƒå›´: [{data_min:.6f}, {data_max:.6f}]")
                print(f"  å‡å€¼: {data_mean:.6f}")
                print(f"  æ ‡å‡†å·®: {data_std:.6f}")
                
                # DarcyFlowç‰©ç†åœºé€šå¸¸åœ¨åˆç†èŒƒå›´å†…
                if -10 <= data_min and data_max <= 10:
                    print("âœ“ æ•°å€¼èŒƒå›´ç¬¦åˆDarcyFlowç‰©ç†åœºç‰¹å¾")
                else:
                    print("âš  æ•°å€¼èŒƒå›´å¯èƒ½å¼‚å¸¸")
                    
                # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
                if 0.01 <= data_std <= 10:
                    print("âœ“ æ•°æ®æ ‡å‡†å·®åˆç†ï¼Œå…·æœ‰ç‰©ç†å˜åŒ–")
                else:
                    print("âš  æ•°æ®æ ‡å‡†å·®å¼‚å¸¸")
                    
    except Exception as e:
        print(f"éªŒè¯æ•°æ®æ—¶å‡ºé”™: {e}")
        return False
        
    return True

def check_config_consistency(config):
    """æ£€æŸ¥é…ç½®æ–‡ä»¶çš„ä¸€è‡´æ€§"""
    print(f"\n=== æ£€æŸ¥é…ç½®ä¸€è‡´æ€§ ===")
    
    # æ£€æŸ¥æ•°æ®é…ç½®
    data_config = config['data']
    print(f"æ•°æ®é›†åç§°: {data_config['dataset_name']}")
    print(f"æ•°æ®é”®: {data_config['keys']}")
    print(f"å›¾åƒå°ºå¯¸: {data_config['image_size']}")
    
    # æ£€æŸ¥è§‚æµ‹é…ç½®
    obs_config = data_config['observation']
    print(f"\nè§‚æµ‹æ¨¡å¼: {obs_config['mode']}")
    
    if obs_config['mode'] == 'SR':
        sr_config = obs_config['sr']
        print(f"è¶…åˆ†è¾¨ç‡é…ç½®:")
        print(f"  ç¼©æ”¾å› å­: {sr_config['scale_factor']}")
        print(f"  æ¨¡ç³Šå‚æ•°: Ïƒ={sr_config['blur_sigma']}, kernel_size={sr_config['blur_kernel_size']}")
        print(f"  è¾¹ç•Œæ¨¡å¼: {sr_config['boundary_mode']}")
        
        # éªŒè¯é…ç½®åˆç†æ€§
        if sr_config['scale_factor'] in [2, 4, 8]:
            print("âœ“ ç¼©æ”¾å› å­åˆç†")
        else:
            print("âš  ç¼©æ”¾å› å­å¼‚å¸¸")
            
    # æ£€æŸ¥é¢„å¤„ç†é…ç½®
    preprocess_config = data_config['preprocessing']
    print(f"\né¢„å¤„ç†é…ç½®:")
    print(f"  æ ‡å‡†åŒ–: {preprocess_config['normalize']}")
    print(f"  ç¼“å­˜æ•°æ®: {preprocess_config['cache_data']}")
    
    # æ£€æŸ¥æ‰¹æ¬¡å¤§å°
    batch_size = data_config['dataloader']['batch_size']
    print(f"\næ‰¹æ¬¡å¤§å°: {batch_size}")
    if 1 <= batch_size <= 32:
        print("âœ“ æ‰¹æ¬¡å¤§å°åˆç†")
    else:
        print("âš  æ‰¹æ¬¡å¤§å°å¯èƒ½ä¸åˆé€‚")
        
    return True

def generate_verification_report(config_path, data_path):
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    print("=" * 60)
    print("PDEBenchè®­ç»ƒæ•°æ®éªŒè¯æŠ¥å‘Š")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # éªŒè¯æ•°æ®è·¯å¾„
    data_path_from_config = config['data']['data_path']
    print(f"é…ç½®æ–‡ä»¶è·¯å¾„: {config_path}")
    print(f"é…ç½®ä¸­çš„æ•°æ®è·¯å¾„: {data_path_from_config}")
    print(f"å®é™…æ£€æŸ¥è·¯å¾„: {data_path}")
    
    if data_path_from_config == data_path:
        print("âœ“ æ•°æ®è·¯å¾„ä¸€è‡´")
    else:
        print("âš  æ•°æ®è·¯å¾„ä¸ä¸€è‡´")
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if os.path.exists(data_path):
        print("âœ“ æ•°æ®æ–‡ä»¶å­˜åœ¨")
    else:
        print("âœ— æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # æ£€æŸ¥HDF5ç»“æ„
    hdf5_ok = check_hdf5_structure(data_path)
    
    # éªŒè¯PDEæ•°æ®
    pde_ok = validate_pdebench_data(data_path, config)
    
    # æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
    config_ok = check_config_consistency(config)
    
    # æ€»ç»“
    print(f"\n=== éªŒè¯æ€»ç»“ ===")
    print(f"HDF5æ–‡ä»¶ç»“æ„: {'âœ“ æ­£å¸¸' if hdf5_ok else 'âœ— å¼‚å¸¸'}")
    print(f"PDEæ•°æ®éªŒè¯: {'âœ“ é€šè¿‡' if pde_ok else 'âœ— å¤±è´¥'}")
    print(f"é…ç½®ä¸€è‡´æ€§: {'âœ“ æ­£å¸¸' if config_ok else 'âœ— å¼‚å¸¸'}")
    
    if hdf5_ok and pde_ok and config_ok:
        print("\nğŸ‰ ç»“è®º: è®­ç»ƒæ•°æ®ç¡®å®æ˜¯çœŸå®çš„PDEBench DarcyFlowæ•°æ®é›†!")
        return True
    else:
        print("\nâš ï¸  ç»“è®º: è®­ç»ƒæ•°æ®å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        return False

def main():
    """ä¸»å‡½æ•°"""
    config_path = "f:/Zhaoyang/Sparse2Full/configs/train.yaml"
    data_path = "E:/2D/DarcyFlow/2D_DarcyFlow_beta1.0_Train.hdf5"
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    result = generate_verification_report(config_path, data_path)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = "runs/train_data_verification_report.md"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# PDEBenchè®­ç»ƒæ•°æ®éªŒè¯æŠ¥å‘Š\n\n")
        f.write(f"**éªŒè¯æ—¶é—´**: {np.datetime64('now')}\n\n")
        f.write(f"**é…ç½®æ–‡ä»¶**: {config_path}\n\n")
        f.write(f"**æ•°æ®æ–‡ä»¶**: {data_path}\n\n")
        f.write(f"**éªŒè¯ç»“æœ**: {'âœ“ é€šè¿‡' if result else 'âœ— å¤±è´¥'}\n\n")
        f.write("è¯¦ç»†éªŒè¯ä¿¡æ¯è¯·æŸ¥çœ‹ç»ˆç«¯è¾“å‡ºã€‚\n")
    
    print(f"\néªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

if __name__ == "__main__":
    main()