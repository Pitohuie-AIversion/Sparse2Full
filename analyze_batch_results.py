#!/usr/bin/env python3
"""
æ‰¹é‡è®­ç»ƒç»“æœåˆ†æè„šæœ¬

åˆ†ææ‰¹é‡è®­ç»ƒçš„ç»“æœï¼Œç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def load_training_summary(batch_dir: str) -> Dict[str, Any]:
    """åŠ è½½è®­ç»ƒæ±‡æ€»ä¿¡æ¯"""
    summary_path = Path(batch_dir) / "training_summary.json"
    if not summary_path.exists():
        raise FileNotFoundError(f"è®­ç»ƒæ±‡æ€»æ–‡ä»¶ä¸å­˜åœ¨: {summary_path}")
    
    with open(summary_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def extract_model_metrics(batch_dir: str, model_name: str) -> Optional[Dict[str, Any]]:
    """æå–å•ä¸ªæ¨¡å‹çš„æŒ‡æ ‡"""
    model_dir = Path(batch_dir) / model_name.lower()
    
    # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
    best_model_path = model_dir / "checkpoints" / "best.pth"
    if not best_model_path.exists():
        print(f"è­¦å‘Š: æ¨¡å‹ {model_name} çš„æœ€ä½³æ£€æŸ¥ç‚¹ä¸å­˜åœ¨")
        return None
    
    # æŸ¥æ‰¾è®­ç»ƒæ—¥å¿—
    train_log_path = model_dir / "train.log"
    if not train_log_path.exists():
        print(f"è­¦å‘Š: æ¨¡å‹ {model_name} çš„è®­ç»ƒæ—¥å¿—ä¸å­˜åœ¨")
        return None
    
    # è§£æè®­ç»ƒæ—¥å¿—è·å–æœ€ç»ˆæŒ‡æ ‡
    metrics = parse_training_log(train_log_path)
    
    # æ·»åŠ æ¨¡å‹ä¿¡æ¯
    if best_model_path.exists():
        checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
        metrics.update({
            'epoch': checkpoint.get('epoch', 0),
            'best_val_loss': checkpoint.get('best_val_loss', float('inf')),
            'model_params': checkpoint.get('model_params', 0)
        })
    
    return metrics

def parse_training_log(log_path: Path) -> Dict[str, Any]:
    """è§£æè®­ç»ƒæ—¥å¿—è·å–æŒ‡æ ‡"""
    metrics = {
        'final_train_loss': None,
        'final_val_loss': None,
        'best_val_loss': float('inf'),
        'final_rel_l2': None,
        'final_mae': None,
        'final_psnr': None,
        'final_ssim': None
    }
    
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # ä»åå¾€å‰æŸ¥æ‰¾æœ€åçš„éªŒè¯æŒ‡æ ‡
        for line in reversed(lines):
            if 'val_loss' in line and 'rel_l2' in line:
                # è§£æéªŒè¯æŒ‡æ ‡è¡Œ
                parts = line.strip().split()
                for i, part in enumerate(parts):
                    if part.startswith('val_loss='):
                        metrics['final_val_loss'] = float(part.split('=')[1])
                    elif part.startswith('rel_l2='):
                        metrics['final_rel_l2'] = float(part.split('=')[1])
                    elif part.startswith('mae='):
                        metrics['final_mae'] = float(part.split('=')[1])
                    elif part.startswith('psnr='):
                        metrics['final_psnr'] = float(part.split('=')[1])
                    elif part.startswith('ssim='):
                        metrics['final_ssim'] = float(part.split('=')[1])
                break
        
        # æŸ¥æ‰¾æœ€ä½³éªŒè¯æŸå¤±
        for line in lines:
            if 'Best val_loss' in line or 'best_val_loss' in line:
                try:
                    # æå–æ•°å€¼
                    import re
                    numbers = re.findall(r'[\d.]+', line)
                    if numbers:
                        metrics['best_val_loss'] = float(numbers[0])
                except:
                    pass
    
    except Exception as e:
        print(f"è§£æè®­ç»ƒæ—¥å¿—å¤±è´¥ {log_path}: {e}")
    
    return metrics

def create_performance_comparison(results: Dict[str, Dict[str, Any]], output_dir: Path):
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    # å‡†å¤‡æ•°æ®
    models = []
    rel_l2_values = []
    mae_values = []
    psnr_values = []
    ssim_values = []
    val_loss_values = []
    training_times = []
    
    for model_name, data in results.items():
        if data['status'] == 'success' and data.get('metrics'):
            models.append(model_name)
            rel_l2_values.append(data['metrics'].get('final_rel_l2', 0))
            mae_values.append(data['metrics'].get('final_mae', 0))
            psnr_values.append(data['metrics'].get('final_psnr', 0))
            ssim_values.append(data['metrics'].get('final_ssim', 0))
            val_loss_values.append(data['metrics'].get('final_val_loss', 0))
            training_times.append(data.get('training_time', 0) / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿ
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    
    # Rel-L2è¯¯å·®å¯¹æ¯”
    axes[0, 0].bar(models, rel_l2_values, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('ç›¸å¯¹L2è¯¯å·® (è¶Šå°è¶Šå¥½)')
    axes[0, 0].set_ylabel('Rel-L2')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # MAEå¯¹æ¯”
    axes[0, 1].bar(models, mae_values, color='lightgreen', alpha=0.7)
    axes[0, 1].set_title('å¹³å‡ç»å¯¹è¯¯å·® (è¶Šå°è¶Šå¥½)')
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # PSNRå¯¹æ¯”
    axes[0, 2].bar(models, psnr_values, color='orange', alpha=0.7)
    axes[0, 2].set_title('å³°å€¼ä¿¡å™ªæ¯” (è¶Šå¤§è¶Šå¥½)')
    axes[0, 2].set_ylabel('PSNR (dB)')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    # SSIMå¯¹æ¯”
    axes[1, 0].bar(models, ssim_values, color='pink', alpha=0.7)
    axes[1, 0].set_title('ç»“æ„ç›¸ä¼¼æ€§ (è¶Šå¤§è¶Šå¥½)')
    axes[1, 0].set_ylabel('SSIM')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # éªŒè¯æŸå¤±å¯¹æ¯”
    axes[1, 1].bar(models, val_loss_values, color='lightcoral', alpha=0.7)
    axes[1, 1].set_title('éªŒè¯æŸå¤± (è¶Šå°è¶Šå¥½)')
    axes[1, 1].set_ylabel('Validation Loss')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # è®­ç»ƒæ—¶é—´å¯¹æ¯”
    axes[1, 2].bar(models, training_times, color='gold', alpha=0.7)
    axes[1, 2].set_title('è®­ç»ƒæ—¶é—´ (åˆ†é’Ÿ)')
    axes[1, 2].set_ylabel('æ—¶é—´ (åˆ†é’Ÿ)')
    axes[1, 2].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_ranking_table(results: Dict[str, Dict[str, Any]], output_dir: Path):
    """åˆ›å»ºæ¨¡å‹æ’åè¡¨"""
    # å‡†å¤‡æ•°æ®
    data = []
    for model_name, result in results.items():
        if result['status'] == 'success' and result.get('metrics'):
            metrics = result['metrics']
            data.append({
                'æ¨¡å‹': model_name,
                'Rel-L2': metrics.get('final_rel_l2', 0),
                'MAE': metrics.get('final_mae', 0),
                'PSNR': metrics.get('final_psnr', 0),
                'SSIM': metrics.get('final_ssim', 0),
                'éªŒè¯æŸå¤±': metrics.get('final_val_loss', 0),
                'è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)': round(result.get('training_time', 0) / 60, 2),
                'å‚æ•°é‡': result.get('model_params', 0)
            })
    
    if not data:
        print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹æ•°æ®ç”¨äºæ’å")
        return
    
    df = pd.DataFrame(data)
    
    # æŒ‰Rel-L2æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    df_sorted = df.sort_values('Rel-L2')
    
    # ä¿å­˜ä¸ºCSV
    df_sorted.to_csv(output_dir / 'model_ranking.csv', index=False, encoding='utf-8-sig')
    
    # åˆ›å»ºæ’åè¡¨å›¾
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # åˆ›å»ºè¡¨æ ¼
    table_data = df_sorted.values
    table = ax.table(cellText=table_data, 
                    colLabels=df_sorted.columns,
                    cellLoc='center',
                    loc='center',
                    bbox=[0, 0, 1, 1])
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(len(df_sorted.columns)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # è®¾ç½®æœ€ä½³æ€§èƒ½è¡Œçš„é¢œè‰²
    if len(table_data) > 0:
        for i in range(len(df_sorted.columns)):
            table[(1, i)].set_facecolor('#E8F5E8')  # æµ…ç»¿è‰²è¡¨ç¤ºæœ€ä½³
    
    ax.axis('off')
    ax.set_title('æ¨¡å‹æ€§èƒ½æ’åè¡¨ (æŒ‰Rel-L2æ’åº)', fontsize=14, fontweight='bold', pad=20)
    
    plt.savefig(output_dir / 'model_ranking_table.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return df_sorted

def generate_analysis_report(summary: Dict[str, Any], results: Dict[str, Dict[str, Any]], 
                           ranking_df: pd.DataFrame, output_dir: Path):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report_path = output_dir / 'batch_training_analysis_report.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ‰¹é‡è®­ç»ƒç»“æœåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # è®­ç»ƒæ¦‚è§ˆ
        f.write("## ğŸ“Š è®­ç»ƒæ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»æ¨¡å‹æ•°**: {summary['batch_training_info']['total_models']}\n")
        f.write(f"- **æˆåŠŸè®­ç»ƒ**: {summary['batch_training_info']['successful_models']}\n")
        f.write(f"- **è®­ç»ƒå¤±è´¥**: {summary['batch_training_info']['failed_models']}\n")
        f.write(f"- **æˆåŠŸç‡**: {summary['batch_training_info']['successful_models'] / summary['batch_training_info']['total_models'] * 100:.1f}%\n\n")
        
        # æˆåŠŸæ¨¡å‹åˆ—è¡¨
        f.write("### âœ… æˆåŠŸè®­ç»ƒçš„æ¨¡å‹\n\n")
        for model in summary['successful_models']:
            f.write(f"- {model}\n")
        f.write("\n")
        
        # å¤±è´¥æ¨¡å‹åˆ—è¡¨
        if summary['failed_models']:
            f.write("### âŒ è®­ç»ƒå¤±è´¥çš„æ¨¡å‹\n\n")
            for model in summary['failed_models']:
                f.write(f"- {model}\n")
            f.write("\n")
        
        # æ€§èƒ½æ’å
        f.write("## ğŸ† æ€§èƒ½æ’å (æŒ‰Rel-L2æ’åº)\n\n")
        if not ranking_df.empty:
            f.write("| æ’å | æ¨¡å‹ | Rel-L2 | MAE | PSNR | SSIM | éªŒè¯æŸå¤± | è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ) |\n")
            f.write("|------|------|--------|-----|------|------|----------|----------------|\n")
            
            for idx, row in ranking_df.iterrows():
                rank = ranking_df.index.get_loc(idx) + 1
                f.write(f"| {rank} | {row['æ¨¡å‹']} | {row['Rel-L2']:.4f} | {row['MAE']:.4f} | "
                       f"{row['PSNR']:.2f} | {row['SSIM']:.4f} | {row['éªŒè¯æŸå¤±']:.4f} | {row['è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)']} |\n")
        f.write("\n")
        
        # å…³é”®å‘ç°
        f.write("## ğŸ” å…³é”®å‘ç°\n\n")
        if not ranking_df.empty:
            best_model = ranking_df.iloc[0]
            f.write(f"### æœ€ä½³æ¨¡å‹: {best_model['æ¨¡å‹']}\n\n")
            f.write(f"- **Rel-L2è¯¯å·®**: {best_model['Rel-L2']:.4f}\n")
            f.write(f"- **PSNR**: {best_model['PSNR']:.2f} dB\n")
            f.write(f"- **SSIM**: {best_model['SSIM']:.4f}\n")
            f.write(f"- **è®­ç»ƒæ—¶é—´**: {best_model['è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)']} åˆ†é’Ÿ\n\n")
        
        # è®­ç»ƒé…ç½®
        f.write("## âš™ï¸ è®­ç»ƒé…ç½®\n\n")
        stable_config = summary['batch_training_info']['stable_config']
        f.write("### æŸå¤±å‡½æ•°é…ç½®\n")
        f.write(f"- rec_weight: {stable_config['loss']['rec_weight']}\n")
        f.write(f"- spec_weight: {stable_config['loss']['spec_weight']}\n")
        f.write(f"- dc_weight: {stable_config['loss']['dc_weight']}\n\n")
        
        f.write("### è®­ç»ƒå‚æ•°\n")
        f.write(f"- batch_size: {stable_config['training']['batch_size']}\n")
        f.write(f"- learning_rate: {stable_config['training']['lr']}\n")
        f.write(f"- epochs: {stable_config['training']['epochs']}\n")
        f.write(f"- use_amp: {stable_config['training']['use_amp']}\n\n")
        
        # æ–‡ä»¶è¯´æ˜
        f.write("## ğŸ“ ç”Ÿæˆæ–‡ä»¶è¯´æ˜\n\n")
        f.write("- `performance_comparison.png`: æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾è¡¨\n")
        f.write("- `model_ranking_table.png`: æ¨¡å‹æ’åè¡¨\n")
        f.write("- `model_ranking.csv`: è¯¦ç»†æ’åæ•°æ®\n")
        f.write("- `batch_training_analysis_report.md`: æœ¬åˆ†ææŠ¥å‘Š\n\n")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡è®­ç»ƒç»“æœåˆ†æ')
    parser.add_argument('--batch_dir', type=str, required=True, help='æ‰¹é‡è®­ç»ƒç»“æœç›®å½•')
    parser.add_argument('--output_dir', type=str, help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºbatch_dir/analysisï¼‰')
    
    args = parser.parse_args()
    
    batch_dir = Path(args.batch_dir)
    if not batch_dir.exists():
        print(f"é”™è¯¯: æ‰¹é‡è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {batch_dir}")
        return
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = batch_dir / 'analysis'
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ”„ å¼€å§‹åˆ†ææ‰¹é‡è®­ç»ƒç»“æœ...")
    print(f"ğŸ“ æ‰¹é‡è®­ç»ƒç›®å½•: {batch_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # åŠ è½½è®­ç»ƒæ±‡æ€»
        summary = load_training_summary(batch_dir)
        print(f"âœ… åŠ è½½è®­ç»ƒæ±‡æ€»æˆåŠŸ")
        
        # æå–æ¯ä¸ªæ¨¡å‹çš„æŒ‡æ ‡
        results = {}
        for model_info in summary['detailed_log']:
            model_name = model_info['model']
            print(f"ğŸ”„ åˆ†ææ¨¡å‹: {model_name}")
            
            result = {
                'status': model_info['status'],
                'training_time': model_info['training_time'],
                'model_params': 0  # é»˜è®¤å€¼
            }
            
            if model_info['status'] == 'success':
                metrics = extract_model_metrics(batch_dir, model_name)
                result['metrics'] = metrics
            
            results[model_name] = result
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
        print("ğŸ”„ åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾...")
        create_performance_comparison(results, output_dir)
        
        # åˆ›å»ºæ’åè¡¨
        print("ğŸ”„ åˆ›å»ºæ’åè¡¨...")
        ranking_df = create_ranking_table(results, output_dir)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("ğŸ”„ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        generate_analysis_report(summary, results, ranking_df, output_dir)
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_path in sorted(output_dir.glob("*")):
            print(f"  - {file_path.name}")
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # å¯¼å…¥å¿…è¦çš„åº“
    import torch
    import matplotlib
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    
    main()