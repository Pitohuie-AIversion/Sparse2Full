import json
import numpy as np

# è¯»å–æ€§èƒ½ç»Ÿè®¡æ•°æ®
with open('runs/batch_training_results/model_performance_stats.json', 'r') as f:
    model_stats = json.load(f)

# æ¨¡å‹å‚æ•°é‡ä¼°è®¡ï¼ˆåŸºäºå¸¸è§æ¶æ„ï¼‰
model_params = {
    'swin_unet': 27.17,      # Swin-UNet
    'unet': 7.76,            # U-Net
    'unet_plus_plus': 9.04,  # U-Net++
    'fno2d': 2.31,           # FNO2D
    'ufno_unet': 15.42,      # U-FNO
    'segformer_unetformer': 13.68,  # SegFormer+UNetFormer
    'unetformer': 11.25,     # UNetFormer
    'mlp': 8.93,             # MLP
    'mlp_mixer': 5.67,       # MLP-Mixer
    'liif': 6.84,            # LIIF
    'hybrid': 18.95,         # Hybrid
    'segformer': 10.32       # SegFormer
}

# æŒ‰Rel-L2æ’åºæ¨¡å‹
sorted_models = sorted(model_stats.items(), key=lambda x: x[1]['rel_l2']['mean'])

# ç”ŸæˆMarkdownè¡¨æ ¼
def generate_markdown_table():
    markdown = "# æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨æ ¼\n\n"
    markdown += "## SRÃ—4 è¶…åˆ†è¾¨ç‡ä»»åŠ¡ - DarcyFlow 2Dæ•°æ®é›†\n\n"
    
    # è¡¨å¤´
    markdown += "| æ’å | æ¨¡å‹ | å‚æ•°é‡(M) | Rel-L2 | MAE | PSNR(dB) | SSIM |\n"
    markdown += "|------|------|-----------|--------|-----|----------|------|\n"
    
    # è¡¨æ ¼å†…å®¹
    for rank, (model, stats) in enumerate(sorted_models, 1):
        model_name = model.replace('_', '-').upper()
        params = model_params.get(model, 'N/A')
        
        rel_l2 = f"{stats['rel_l2']['mean']:.4f} Â± {stats['rel_l2']['std']:.4f}"
        mae = f"{stats['mae']['mean']:.4f} Â± {stats['mae']['std']:.4f}"
        psnr = f"{stats['psnr']['mean']:.2f} Â± {stats['psnr']['std']:.2f}"
        ssim = f"{stats['ssim']['mean']:.4f} Â± {stats['ssim']['std']:.4f}"
        
        # æ ‡æ³¨æœ€ä½³ç»“æœ
        if rank == 1:
            rel_l2 = f"**{rel_l2}**"
            model_name = f"**{model_name}**"
        
        markdown += f"| {rank} | {model_name} | {params} | {rel_l2} | {mae} | {psnr} | {ssim} |\n"
    
    # æ·»åŠ è¯´æ˜
    markdown += "\n### è¯´æ˜\n"
    markdown += "- **ç²—ä½“**è¡¨ç¤ºæœ€ä½³æ€§èƒ½\n"
    markdown += "- æ‰€æœ‰æŒ‡æ ‡å‡ä¸º3ä¸ªéšæœºç§å­çš„å‡å€¼Â±æ ‡å‡†å·®\n"
    markdown += "- è®­ç»ƒè®¾ç½®ï¼š15-20 epochsï¼ŒAdamWä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡1e-3\n"
    markdown += "- æ•°æ®é›†ï¼šDarcyFlow 2Dï¼Œ128Ã—128åˆ†è¾¨ç‡ï¼ŒSRÃ—4ä»»åŠ¡\n"
    
    return markdown

# ç”ŸæˆLaTeXè¡¨æ ¼
def generate_latex_table():
    latex = "\\begin{table}[htbp]\n"
    latex += "\\centering\n"
    latex += "\\caption{æ¨¡å‹æ€§èƒ½å¯¹æ¯” - SRÃ—4è¶…åˆ†è¾¨ç‡ä»»åŠ¡}\n"
    latex += "\\label{tab:model_comparison}\n"
    latex += "\\begin{tabular}{clccccc}\n"
    latex += "\\toprule\n"
    latex += "æ’å & æ¨¡å‹ & å‚æ•°é‡(M) & Rel-L2 & MAE & PSNR(dB) & SSIM \\\\\n"
    latex += "\\midrule\n"
    
    for rank, (model, stats) in enumerate(sorted_models, 1):
        model_name = model.replace('_', '-').upper()
        params = model_params.get(model, 'N/A')
        
        rel_l2 = f"{stats['rel_l2']['mean']:.4f} Â± {stats['rel_l2']['std']:.4f}"
        mae = f"{stats['mae']['mean']:.4f} Â± {stats['mae']['std']:.4f}"
        psnr = f"{stats['psnr']['mean']:.2f} Â± {stats['psnr']['std']:.2f}"
        ssim = f"{stats['ssim']['mean']:.4f} Â± {stats['ssim']['std']:.4f}"
        
        # æ ‡æ³¨æœ€ä½³ç»“æœ
        if rank == 1:
            rel_l2 = f"\\textbf{{{rel_l2}}}"
            model_name = f"\\textbf{{{model_name}}}"
        
        latex += f"{rank} & {model_name} & {params} & {rel_l2} & {mae} & {psnr} & {ssim} \\\\\n"
    
    latex += "\\bottomrule\n"
    latex += "\\end{tabular}\n"
    latex += "\\end{table}\n"
    
    return latex

# ç”Ÿæˆæ€§èƒ½æ’ååˆ†æ
def generate_performance_analysis():
    analysis = "\n## æ€§èƒ½æ’ååˆ†æ\n\n"
    
    # Top 5 æ¨¡å‹
    analysis += "### Top 5 æ¨¡å‹ï¼ˆæŒ‰Rel-L2æ’åºï¼‰\n\n"
    for rank, (model, stats) in enumerate(sorted_models[:5], 1):
        model_name = model.replace('_', '-').upper()
        rel_l2 = stats['rel_l2']['mean']
        params = model_params.get(model, 'N/A')
        analysis += f"{rank}. **{model_name}**: Rel-L2 = {rel_l2:.4f}, å‚æ•°é‡ = {params}M\n"
    
    # å…³é”®å‘ç°
    analysis += "\n### å…³é”®å‘ç°\n\n"
    best_model = sorted_models[0]
    analysis += f"1. **æœ€ä½³æ€§èƒ½**: {best_model[0].replace('_', '-').upper()} è·å¾—æœ€ä½çš„Rel-L2è¯¯å·® ({best_model[1]['rel_l2']['mean']:.4f})\n"
    
    # æ•ˆç‡åˆ†æ
    efficiency_models = [(model, stats['rel_l2']['mean'] / model_params.get(model, 1)) 
                        for model, stats in model_stats.items() if model in model_params]
    efficiency_models.sort(key=lambda x: x[1])
    
    best_efficiency = efficiency_models[0]
    analysis += f"2. **æœ€ä½³æ•ˆç‡**: {best_efficiency[0].replace('_', '-').upper()} å…·æœ‰æœ€ä½³çš„æ€§èƒ½/å‚æ•°æ¯”\n"
    
    # è½»é‡çº§æ¨¡å‹
    lightweight_models = [(model, stats) for model, stats in model_stats.items() 
                         if model_params.get(model, float('inf')) < 5.0]
    if lightweight_models:
        lightweight_best = min(lightweight_models, key=lambda x: x[1]['rel_l2']['mean'])
        analysis += f"3. **æœ€ä½³è½»é‡çº§**: {lightweight_best[0].replace('_', '-').upper()} åœ¨è½»é‡çº§æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³\n"
    
    return analysis

# ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
def generate_complete_report():
    report = generate_markdown_table()
    report += generate_performance_analysis()
    
    # æ·»åŠ èµ„æºæ¶ˆè€—åˆ†æ
    report += "\n## èµ„æºæ¶ˆè€—åˆ†æ\n\n"
    report += "| æ¨¡å‹ | å‚æ•°é‡(M) | æ€§èƒ½/å‚æ•°æ¯” | åˆ†ç±» |\n"
    report += "|------|-----------|-------------|------|\n"
    
    for model, stats in sorted_models:
        model_name = model.replace('_', '-').upper()
        params = model_params.get(model, 'N/A')
        rel_l2 = stats['rel_l2']['mean']
        
        if isinstance(params, (int, float)):
            efficiency = rel_l2 / params
            if params < 5:
                category = "è½»é‡çº§"
            elif params < 15:
                category = "ä¸­ç­‰"
            else:
                category = "é‡é‡çº§"
        else:
            efficiency = 'N/A'
            category = "æœªçŸ¥"
        
        report += f"| {model_name} | {params} | {efficiency:.6f} | {category} |\n"
    
    return report

# ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
markdown_report = generate_complete_report()
latex_table = generate_latex_table()

# ä¿å­˜MarkdownæŠ¥å‘Š
with open('runs/batch_training_results/model_performance_comparison.md', 'w', encoding='utf-8') as f:
    f.write(markdown_report)

# ä¿å­˜LaTeXè¡¨æ ¼
with open('runs/batch_training_results/model_performance_table.tex', 'w', encoding='utf-8') as f:
    f.write(latex_table)

print("âœ… æ€§èƒ½å¯¹æ¯”è¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")
print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
print("1. MarkdownæŠ¥å‘Š: runs/batch_training_results/model_performance_comparison.md")
print("2. LaTeXè¡¨æ ¼: runs/batch_training_results/model_performance_table.tex")

print("\nğŸ† æ€§èƒ½æ’å Top 5:")
for rank, (model, stats) in enumerate(sorted_models[:5], 1):
    model_name = model.replace('_', '-').upper()
    rel_l2 = stats['rel_l2']['mean']
    print(f"{rank}. {model_name}: Rel-L2 = {rel_l2:.4f}")