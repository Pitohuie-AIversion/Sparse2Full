#!/usr/bin/env python3
"""
PDEBenchç¨€ç–è§‚æµ‹é‡å»ºè®ºæ–‡ææ–™ç”Ÿæˆå·¥å…·

ä¸¥æ ¼éµå¾ªé»„é‡‘æ³•åˆ™ï¼š
1. ä¸€è‡´æ€§ä¼˜å…ˆï¼šè§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCå¿…é¡»å¤ç”¨åŒä¸€å®ç°ä¸é…ç½®
2. å¯å¤ç°ï¼šåŒä¸€YAML+ç§å­ï¼ŒéªŒè¯æŒ‡æ ‡æ–¹å·®â‰¤1e-4
3. ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰æ¨¡å‹forward(x[B,C_in,H,W])â†’y[B,C_out,H,W]
4. å¯æ¯”æ€§ï¼šæŠ¥å‘Šå‡å€¼Â±æ ‡å‡†å·®ï¼ˆâ‰¥3ç§å­ï¼‰+èµ„æºæˆæœ¬
5. æ–‡æ¡£å…ˆè¡Œï¼šæ–°å¢ä»»åŠ¡/ç®—å­/æ¨¡å‹å‰ï¼Œå…ˆæäº¤PRD/æŠ€æœ¯æ–‡æ¡£è¡¥ä¸

ç”Ÿæˆå†…å®¹ï¼š
- æ•°æ®å¡ç‰‡ï¼ˆæ¥æº/è®¸å¯/åˆ‡åˆ†ï¼‰
- é…ç½®å¿«ç…§ï¼ˆæœ€ç»ˆYAMLï¼‰
- æ¨¡å‹æƒé‡ï¼ˆå…³é”®ckptï¼Œèµ°LFSï¼‰
- æŒ‡æ ‡æ±‡æ€»ï¼ˆä¸»è¡¨/æ˜¾è‘—æ€§/CSV/æ¯case JSONLï¼‰
- å¯è§†åŒ–å›¾è¡¨ï¼ˆä»£è¡¨å›¾/å¤±è´¥æ¡ˆä¾‹/è°±å›¾ï¼‰
- å¤ç°è„šæœ¬ï¼ˆä¸€é”®å¤ç°ä¸æ±‡æ€»ï¼‰
- READMEï¼ˆç¯å¢ƒ/å‘½ä»¤/ç»“æœé‡ç°ï¼‰
"""

import os
import sys
import json
import shutil
import logging
import argparse
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

import numpy as np
import torch
import yaml
from omegaconf import DictConfig, OmegaConf
import hydra

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é¡¹ç›®å¯¼å…¥
from utils.config import get_environment_info
from utils.reproducibility import set_seed

class PaperPackageGenerator:
    """è®ºæ–‡ææ–™åŒ…ç”Ÿæˆå™¨
    
    è‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„è®ºæ–‡ææ–™åŒ…ï¼ŒåŒ…å«ï¼š
    1. æ•°æ®å¡ç‰‡å’Œé…ç½®å¿«ç…§
    2. æ¨¡å‹æƒé‡å’ŒæŒ‡æ ‡æ±‡æ€»
    3. å¯è§†åŒ–å›¾è¡¨å’Œå¤ç°è„šæœ¬
    4. READMEå’Œç¯å¢ƒä¿¡æ¯
    """
    
    def __init__(self, config: DictConfig, output_dir: Path):
        self.config = config
        self.output_dir = Path(output_dir)
        self.package_config = config.get('paper_package', {})
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.create_directory_structure()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # æ”¶é›†ç¯å¢ƒä¿¡æ¯
        self.env_info = get_environment_info()
        
    def create_directory_structure(self):
        """åˆ›å»ºè®ºæ–‡ææ–™åŒ…ç›®å½•ç»“æ„"""
        self.dirs = {
            'root': self.output_dir,
            'data_cards': self.output_dir / 'data_cards',
            'configs': self.output_dir / 'configs',
            'checkpoints': self.output_dir / 'checkpoints',
            'metrics': self.output_dir / 'metrics',
            'figs': self.output_dir / 'figs',
            'scripts': self.output_dir / 'scripts',
            'logs': self.output_dir / 'logs'
        }
        
        # åˆ›å»ºæ‰€æœ‰ç›®å½•
        for dir_path in self.dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
            
        logging.info(f"Created paper package directory structure at {self.output_dir}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.dirs['logs'] / 'package_generation.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("Paper package generation started")
    
    def generate_data_cards(self):
        """ç”Ÿæˆæ•°æ®å¡ç‰‡"""
        self.logger.info("Generating data cards...")
        
        data_config = self.config.get('data', {})
        
        # åŸºæœ¬æ•°æ®å¡ç‰‡æ¨¡æ¿
        data_card = {
            'dataset_name': data_config.get('dataset_name', 'PDEBench'),
            'version': data_config.get('version', '1.0'),
            'source': {
                'url': data_config.get('source_url', 'https://github.com/pdebench/PDEBench'),
                'license': data_config.get('license', 'MIT'),
                'citation': data_config.get('citation', 'PDEBench: An Extensive Benchmark for Scientific Machine Learning')
            },
            'description': {
                'task_type': data_config.get('task_type', 'sparse_observation_reconstruction'),
                'variables': data_config.get('variables', []),
                'spatial_resolution': data_config.get('spatial_resolution', [256, 256]),
                'temporal_steps': data_config.get('temporal_steps', 1),
                'boundary_conditions': data_config.get('boundary_conditions', 'periodic')
            },
            'splits': {
                'train': data_config.get('train_split', 0.7),
                'val': data_config.get('val_split', 0.15),
                'test': data_config.get('test_split', 0.15),
                'split_method': data_config.get('split_method', 'random'),
                'split_seed': data_config.get('split_seed', 42)
            },
            'preprocessing': {
                'normalization': data_config.get('normalization', 'z_score'),
                'observation_operator': data_config.get('observation_operator', {}),
                'degradation_params': data_config.get('degradation_params', {})
            },
            'statistics': self.compute_data_statistics_from_config(data_config),
            'meta': {
                'generated_at': datetime.now().isoformat(),
                'generator_version': '1.0',
                'environment': self.env_info
            }
        }
        
        # ä¿å­˜æ•°æ®å¡ç‰‡ - è½¬æ¢OmegaConfå¯¹è±¡ä¸ºæ™®é€šå­—å…¸
        def convert_to_serializable(obj):
            """é€’å½’è½¬æ¢OmegaConfå¯¹è±¡ä¸ºå¯åºåˆ—åŒ–çš„æ™®é€šå¯¹è±¡"""
            if hasattr(obj, '_content'):  # OmegaConfå¯¹è±¡
                return OmegaConf.to_container(obj, resolve=True)
            elif isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            else:
                return obj
        
        serializable_data_card = convert_to_serializable(data_card)
        
        data_card_path = self.dirs['data_cards'] / 'dataset_card.json'
        with open(data_card_path, 'w') as f:
            json.dump(serializable_data_card, f, indent=2)
        
        # ç”ŸæˆMarkdownç‰ˆæœ¬
        self.generate_data_card_markdown(data_card)
        
        self.logger.info(f"Data cards saved to {self.dirs['data_cards']}")
    
    def compute_data_statistics_from_config(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """ä»é…ç½®ä¸­è®¡ç®—æ•°æ®ç»Ÿè®¡é‡"""
        try:
            # å°è¯•ä»é…ç½®ä¸­è·å–æ•°æ®è·¯å¾„
            data_path = data_config.get('data_path', data_config.get('path', ''))
            
            if data_path and Path(data_path).exists():
                return self.compute_data_statistics(Path(data_path))
            else:
                # è¿”å›æ¨¡æ¿ç»Ÿè®¡é‡
                return {
                    'total_samples': data_config.get('total_samples', 10000),
                    'train_samples': int(data_config.get('total_samples', 10000) * data_config.get('train_split', 0.7)),
                    'val_samples': int(data_config.get('total_samples', 10000) * data_config.get('val_split', 0.15)),
                    'test_samples': int(data_config.get('total_samples', 10000) * data_config.get('test_split', 0.15)),
                    'data_shape': data_config.get('spatial_resolution', [256, 256]),
                    'channels': data_config.get('channels', 1),
                    'data_range': data_config.get('data_range', [-3.0, 3.0]),
                    'mean': data_config.get('mean', 0.0),
                    'std': data_config.get('std', 1.0),
                    'note': 'Statistics computed from configuration (actual data not found)'
                }
        except Exception as e:
            self.logger.warning(f"Could not compute data statistics: {e}")
            return {'error': str(e)}

    def compute_data_statistics(self, data_path: Path) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®ç»Ÿè®¡é‡
        
        Args:
            data_path: æ•°æ®è·¯å¾„
            
        Returns:
            statistics: æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            import h5py
            import numpy as np
            
            stats = {}
            
            if data_path.suffix == '.h5' or data_path.suffix == '.hdf5':
                with h5py.File(data_path, 'r') as f:
                    # è·å–æ•°æ®é›†ä¿¡æ¯
                    datasets = list(f.keys())
                    stats['datasets'] = datasets
                    
                    if datasets:
                        # åˆ†æç¬¬ä¸€ä¸ªæ•°æ®é›†
                        first_dataset = f[datasets[0]]
                        stats['data_shape'] = list(first_dataset.shape)
                        stats['data_dtype'] = str(first_dataset.dtype)
                        
                        # é‡‡æ ·è®¡ç®—ç»Ÿè®¡é‡ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
                        if first_dataset.size > 1000000:  # å¦‚æœæ•°æ®å¤ªå¤§ï¼Œé‡‡æ ·
                            sample_indices = np.random.choice(
                                first_dataset.size, 
                                size=min(100000, first_dataset.size), 
                                replace=False
                            )
                            sample_data = first_dataset.flat[sample_indices]
                        else:
                            sample_data = first_dataset[:]
                        
                        stats['data_range'] = [float(np.min(sample_data)), float(np.max(sample_data))]
                        stats['data_mean'] = float(np.mean(sample_data))
                        stats['data_std'] = float(np.std(sample_data))
                        stats['total_samples'] = first_dataset.shape[0] if len(first_dataset.shape) > 0 else 1
                        
                        if len(first_dataset.shape) >= 3:
                            stats['channels'] = first_dataset.shape[1] if len(first_dataset.shape) == 4 else 1
                        else:
                            stats['channels'] = 1
            
            elif data_path.suffix == '.nc':
                import xarray as xr
                
                ds = xr.open_dataset(data_path)
                data_vars = list(ds.data_vars.keys())
                stats['data_vars'] = data_vars
                
                if data_vars:
                    first_var = ds[data_vars[0]]
                    stats['data_shape'] = list(first_var.shape)
                    stats['data_dtype'] = str(first_var.dtype)
                    
                    # è®¡ç®—ç»Ÿè®¡é‡
                    sample_data = first_var.values.flatten()
                    if len(sample_data) > 100000:
                        sample_data = np.random.choice(sample_data, 100000, replace=False)
                    
                    stats['data_range'] = [float(np.min(sample_data)), float(np.max(sample_data))]
                    stats['data_mean'] = float(np.mean(sample_data))
                    stats['data_std'] = float(np.std(sample_data))
                    stats['total_samples'] = first_var.shape[0] if len(first_var.shape) > 0 else 1
                    stats['channels'] = first_var.shape[1] if len(first_var.shape) >= 3 else 1
                
                ds.close()
            
            else:
                # å…¶ä»–æ ¼å¼çš„åŸºæœ¬ä¿¡æ¯
                stats['file_size_mb'] = data_path.stat().st_size / (1024 * 1024)
                stats['file_format'] = data_path.suffix
            
            return stats
            
        except Exception as e:
            self.logger.warning(f"Could not compute statistics for {data_path}: {e}")
            return {'error': str(e)}
    
    def generate_data_card_markdown(self, data_card: Dict[str, Any]):
        """ç”Ÿæˆæ•°æ®å¡ç‰‡Markdownç‰ˆæœ¬"""
        md_path = self.dirs['data_cards'] / 'dataset_card.md'
        
        with open(md_path, 'w') as f:
            f.write("# æ•°æ®é›†å¡ç‰‡\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write("## åŸºæœ¬ä¿¡æ¯\n\n")
            f.write(f"- **æ•°æ®é›†åç§°**: {data_card['dataset_name']}\n")
            f.write(f"- **ç‰ˆæœ¬**: {data_card['version']}\n")
            f.write(f"- **è®¸å¯è¯**: {data_card['source']['license']}\n")
            f.write(f"- **æ¥æº**: {data_card['source']['url']}\n\n")
            
            # ä»»åŠ¡æè¿°
            desc = data_card['description']
            f.write("## ä»»åŠ¡æè¿°\n\n")
            f.write(f"- **ä»»åŠ¡ç±»å‹**: {desc['task_type']}\n")
            f.write(f"- **ç©ºé—´åˆ†è¾¨ç‡**: {desc['spatial_resolution']}\n")
            f.write(f"- **è¾¹ç•Œæ¡ä»¶**: {desc['boundary_conditions']}\n\n")
            
            # æ•°æ®åˆ‡åˆ†
            splits = data_card['splits']
            f.write("## æ•°æ®åˆ‡åˆ†\n\n")
            f.write(f"- **è®­ç»ƒé›†**: {splits['train']:.1%}\n")
            f.write(f"- **éªŒè¯é›†**: {splits['val']:.1%}\n")
            f.write(f"- **æµ‹è¯•é›†**: {splits['test']:.1%}\n")
            f.write(f"- **åˆ‡åˆ†æ–¹æ³•**: {splits['split_method']}\n")
            f.write(f"- **éšæœºç§å­**: {splits['split_seed']}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            if 'statistics' in data_card and data_card['statistics']:
                stats = data_card['statistics']
                f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
                f.write(f"- **æ€»æ ·æœ¬æ•°**: {stats.get('total_samples', 'N/A')}\n")
                f.write(f"- **æ•°æ®å½¢çŠ¶**: {stats.get('data_shape', 'N/A')}\n")
                f.write(f"- **é€šé“æ•°**: {stats.get('channels', 'N/A')}\n")
                f.write(f"- **æ•°æ®èŒƒå›´**: {stats.get('data_range', 'N/A')}\n\n")
    
    def collect_config_snapshots(self):
        """æ”¶é›†é…ç½®å¿«ç…§"""
        self.logger.info("Collecting configuration snapshots...")
        
        # ä¿å­˜å½“å‰å®Œæ•´é…ç½®
        config_snapshot = {
            'config': OmegaConf.to_container(self.config, resolve=True),
            'timestamp': datetime.now().isoformat(),
            'git_info': self.get_git_info(),
            'environment': self.env_info
        }
        
        # ä¿å­˜JSONæ ¼å¼
        config_json_path = self.dirs['configs'] / 'config_merged.json'
        with open(config_json_path, 'w') as f:
            json.dump(config_snapshot, f, indent=2)
        
        # ä¿å­˜YAMLæ ¼å¼
        config_yaml_path = self.dirs['configs'] / 'config_merged.yaml'
        with open(config_yaml_path, 'w') as f:
            yaml.dump(config_snapshot, f, default_flow_style=False)
        
        self.logger.info(f"Configuration snapshots saved to {self.dirs['configs']}")
    
    def get_git_info(self) -> Dict[str, Any]:
        """è·å–Gitä¿¡æ¯"""
        try:
            import subprocess
            
            # è·å–å½“å‰commit hash
            commit_hash = subprocess.check_output(
                ['git', 'rev-parse', 'HEAD'], 
                cwd=Path.cwd(),
                text=True
            ).strip()
            
            # è·å–å½“å‰åˆ†æ”¯
            branch = subprocess.check_output(
                ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                cwd=Path.cwd(),
                text=True
            ).strip()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœªæäº¤çš„æ›´æ”¹
            status = subprocess.check_output(
                ['git', 'status', '--porcelain'],
                cwd=Path.cwd(),
                text=True
            ).strip()
            
            return {
                'commit_hash': commit_hash,
                'branch': branch,
                'has_uncommitted_changes': bool(status),
                'status': status
            }
            
        except Exception as e:
            self.logger.warning(f"Could not get git info: {e}")
            return {'error': str(e)}
    
    def collect_checkpoints(self):
        """æ”¶é›†æ¨¡å‹æƒé‡"""
        self.logger.info("Collecting model checkpoints...")
        
        # æŸ¥æ‰¾runsç›®å½•ä¸­çš„checkpoints
        runs_dir = Path('runs')
        if not runs_dir.exists():
            self.logger.warning("No runs directory found")
            return
        
        checkpoint_info = []
        
        # éå†æ‰€æœ‰å®éªŒç›®å½•
        for exp_dir in runs_dir.iterdir():
            if not exp_dir.is_dir():
                continue
                
            # æŸ¥æ‰¾checkpoints
            ckpt_dir = exp_dir / 'checkpoints'
            if ckpt_dir.exists():
                for ckpt_file in ckpt_dir.glob('*.ckpt'):
                    # å¤åˆ¶é‡è¦çš„checkpoints
                    if any(keyword in ckpt_file.name.lower() 
                          for keyword in ['best', 'final', 'epoch']):
                        
                        dest_path = self.dirs['checkpoints'] / f"{exp_dir.name}_{ckpt_file.name}"
                        shutil.copy2(ckpt_file, dest_path)
                        
                        checkpoint_info.append({
                            'experiment': exp_dir.name,
                            'checkpoint': ckpt_file.name,
                            'path': str(dest_path.relative_to(self.output_dir)),
                            'size_mb': ckpt_file.stat().st_size / (1024 * 1024),
                            'modified': datetime.fromtimestamp(ckpt_file.stat().st_mtime).isoformat()
                        })
        
        # ä¿å­˜checkpointä¿¡æ¯
        ckpt_info_path = self.dirs['checkpoints'] / 'checkpoint_info.json'
        with open(ckpt_info_path, 'w') as f:
            json.dump(checkpoint_info, f, indent=2)
        
        self.logger.info(f"Collected {len(checkpoint_info)} checkpoints")
    
    def collect_metrics(self):
        """æ”¶é›†æŒ‡æ ‡æ±‡æ€»"""
        self.logger.info("Collecting metrics...")
        
        # æŸ¥æ‰¾runsç›®å½•ä¸­çš„æŒ‡æ ‡æ–‡ä»¶
        runs_dir = Path('runs')
        if not runs_dir.exists():
            self.logger.warning("No runs directory found")
            return
        
        all_metrics = []
        experiment_summaries = {}
        
        # éå†æ‰€æœ‰å®éªŒç›®å½•
        for exp_dir in runs_dir.iterdir():
            if not exp_dir.is_dir():
                continue
            
            exp_name = exp_dir.name
            exp_metrics = {
                'experiment': exp_name,
                'metrics': {},
                'config': {},
                'resources': {}
            }
            
            # æ”¶é›†æŒ‡æ ‡æ–‡ä»¶
            metrics_files = list(exp_dir.glob('**/metrics*.json*'))
            for metrics_file in metrics_files:
                try:
                    if metrics_file.suffix == '.jsonl':
                        # JSONLæ ¼å¼
                        with open(metrics_file, 'r') as f:
                            for line in f:
                                metric_data = json.loads(line)
                                all_metrics.append({
                                    'experiment': exp_name,
                                    **metric_data
                                })
                    else:
                        # JSONæ ¼å¼
                        with open(metrics_file, 'r') as f:
                            metric_data = json.load(f)
                            exp_metrics['metrics'].update(metric_data)
                            
                except Exception as e:
                    self.logger.warning(f"Could not read metrics file {metrics_file}: {e}")
            
            # æ”¶é›†é…ç½®ä¿¡æ¯
            config_files = list(exp_dir.glob('**/config*.yaml')) + list(exp_dir.glob('**/config*.json'))
            for config_file in config_files:
                try:
                    if config_file.suffix == '.yaml':
                        with open(config_file, 'r') as f:
                            config_data = yaml.safe_load(f)
                    else:
                        with open(config_file, 'r') as f:
                            config_data = json.load(f)
                    
                    exp_metrics['config'].update(config_data)
                    
                except Exception as e:
                    self.logger.warning(f"Could not read config file {config_file}: {e}")
            
            experiment_summaries[exp_name] = exp_metrics
        
        # ä¿å­˜æ‰€æœ‰æŒ‡æ ‡
        if all_metrics:
            all_metrics_path = self.dirs['metrics'] / 'all_metrics.jsonl'
            with open(all_metrics_path, 'w') as f:
                for metric in all_metrics:
                    f.write(json.dumps(metric) + '\n')
        
        # ä¿å­˜å®éªŒæ±‡æ€»
        if experiment_summaries:
            summary_path = self.dirs['metrics'] / 'experiment_summaries.json'
            with open(summary_path, 'w') as f:
                json.dump(experiment_summaries, f, indent=2)
        
        # ç”Ÿæˆä¸»è¡¨
        self.generate_main_results_table(experiment_summaries)
        
        self.logger.info(f"Collected metrics from {len(experiment_summaries)} experiments")
    
    def generate_main_results_table(self, experiment_summaries: Dict[str, Any]):
        """ç”Ÿæˆä¸»ç»“æœè¡¨æ ¼"""
        if not experiment_summaries:
            return
        
        # ç”ŸæˆCSVæ ¼å¼ä¸»è¡¨
        csv_path = self.dirs['metrics'] / 'main_results.csv'
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = ['rel_l2', 'mae', 'psnr', 'ssim', 'params_m', 'flops_g']
        
        with open(csv_path, 'w') as f:
            # å†™å…¥è¡¨å¤´
            f.write('Experiment,' + ','.join(key_metrics) + '\n')
            
            # å†™å…¥æ¯ä¸ªå®éªŒçš„ç»“æœ
            for exp_name, exp_data in experiment_summaries.items():
                metrics = exp_data.get('metrics', {})
                row = [exp_name]
                
                for metric in key_metrics:
                    value = metrics.get(metric, 'N/A')
                    if isinstance(value, (int, float)):
                        row.append(f"{value:.4f}")
                    else:
                        row.append(str(value))
                
                f.write(','.join(row) + '\n')
        
        # ç”ŸæˆMarkdownæ ¼å¼ä¸»è¡¨
        self.generate_main_results_markdown(experiment_summaries, key_metrics)
    
    def generate_main_results_markdown(self, experiment_summaries: Dict[str, Any], key_metrics: List[str]):
        """ç”ŸæˆMarkdownæ ¼å¼ä¸»ç»“æœè¡¨"""
        md_path = self.dirs['metrics'] / 'main_results.md'
        
        with open(md_path, 'w') as f:
            f.write("# ä¸»è¦å®éªŒç»“æœ\n\n")
            
            # è¡¨æ ¼æ ‡é¢˜
            f.write("| å®éªŒ | " + " | ".join(key_metrics) + " |\n")
            f.write("|" + "---|" * (len(key_metrics) + 1) + "\n")
            
            # è¡¨æ ¼å†…å®¹
            for exp_name, exp_data in experiment_summaries.items():
                metrics = exp_data.get('metrics', {})
                row = [exp_name]
                
                for metric in key_metrics:
                    value = metrics.get(metric, 'N/A')
                    if isinstance(value, (int, float)):
                        if metric in ['rel_l2', 'mae']:
                            row.append(f"{value:.4f}")
                        elif metric in ['psnr', 'ssim']:
                            row.append(f"{value:.2f}")
                        elif metric in ['params_m', 'flops_g']:
                            row.append(f"{value:.1f}")
                        else:
                            row.append(f"{value:.4f}")
                    else:
                        row.append(str(value))
                
                f.write("| " + " | ".join(row) + " |\n")
            
            f.write("\n## æŒ‡æ ‡è¯´æ˜\n\n")
            f.write("- **rel_l2**: ç›¸å¯¹L2è¯¯å·®\n")
            f.write("- **mae**: å¹³å‡ç»å¯¹è¯¯å·®\n")
            f.write("- **psnr**: å³°å€¼ä¿¡å™ªæ¯”\n")
            f.write("- **ssim**: ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°\n")
            f.write("- **params_m**: å‚æ•°é‡ï¼ˆç™¾ä¸‡ï¼‰\n")
            f.write("- **flops_g**: è®¡ç®—é‡ï¼ˆåäº¿FLOPsï¼‰\n")
    
    def collect_figures(self):
        """æ”¶é›†å¯è§†åŒ–å›¾è¡¨"""
        self.logger.info("Collecting figures...")
        
        # æŸ¥æ‰¾runsç›®å½•ä¸­çš„å›¾è¡¨æ–‡ä»¶
        runs_dir = Path('runs')
        if not runs_dir.exists():
            self.logger.warning("No runs directory found")
            return
        
        figure_info = []
        
        # éå†æ‰€æœ‰å®éªŒç›®å½•
        for exp_dir in runs_dir.iterdir():
            if not exp_dir.is_dir():
                continue
            
            # æŸ¥æ‰¾å›¾è¡¨æ–‡ä»¶
            fig_patterns = ['*.png', '*.jpg', '*.jpeg', '*.pdf', '*.svg']
            for pattern in fig_patterns:
                for fig_file in exp_dir.rglob(pattern):
                    # å¤åˆ¶å›¾è¡¨æ–‡ä»¶
                    dest_path = self.dirs['figs'] / f"{exp_dir.name}_{fig_file.name}"
                    shutil.copy2(fig_file, dest_path)
                    
                    figure_info.append({
                        'experiment': exp_dir.name,
                        'figure': fig_file.name,
                        'path': str(dest_path.relative_to(self.output_dir)),
                        'type': fig_file.suffix[1:],
                        'size_kb': fig_file.stat().st_size / 1024
                    })
        
        # ä¿å­˜å›¾è¡¨ä¿¡æ¯
        if figure_info:
            fig_info_path = self.dirs['figs'] / 'figure_info.json'
            with open(fig_info_path, 'w') as f:
                json.dump(figure_info, f, indent=2)
        
        self.logger.info(f"Collected {len(figure_info)} figures")
    
    def generate_reproduction_scripts(self):
        """ç”Ÿæˆå¤ç°è„šæœ¬"""
        self.logger.info("Generating reproduction scripts...")
        
        # ç”Ÿæˆä¸»å¤ç°è„šæœ¬
        main_script_path = self.dirs['scripts'] / 'reproduce_all.py'
        
        with open(main_script_path, 'w') as f:
            f.write('''#!/usr/bin/env python3
"""
PDEBenchç¨€ç–è§‚æµ‹é‡å»ºå®éªŒå¤ç°è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python reproduce_all.py --config config_merged.yaml --seed 42

è¦æ±‚:
    - Python 3.10+
    - PyTorch >= 2.1
    - æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path

def run_command(cmd, cwd=None):
    """è¿è¡Œå‘½ä»¤å¹¶æ‰“å°è¾“å‡º"""
    print(f"Running: {cmd}")
    result = subprocess.run(cmd, shell=True, cwd=cwd, capture_output=True, text=True)
    
    if result.stdout:
        print(result.stdout)
    if result.stderr:
        print(result.stderr, file=sys.stderr)
    
    return result.returncode == 0

def main():
    parser = argparse.ArgumentParser(description='Reproduce PDEBench experiments')
    parser.add_argument('--config', required=True, help='Config file path')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--gpu', type=int, default=0, help='GPU device')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)
    
    # é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    
    print("=== PDEBenchå®éªŒå¤ç° ===")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"éšæœºç§å­: {args.seed}")
    print(f"GPUè®¾å¤‡: {args.gpu}")
    print()
    
    # 1. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    print("1. è¿è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥...")
    cmd = f"python tools/check_dc_equivalence.py --config-name consistency_check seed={args.seed}"
    if not run_command(cmd, cwd=project_root):
        print("æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼")
        return 1
    
    # 2. è®­ç»ƒæ¨¡å‹
    print("2. å¼€å§‹è®­ç»ƒ...")
    cmd = f"python tools/train.py --config-path ../configs --config-name {Path(args.config).stem} seed={args.seed}"
    if not run_command(cmd, cwd=project_root):
        print("è®­ç»ƒå¤±è´¥ï¼")
        return 1
    
    # 3. è¯„ä¼°æ¨¡å‹
    print("3. å¼€å§‹è¯„ä¼°...")
    cmd = f"python tools/eval.py --config-path ../configs --config-name {Path(args.config).stem} seed={args.seed}"
    if not run_command(cmd, cwd=project_root):
        print("è¯„ä¼°å¤±è´¥ï¼")
        return 1
    
    print("=== å®éªŒå¤ç°å®Œæˆ ===")
    return 0

if __name__ == "__main__":
    exit(main())
''')
        
        # ç”Ÿæˆç¯å¢ƒå®‰è£…è„šæœ¬
        env_script_path = self.dirs['scripts'] / 'setup_environment.py'
        
        with open(env_script_path, 'w') as f:
            f.write('''#!/usr/bin/env python3
"""
PDEBenchç¯å¢ƒå®‰è£…è„šæœ¬
"""

import subprocess
import sys

def install_requirements():
    """å®‰è£…ä¾èµ–åŒ…"""
    requirements = [
        "torch>=2.1.0",
        "torchvision",
        "numpy",
        "scipy",
        "matplotlib",
        "seaborn",
        "pandas",
        "scikit-learn",
        "hydra-core",
        "omegaconf",
        "tensorboard",
        "tqdm",
        "h5py",
        "netcdf4",
        "xarray"
    ]
    
    for req in requirements:
        print(f"Installing {req}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", req])

if __name__ == "__main__":
    install_requirements()
    print("Environment setup completed!")
''')
        
        # è®¾ç½®è„šæœ¬å¯æ‰§è¡Œæƒé™
        main_script_path.chmod(0o755)
        env_script_path.chmod(0o755)
        
        self.logger.info(f"Reproduction scripts saved to {self.dirs['scripts']}")
    
    def generate_readme(self):
        """ç”ŸæˆREADMEæ–‡ä»¶"""
        self.logger.info("Generating README...")
        
        readme_path = self.output_dir / 'README.md'
        
        with open(readme_path, 'w') as f:
            f.write(f"""# PDEBenchç¨€ç–è§‚æµ‹é‡å»ºè®ºæ–‡ææ–™åŒ…

æœ¬ææ–™åŒ…åŒ…å«äº†PDEBenchç¨€ç–è§‚æµ‹é‡å»ºå®éªŒçš„å®Œæ•´ææ–™ï¼Œæ”¯æŒè®ºæ–‡å®¡é˜…å’Œç»“æœå¤ç°ã€‚

## ç›®å½•ç»“æ„

```
paper_package/
â”œâ”€â”€ data_cards/          # æ•°æ®å¡ç‰‡ï¼ˆæ¥æº/è®¸å¯/åˆ‡åˆ†ï¼‰
â”œâ”€â”€ configs/            # é…ç½®å¿«ç…§ï¼ˆæœ€ç»ˆYAMLï¼‰
â”œâ”€â”€ checkpoints/        # æ¨¡å‹æƒé‡ï¼ˆå…³é”®ckptï¼‰
â”œâ”€â”€ metrics/           # æŒ‡æ ‡æ±‡æ€»ï¼ˆä¸»è¡¨/æ˜¾è‘—æ€§/CSVï¼‰
â”œâ”€â”€ figs/              # å¯è§†åŒ–å›¾è¡¨ï¼ˆä»£è¡¨å›¾/å¤±è´¥æ¡ˆä¾‹/è°±å›¾ï¼‰
â”œâ”€â”€ scripts/           # å¤ç°è„šæœ¬ï¼ˆä¸€é”®å¤ç°ä¸æ±‡æ€»ï¼‰
â”œâ”€â”€ logs/              # ç”Ÿæˆæ—¥å¿—
â””â”€â”€ README.md          # æœ¬æ–‡ä»¶
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…Pythonä¾èµ–
python scripts/setup_environment.py

# æˆ–æ‰‹åŠ¨å®‰è£…
pip install torch>=2.1.0 torchvision numpy scipy matplotlib seaborn pandas scikit-learn hydra-core omegaconf tensorboard tqdm h5py netcdf4 xarray
```

### 2. æ•°æ®å‡†å¤‡

è¯·ä»ä»¥ä¸‹æ¥æºä¸‹è½½PDEBenchæ•°æ®é›†ï¼š
- å®˜æ–¹ç½‘ç«™: https://github.com/pdebench/PDEBench
- æ•°æ®è¯¦æƒ…: å‚è§ `data_cards/dataset_card.md`

### 3. å¤ç°å®éªŒ

```bash
# ä¸€é”®å¤ç°æ‰€æœ‰å®éªŒ
python scripts/reproduce_all.py --config configs/config_merged.yaml --seed 42

# æˆ–åˆ†æ­¥æ‰§è¡Œ
python tools/check_dc_equivalence.py  # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
python tools/train.py                 # è®­ç»ƒæ¨¡å‹
python tools/eval.py                  # è¯„ä¼°æ¨¡å‹
```

## ä¸»è¦ç»“æœ

è¯¦ç»†ç»“æœè¯·å‚è§ï¼š
- `metrics/main_results.md` - ä¸»è¦å®éªŒç»“æœè¡¨æ ¼
- `metrics/main_results.csv` - CSVæ ¼å¼ç»“æœ
- `figs/` - å¯è§†åŒ–å›¾è¡¨

## é»„é‡‘æ³•åˆ™éªŒè¯

æœ¬å®éªŒä¸¥æ ¼éµå¾ªä»¥ä¸‹é»„é‡‘æ³•åˆ™ï¼š

1. **ä¸€è‡´æ€§ä¼˜å…ˆ**: è§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCå¤ç”¨åŒä¸€å®ç°ä¸é…ç½®
2. **å¯å¤ç°**: åŒä¸€YAML+ç§å­ï¼ŒéªŒè¯æŒ‡æ ‡æ–¹å·®â‰¤1e-4
3. **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ¨¡å‹forward(x[B,C_in,H,W])â†’y[B,C_out,H,W]
4. **å¯æ¯”æ€§**: æŠ¥å‘Šå‡å€¼Â±æ ‡å‡†å·®ï¼ˆâ‰¥3ç§å­ï¼‰+èµ„æºæˆæœ¬
5. **æ–‡æ¡£å…ˆè¡Œ**: å®Œæ•´çš„æ–‡æ¡£å’Œæµ‹è¯•è¦†ç›–

## éªŒè¯æ¸…å•

- [ ] æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ï¼ˆMSE < 1e-8ï¼‰
- [ ] è®­ç»ƒè„šæœ¬è¿è¡ŒæˆåŠŸ
- [ ] è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ­£ç¡®
- [ ] å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæ•´
- [ ] å¤ç°è„šæœ¬å¯æ‰§è¡Œ

## æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. Pythonç‰ˆæœ¬ >= 3.10
2. PyTorchç‰ˆæœ¬ >= 2.1
3. CUDAç¯å¢ƒé…ç½®æ­£ç¡®
4. æ•°æ®è·¯å¾„è®¾ç½®æ­£ç¡®

## è®¸å¯è¯

æœ¬ææ–™åŒ…éµå¾ªMITè®¸å¯è¯ï¼Œæ•°æ®é›†è¯·éµå¾ªå„è‡ªçš„è®¸å¯è¯è¦æ±‚ã€‚

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬ææ–™åŒ…ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{{pdebench_sparse_reconstruction,
  title={{PDEBenchç¨€ç–è§‚æµ‹é‡å»ºï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç³»ç»Ÿæ€§è¯„ä¼°}},
  author={{ä½œè€…å§“å}},
  journal={{æœŸåˆŠåç§°}},
  year={{2024}}
}}
```

---

ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}
ç”Ÿæˆå™¨ç‰ˆæœ¬: 1.0
""")
        
        self.logger.info(f"README saved to {readme_path}")
    
    def generate_package(self):
        """ç”Ÿæˆå®Œæ•´çš„è®ºæ–‡ææ–™åŒ…"""
        self.logger.info("Starting paper package generation...")
        
        try:
            # 1. ç”Ÿæˆæ•°æ®å¡ç‰‡
            self.generate_data_cards()
            
            # 2. æ”¶é›†é…ç½®å¿«ç…§
            self.collect_config_snapshots()
            
            # 3. æ”¶é›†æ¨¡å‹æƒé‡
            self.collect_checkpoints()
            
            # 4. æ”¶é›†æŒ‡æ ‡æ±‡æ€»
            self.collect_metrics()
            
            # 5. æ”¶é›†å¯è§†åŒ–å›¾è¡¨
            self.collect_figures()
            
            # 6. ç”Ÿæˆå¤ç°è„šæœ¬
            self.generate_reproduction_scripts()
            
            # 7. ç”ŸæˆREADME
            self.generate_readme()
            
            # 8. ç”Ÿæˆå…ƒä¿¡æ¯
            self.generate_meta_info()
            
            self.logger.info("Paper package generation completed successfully!")
            self.logger.info(f"Package saved to: {self.output_dir}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Paper package generation failed: {e}")
            return False
    
    def generate_meta_info(self):
        """ç”Ÿæˆå…ƒä¿¡æ¯æ–‡ä»¶"""
        meta_info = {
            'package_info': {
                'name': 'PDEBenchç¨€ç–è§‚æµ‹é‡å»ºè®ºæ–‡ææ–™åŒ…',
                'version': '1.0',
                'generated_at': datetime.now().isoformat(),
                'generator': 'PaperPackageGenerator',
                'total_size_mb': self.calculate_package_size()
            },
            'contents': {
                'data_cards': len(list(self.dirs['data_cards'].glob('*'))),
                'configs': len(list(self.dirs['configs'].glob('*'))),
                'checkpoints': len(list(self.dirs['checkpoints'].glob('*.ckpt'))),
                'metrics': len(list(self.dirs['metrics'].glob('*'))),
                'figures': len(list(self.dirs['figs'].glob('*'))),
                'scripts': len(list(self.dirs['scripts'].glob('*')))
            },
            'environment': self.env_info,
            'git_info': self.get_git_info()
        }
        
        meta_path = self.output_dir / 'package_meta.json'
        with open(meta_path, 'w') as f:
            json.dump(meta_info, f, indent=2)
    
    def calculate_package_size(self) -> float:
        """è®¡ç®—ææ–™åŒ…æ€»å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        for root, dirs, files in os.walk(self.output_dir):
            for file in files:
                file_path = Path(root) / file
                total_size += file_path.stat().st_size
        
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB


@hydra.main(version_base=None, config_path="../configs", config_name="paper_package")
def main(cfg: DictConfig) -> None:
    """ä¸»ç”Ÿæˆå‡½æ•°
    
    Args:
        cfg: é…ç½®å¯¹è±¡
    """
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(cfg.get('output_dir', 'paper_package'))
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = PaperPackageGenerator(cfg, output_dir)
    
    # ç”Ÿæˆææ–™åŒ…
    success = generator.generate_package()
    
    if success:
        print(f"\nâœ… è®ºæ–‡ææ–™åŒ…ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
        print(f"ğŸ“Š åŒ…å¤§å°: {generator.calculate_package_size():.1f} MB")
        print(f"\nğŸ“‹ å†…å®¹æ¸…å•:")
        for name, path in generator.dirs.items():
            if name != 'root':
                count = len(list(path.glob('*')))
                print(f"   - {name}: {count} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
        print(f"   cd {output_dir}")
        print(f"   python scripts/reproduce_all.py --config configs/config_merged.yaml")
        
        return 0
    else:
        print("âŒ è®ºæ–‡ææ–™åŒ…ç”Ÿæˆå¤±è´¥ï¼")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)