#!/usr/bin/env python3
"""
æ‰¹é‡è®­ç»ƒè„šæœ¬ - PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ
æ”¯æŒè‡ªåŠ¨åŒ–è®­ç»ƒæ‰€æœ‰å·²éªŒè¯çš„æ¨¡å‹

éµå¾ªé»„é‡‘æ³•åˆ™ï¼š
1. ä¸€è‡´æ€§ä¼˜å…ˆï¼šæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®é…ç½®å’Œè®­ç»ƒå‚æ•°
2. å¯å¤ç°ï¼šå›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
3. ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰æ¨¡å‹éµå¾ªç»Ÿä¸€çš„è®­ç»ƒæ¥å£
4. å¯æ¯”æ€§ï¼šè®°å½•è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡å’Œèµ„æºæ¶ˆè€—
5. æ–‡æ¡£å…ˆè¡Œï¼šç”Ÿæˆå®Œæ•´çš„è®­ç»ƒæŠ¥å‘Šå’Œæ—¥å¿—
"""

import os
import sys
import time
import json
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

# è®¾ç½®UTF-8ç¼–ç ï¼Œè§£å†³Windowsä¸‹çš„ç¼–ç é—®é¢˜
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.platform.startswith('win'):
    # Windowsç³»ç»Ÿä¸‹è®¾ç½®æ§åˆ¶å°ç¼–ç 
    import locale
    try:
        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
    except locale.Error:
        try:
            locale.setlocale(locale.LC_ALL, 'C.UTF-8')
        except locale.Error:
            pass  # å¦‚æœéƒ½å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ

import yaml
import torch
# import pandas as pd  # ç§»é™¤pandasä¾èµ–

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class BatchTrainer:
    """æ‰¹é‡è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, 
                 data_config: str = "pdebench",
                 base_epochs: int = 15,
                 base_batch_size: int = 2,
                 seeds: List[int] = [2025, 2026, 2027],
                 output_dir: str = "runs"):
        """
        åˆå§‹åŒ–æ‰¹é‡è®­ç»ƒå™¨
        
        Args:
            data_config: æ•°æ®é…ç½®åç§°
            base_epochs: åŸºç¡€è®­ç»ƒè½®æ•°
            base_batch_size: åŸºç¡€æ‰¹æ¬¡å¤§å°
            seeds: éšæœºç§å­åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        self.data_config = data_config
        self.base_epochs = base_epochs
        self.base_batch_size = base_batch_size
        self.seeds = seeds
        self.output_dir = Path(output_dir)
        
        # æ‰€æœ‰å·²éªŒè¯çš„æ¨¡å‹åˆ—è¡¨
        self.models = [
            "unet", "unet_plus_plus", "fno2d", "ufno_unet",
            "segformer_unetformer", "unetformer", "mlp", "mlp_mixer",
            "liif", "hybrid", "segformer", "swin_unet"
        ]
        
        # æ¨¡å‹ç‰¹å®šé…ç½®
        self.model_configs = {
            "fno2d": {"batch_size": 4, "epochs": 20},
            "hybrid": {"batch_size": 2, "epochs": 18},
            "segformer": {"batch_size": 2, "epochs": 18},
            "segformer_unetformer": {"batch_size": 2, "epochs": 18},
            "swin_unet": {"batch_size": 2, "epochs": 18},
            "liif": {"batch_size": 4, "epochs": 20},
        }
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # è®­ç»ƒç»“æœå­˜å‚¨
        self.results = []
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = self.output_dir / "batch_training_logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"batch_train_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def get_model_config(self, model_name: str) -> Dict:
        """è·å–æ¨¡å‹ç‰¹å®šé…ç½®"""
        config = self.model_configs.get(model_name, {})
        return {
            "batch_size": config.get("batch_size", self.base_batch_size),
            "epochs": config.get("epochs", self.base_epochs)
        }
        
    def generate_experiment_name(self, model_name: str, seed: int) -> str:
        """ç”Ÿæˆå®éªŒåç§°"""
        timestamp = datetime.now().strftime("%Y%m%d")
        return f"SRx4-DarcyFlow-128-{model_name.upper()}-batch-s{seed}-{timestamp}"
        
    def train_single_model(self, model_name: str, seed: int) -> Dict:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        self.logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}, ç§å­: {seed}")
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self.get_model_config(model_name)
        exp_name = self.generate_experiment_name(model_name, seed)
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤ - ä¿®å¤Hydraé…ç½®è¦†ç›–é—®é¢˜
        cmd = [
            sys.executable, "train.py",
            f"data={self.data_config}",  # ç›´æ¥è¦†ç›–é»˜è®¤é…ç½®
            f"model={model_name}",  # ç›´æ¥è¦†ç›–é»˜è®¤é…ç½®
            f"train.epochs={model_config['epochs']}",  # ä½¿ç”¨train.epochsè€Œä¸æ˜¯trainer.max_epochs
            f"data.batch_size={model_config['batch_size']}",
            f"experiment.seed={seed}",  # ä½¿ç”¨experiment.seed
            f"experiment.name={exp_name}",  # ä½¿ç”¨experiment.name
        ]
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # æ‰§è¡Œè®­ç»ƒ
            result = subprocess.run(
                cmd,
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            train_time = time.time() - start_time
            
            if result.returncode == 0:
                self.logger.info(f"æ¨¡å‹ {model_name} (ç§å­ {seed}) è®­ç»ƒæˆåŠŸï¼Œè€—æ—¶ {train_time:.2f}s")
                
                # è§£æè®­ç»ƒç»“æœ
                metrics = self.parse_training_results(exp_name)
                
                return {
                    "model": model_name,
                    "seed": seed,
                    "exp_name": exp_name,
                    "status": "success",
                    "train_time": train_time,
                    "epochs": model_config['epochs'],
                    "batch_size": model_config['batch_size'],
                    **metrics
                }
            else:
                self.logger.error(f"æ¨¡å‹ {model_name} (ç§å­ {seed}) è®­ç»ƒå¤±è´¥")
                self.logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                
                return {
                    "model": model_name,
                    "seed": seed,
                    "exp_name": exp_name,
                    "status": "failed",
                    "train_time": train_time,
                    "error": result.stderr[:500]  # æˆªå–å‰500å­—ç¬¦
                }
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"æ¨¡å‹ {model_name} (ç§å­ {seed}) è®­ç»ƒè¶…æ—¶")
            return {
                "model": model_name,
                "seed": seed,
                "exp_name": exp_name,
                "status": "timeout",
                "train_time": 3600
            }
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ {model_name} (ç§å­ {seed}) è®­ç»ƒå¼‚å¸¸: {str(e)}")
            return {
                "model": model_name,
                "seed": seed,
                "exp_name": exp_name,
                "status": "error",
                "error": str(e)
            }
            
    def parse_training_results(self, exp_name: str) -> Dict:
        """è§£æè®­ç»ƒç»“æœ"""
        exp_dir = self.output_dir / exp_name
        
        # é»˜è®¤æŒ‡æ ‡
        metrics = {
            "rel_l2": None,
            "mae": None,
            "psnr": None,
            "ssim": None,
            "params": None,
            "flops": None,
            "memory": None
        }
        
        try:
            # å°è¯•è¯»å–æŒ‡æ ‡æ–‡ä»¶
            metrics_file = exp_dir / "metrics.json"
            if metrics_file.exists():
                with open(metrics_file, 'r') as f:
                    data = json.load(f)
                    metrics.update(data)
                    
            # å°è¯•è¯»å–æ—¥å¿—æ–‡ä»¶è·å–æ›´å¤šä¿¡æ¯
            log_file = exp_dir / "train.log"
            if log_file.exists():
                with open(log_file, 'r') as f:
                    log_content = f.read()
                    
                # è§£æå‚æ•°é‡å’ŒFLOPs
                if "Total params:" in log_content:
                    import re
                    params_match = re.search(r"Total params: ([\d,]+)", log_content)
                    if params_match:
                        metrics["params"] = int(params_match.group(1).replace(',', ''))
                        
        except Exception as e:
            self.logger.warning(f"è§£æå®éªŒ {exp_name} ç»“æœæ—¶å‡ºé”™: {str(e)}")
            
        return metrics
        
    def run_batch_training(self, models: Optional[List[str]] = None):
        """æ‰§è¡Œæ‰¹é‡è®­ç»ƒ"""
        if models is None:
            models = self.models
            
        self.logger.info(f"å¼€å§‹æ‰¹é‡è®­ç»ƒï¼Œæ¨¡å‹æ•°é‡: {len(models)}, ç§å­æ•°é‡: {len(self.seeds)}")
        self.logger.info(f"æ€»è®­ç»ƒä»»åŠ¡æ•°: {len(models) * len(self.seeds)}")
        
        total_start_time = time.time()
        
        for model_name in models:
            for seed in self.seeds:
                result = self.train_single_model(model_name, seed)
                self.results.append(result)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self.save_intermediate_results()
                
        total_time = time.time() - total_start_time
        self.logger.info(f"æ‰¹é‡è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report()
        
    def save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        results_file = self.output_dir / "batch_training_logs" / "intermediate_results.json"
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
            
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæŠ¥å‘Š...")
        
        # ä¿å­˜è¯¦ç»†ç»“æœä¸ºJSONæ ¼å¼
        results_file = self.output_dir / "batch_training_logs" / "batch_training_results.json"
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        summary = self.generate_summary_statistics(self.results)
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = self.output_dir / "batch_training_logs" / "training_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
            
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(self.results, summary)
        
        self.logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir / 'batch_training_logs'}")
        
    def generate_summary_statistics(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        total_experiments = len(results)
        successful_experiments = len([r for r in results if r['status'] == 'success'])
        failed_experiments = len([r for r in results if r['status'] == 'failed'])
        timeout_experiments = len([r for r in results if r['status'] == 'timeout'])
        
        summary = {
            "total_experiments": total_experiments,
            "successful_experiments": successful_experiments,
            "failed_experiments": failed_experiments,
            "timeout_experiments": timeout_experiments,
            "success_rate": successful_experiments / total_experiments * 100 if total_experiments > 0 else 0,
            "total_training_time": sum(r.get('train_time', 0) for r in results),
            "average_training_time": sum(r.get('train_time', 0) for r in results) / total_experiments if total_experiments > 0 else 0,
            "models_trained": len(set(r['model'] for r in results)),
            "seeds_used": len(set(r['seed'] for r in results))
        }
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        model_stats = {}
        models = set(r['model'] for r in results)
        
        for model in models:
            model_results = [r for r in results if r['model'] == model]
            successful_model_results = [r for r in model_results if r['status'] == 'success']
            
            model_stats[model] = {
                "total_runs": len(model_results),
                "successful_runs": len(successful_model_results),
                "success_rate": len(successful_model_results) / len(model_results) * 100 if len(model_results) > 0 else 0,
                "avg_train_time": sum(r.get('train_time', 0) for r in model_results) / len(model_results) if len(model_results) > 0 else 0,
                "avg_rel_l2": sum(r.get('rel_l2', 0) for r in successful_model_results if r.get('rel_l2') is not None) / len(successful_model_results) if len(successful_model_results) > 0 else None,
                "avg_psnr": sum(r.get('psnr', 0) for r in successful_model_results if r.get('psnr') is not None) / len(successful_model_results) if len(successful_model_results) > 0 else None
            }
            
        summary["model_statistics"] = model_stats
        
        return summary
        
    def generate_markdown_report(self, results: List[Dict], summary: Dict):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""# PDEBenchæ‰¹é‡è®­ç»ƒæŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **ç”Ÿæˆæ—¶é—´**: {timestamp}
- **æ•°æ®é…ç½®**: {self.data_config}
- **è®­ç»ƒè½®æ•°**: {self.base_epochs}
- **éšæœºç§å­**: {self.seeds}

## è®­ç»ƒæ¦‚è§ˆ

- **æ€»å®éªŒæ•°**: {summary['total_experiments']}
- **æˆåŠŸå®éªŒæ•°**: {summary['successful_experiments']}
- **å¤±è´¥å®éªŒæ•°**: {summary['failed_experiments']}
- **è¶…æ—¶å®éªŒæ•°**: {summary['timeout_experiments']}
- **æˆåŠŸç‡**: {summary['success_rate']:.1f}%
- **æ€»è®­ç»ƒæ—¶é—´**: {summary['total_training_time']:.2f}s ({summary['total_training_time']/3600:.2f}h)
- **å¹³å‡è®­ç»ƒæ—¶é—´**: {summary['average_training_time']:.2f}s

## æ¨¡å‹æ€§èƒ½ç»Ÿè®¡

| æ¨¡å‹ | è¿è¡Œæ¬¡æ•° | æˆåŠŸæ¬¡æ•° | æˆåŠŸç‡ | å¹³å‡è®­ç»ƒæ—¶é—´(s) | å¹³å‡Rel-L2 | å¹³å‡PSNR |
|------|----------|----------|--------|----------------|------------|----------|
"""
        
        for model, stats in summary['model_statistics'].items():
            rel_l2 = f"{stats['avg_rel_l2']:.4f}" if stats['avg_rel_l2'] else "N/A"
            psnr = f"{stats['avg_psnr']:.2f}" if stats['avg_psnr'] else "N/A"
            
            report += f"| {model} | {stats['total_runs']} | {stats['successful_runs']} | {stats['success_rate']:.1f}% | {stats['avg_train_time']:.2f} | {rel_l2} | {psnr} |\n"
            
        report += f"""

## è¯¦ç»†ç»“æœ

è¯¦ç»†çš„è®­ç»ƒç»“æœè¯·æŸ¥çœ‹: `batch_training_results.json`

## å¤±è´¥åˆ†æ

"""
        
        # æ·»åŠ å¤±è´¥åˆ†æ
        failed_results = [r for r in results if r['status'] != 'success']
        if len(failed_results) > 0:
            report += "### å¤±è´¥å®éªŒåˆ—è¡¨\n\n"
            for result in failed_results:
                report += f"- **{result['model']}** (ç§å­ {result['seed']}): {result['status']}\n"
                if 'error' in result and result['error']:
                    report += f"  - é”™è¯¯ä¿¡æ¯: {result['error'][:100]}...\n"
        else:
            report += "ğŸ‰ æ‰€æœ‰å®éªŒéƒ½æˆåŠŸå®Œæˆï¼\n"
            
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "batch_training_logs" / "training_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="PDEBenchæ‰¹é‡è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data", default="pdebench", help="æ•°æ®é…ç½®åç§°")
    parser.add_argument("--epochs", type=int, default=15, help="åŸºç¡€è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=2, help="åŸºç¡€æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--seeds", nargs="+", type=int, default=[2025, 2026, 2027], help="éšæœºç§å­åˆ—è¡¨")
    parser.add_argument("--models", nargs="+", help="æŒ‡å®šè¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--output-dir", default="runs", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡è®­ç»ƒå™¨
    trainer = BatchTrainer(
        data_config=args.data,
        base_epochs=args.epochs,
        base_batch_size=args.batch_size,
        seeds=args.seeds,
        output_dir=args.output_dir
    )
    
    # æ‰§è¡Œæ‰¹é‡è®­ç»ƒ
    trainer.run_batch_training(models=args.models)


if __name__ == "__main__":
    main()