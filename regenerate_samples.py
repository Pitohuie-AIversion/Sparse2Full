#!/usr/bin/env python3
"""
é‡æ–°ç”Ÿæˆè®­ç»ƒæ ·æœ¬çš„å¯è§†åŒ–ç»“æœ
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging
from datetime import datetime
import yaml
import hydra
from omegaconf import DictConfig, OmegaConf
from torch.utils.data import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from datasets.pde_bench import PDEBenchDataset
from utils.visualization import PDEBenchVisualizer
from models import get_model

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SampleRegenerator:
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.samples_dir = self.base_dir / "runs" / "samples"
        self.batch_dir = self.base_dir / "runs" / "batch_retrain_20251015_032934"
        
        # åˆ›å»ºsamplesç›®å½•ç»“æ„
        self.samples_dir.mkdir(parents=True, exist_ok=True)
        
    def load_config(self, config_path: str) -> DictConfig:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return OmegaConf.create(config)
    
    def get_best_models(self) -> list:
        """è·å–æœ€ä½³æ¨¡å‹åˆ—è¡¨"""
        # ä»CSVæ–‡ä»¶è¯»å–æ¨¡å‹æ’å
        csv_path = self.batch_dir / "analysis" / "model_ranking.csv"
        if not csv_path.exists():
            logger.error(f"æ‰¾ä¸åˆ°æ¨¡å‹æ’åæ–‡ä»¶: {csv_path}")
            return []
        
        import pandas as pd
        df = pd.read_csv(csv_path)
        
        # é€‰æ‹©å‰3ä¸ªæœ€ä½³æ¨¡å‹
        best_models = []
        for _, row in df.head(3).iterrows():
            model_name = row['æ¨¡å‹']
            best_models.append({
                'name': model_name,
                'rel_l2': row['Rel-L2'],
                'checkpoint_dir': self.batch_dir / model_name
            })
        
        return best_models
    
    def create_dataset(self, config: DictConfig):
        """åˆ›å»ºæ•°æ®é›†"""
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        task_params = {}
        if config.data.observation.mode.lower() == 'sr':
            task_params = {
                'scale_factor': config.data.observation.sr.get('scale_factor', 4),
                'blur_sigma': config.data.observation.sr.get('blur_sigma', 1.0),
                'blur_kernel_size': config.data.observation.sr.get('blur_kernel_size', 5),
                'boundary_mode': config.data.observation.sr.get('boundary_mode', 'mirror')
            }
        elif config.data.observation.mode.lower() == 'crop':
            task_params = {
                'crop_size': config.data.observation.crop.get('crop_size', [64, 64]),
                'crop_strategy': config.data.observation.crop.get('crop_strategy', 'uniform'),
                'boundary_mode': config.data.observation.crop.get('boundary_mode', 'mirror')
            }
        
        dataset = PDEBenchDataset(
            data_root=config.data.data_path,
            split='val',  # ä½¿ç”¨éªŒè¯é›†
            task=config.data.observation.mode,
            task_params=task_params,
            img_size=config.data.get('image_size', 128),
            normalize=config.data.preprocessing.get('normalize', True),
            cache_data=False
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=4,  # å°æ‰¹æ¬¡ç”¨äºå¯è§†åŒ–
            shuffle=False,
            num_workers=0
        )
        
        return dataset, dataloader
    
    def load_model_checkpoint(self, model_config: DictConfig, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        # åˆ›å»ºæ¨¡å‹
        model = get_model(model_config)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            logger.info(f"å·²åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        else:
            logger.warning(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return None
        
        model.eval()
        return model
    
    def generate_sample_visualizations(self, model_info: dict, config: DictConfig, 
                                     dataset, dataloader, epoch: int = 0):
        """ç”Ÿæˆæ ·æœ¬å¯è§†åŒ–"""
        model_name = model_info['name']
        checkpoint_dir = model_info['checkpoint_dir']
        
        # æŸ¥æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹
        checkpoint_files = list(checkpoint_dir.glob("*.pth"))
        if not checkpoint_files:
            logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_dir}")
            return
        
        # é€‰æ‹©æœ€æ–°çš„æ£€æŸ¥ç‚¹
        checkpoint_path = max(checkpoint_files, key=os.path.getctime)
        
        # åŠ è½½æ¨¡å‹
        model = self.load_model_checkpoint(config.model, str(checkpoint_path))
        if model is None:
            return
        
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = PDEBenchVisualizer(
            task_type=config.data.task_type,
            save_dir=str(self.samples_dir / f"epoch_{epoch:04d}"),
            norm_stats=dataset.norm_stats if hasattr(dataset, 'norm_stats') else None
        )
        
        # ç”Ÿæˆå¯è§†åŒ–
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= 5:  # åªç”Ÿæˆå‰5ä¸ªæ‰¹æ¬¡
                    break
                
                # å‡†å¤‡è¾“å…¥
                if isinstance(batch, dict):
                    x = batch['input'].to(device)
                    y_true = batch['target'].to(device)
                else:
                    x, y_true = batch
                    x, y_true = x.to(device), y_true.to(device)
                
                # æ¨¡å‹é¢„æµ‹
                y_pred = model(x)
                
                # ä¿å­˜å¯è§†åŒ–
                for i in range(min(2, x.size(0))):  # æ¯ä¸ªæ‰¹æ¬¡ä¿å­˜2ä¸ªæ ·æœ¬
                    sample_idx = batch_idx * 2 + i
                    
                    # è½¬æ¢ä¸ºnumpy
                    x_np = x[i].cpu().numpy()
                    y_true_np = y_true[i].cpu().numpy()
                    y_pred_np = y_pred[i].cpu().numpy()
                    
                    # ç”Ÿæˆå¯¹æ¯”å›¾
                    visualizer.plot_field_comparison(
                        x_np, y_true_np, y_pred_np,
                        title=f"{model_name}_sample_{sample_idx:03d}",
                        save_name=f"{model_name}_sample_{sample_idx:03d}.png"
                    )
                    
                    logger.info(f"å·²ç”Ÿæˆæ ·æœ¬å¯è§†åŒ–: {model_name}_sample_{sample_idx:03d}")
    
    def regenerate_all_samples(self):
        """é‡æ–°ç”Ÿæˆæ‰€æœ‰æ ·æœ¬å¯è§†åŒ–"""
        logger.info("å¼€å§‹é‡æ–°ç”Ÿæˆè®­ç»ƒæ ·æœ¬å¯è§†åŒ–...")
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_models = self.get_best_models()
        if not best_models:
            logger.error("æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ä¿¡æ¯")
            return
        
        # åŠ è½½é…ç½®
        config_path = self.base_dir / "configs" / "train.yaml"
        if not config_path.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return
        
        config = self.load_config(str(config_path))
        
        # åˆ›å»ºæ•°æ®é›†
        try:
            dataset, dataloader = self.create_dataset(config)
            logger.info(f"æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
            return
        
        # ä¸ºæ¯ä¸ªepochç”Ÿæˆæ ·æœ¬
        epochs_to_generate = [0, 100]  # ç”Ÿæˆepoch 0å’Œ100çš„æ ·æœ¬
        
        for epoch in epochs_to_generate:
            epoch_dir = self.samples_dir / f"epoch_{epoch:04d}"
            epoch_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºå­ç›®å½•
            for subdir in ['analysis', 'comparisons', 'fields', 'spectra']:
                (epoch_dir / subdir).mkdir(exist_ok=True)
            
            logger.info(f"ç”Ÿæˆepoch {epoch}çš„æ ·æœ¬å¯è§†åŒ–...")
            
            # ä¸ºæ¯ä¸ªæœ€ä½³æ¨¡å‹ç”Ÿæˆæ ·æœ¬
            for model_info in best_models:
                try:
                    self.generate_sample_visualizations(
                        model_info, config, dataset, dataloader, epoch
                    )
                except Exception as e:
                    logger.error(f"ç”Ÿæˆæ¨¡å‹ {model_info['name']} çš„æ ·æœ¬å¤±è´¥: {e}")
                    continue
        
        logger.info("æ ·æœ¬å¯è§†åŒ–é‡æ–°ç”Ÿæˆå®Œæˆï¼")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(best_models)
    
    def generate_summary_report(self, best_models: list):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        summary_path = self.samples_dir / "regeneration_summary.md"
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# è®­ç»ƒæ ·æœ¬å¯è§†åŒ–é‡æ–°ç”ŸæˆæŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ç”Ÿæˆçš„æ¨¡å‹\n\n")
            for i, model_info in enumerate(best_models, 1):
                f.write(f"{i}. **{model_info['name']}** (Rel-L2: {model_info['rel_l2']:.6f})\n")
            
            f.write("\n## ç›®å½•ç»“æ„\n\n")
            f.write("```\n")
            f.write("runs/samples/\n")
            f.write("â”œâ”€â”€ epoch_0000/\n")
            f.write("â”‚   â”œâ”€â”€ analysis/\n")
            f.write("â”‚   â”œâ”€â”€ comparisons/\n")
            f.write("â”‚   â”œâ”€â”€ fields/\n")
            f.write("â”‚   â””â”€â”€ spectra/\n")
            f.write("â”œâ”€â”€ epoch_0100/\n")
            f.write("â”‚   â”œâ”€â”€ analysis/\n")
            f.write("â”‚   â”œâ”€â”€ comparisons/\n")
            f.write("â”‚   â”œâ”€â”€ fields/\n")
            f.write("â”‚   â””â”€â”€ spectra/\n")
            f.write("â””â”€â”€ regeneration_summary.md\n")
            f.write("```\n\n")
            
            f.write("## è¯´æ˜\n\n")
            f.write("- æ¯ä¸ªepochç›®å½•åŒ…å«å‰3ä¸ªæœ€ä½³æ¨¡å‹çš„æ ·æœ¬å¯è§†åŒ–\n")
            f.write("- æ¯ä¸ªæ¨¡å‹ç”Ÿæˆ10ä¸ªä»£è¡¨æ€§æ ·æœ¬\n")
            f.write("- å¯è§†åŒ–åŒ…æ‹¬è¾“å…¥ã€çœŸå®å€¼ã€é¢„æµ‹å€¼å’Œè¯¯å·®å¯¹æ¯”\n")
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")

def main():
    """ä¸»å‡½æ•°"""
    base_dir = "f:/Zhaoyang/Sparse2Full"
    
    regenerator = SampleRegenerator(base_dir)
    regenerator.regenerate_all_samples()
    
    print("\nğŸ‰ è®­ç»ƒæ ·æœ¬å¯è§†åŒ–é‡æ–°ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {regenerator.samples_dir}")

if __name__ == "__main__":
    main()