#!/usr/bin/env python3
"""
æ¨¡å‹é…ç½®éªŒè¯è„šæœ¬
éªŒè¯æ‰€æœ‰æ¨¡å‹é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§å’Œå‚æ•°ä¸€è‡´æ€§

ä½œè€…: PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ
æ—¥æœŸ: 2025-01-13
"""

import os
import sys
import yaml
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelConfigValidator:
    """æ¨¡å‹é…ç½®éªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.project_root = Path(__file__).parent
        self.model_configs_dir = self.project_root / "configs" / "model"
        self.base_config_path = self.project_root / "configs" / "train.yaml"
        
        # å®šä¹‰æ‰€æœ‰å¯ç”¨æ¨¡å‹
        self.available_models = [
            "unet",
            "unet_plus_plus", 
            "fno2d",
            "ufno_unet",
            "segformer",
            "unetformer",
            "segformer_unetformer",
            "mlp",
            "mlp_mixer",
            "liif",
            "swin_unet",
            "hybrid"
        ]
        
        # éªŒè¯ç»“æœ
        self.validation_results = {}
        
    def load_base_config(self) -> Dict[str, Any]:
        """åŠ è½½åŸºç¡€é…ç½®æ–‡ä»¶"""
        try:
            with open(self.base_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"æˆåŠŸåŠ è½½åŸºç¡€é…ç½®: {self.base_config_path}")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½åŸºç¡€é…ç½®å¤±è´¥: {e}")
            raise
    
    def load_model_config(self, model_name: str) -> Tuple[Dict[str, Any], bool]:
        """åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶
        
        Returns:
            Tuple[config, exists]: é…ç½®å­—å…¸å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨çš„æ ‡å¿—
        """
        model_config_path = self.model_configs_dir / f"{model_name}.yaml"
        
        if not model_config_path.exists():
            logger.warning(f"æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {model_config_path}")
            return {}, False
            
        try:
            with open(model_config_path, 'r', encoding='utf-8') as f:
                model_config = yaml.safe_load(f)
            return model_config, True
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥ {model_name}: {e}")
            return {}, False
    
    def validate_model_config_structure(self, model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯æ¨¡å‹é…ç½®ç»“æ„"""
        validation_result = {
            'model_name': model_name,
            'structure_valid': True,
            'missing_fields': [],
            'warnings': [],
            'errors': []
        }
        
        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ['name', 'params']
        
        for field in required_fields:
            if field not in config:
                validation_result['missing_fields'].append(field)
                validation_result['structure_valid'] = False
                validation_result['errors'].append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # æ£€æŸ¥æ¨¡å‹åç§°ä¸€è‡´æ€§
        if 'name' in config and config['name'] != model_name:
            validation_result['warnings'].append(
                f"é…ç½®ä¸­çš„æ¨¡å‹åç§° '{config['name']}' ä¸æ–‡ä»¶å '{model_name}' ä¸ä¸€è‡´"
            )
        
        # æ£€æŸ¥å‚æ•°ç»“æ„
        if 'params' in config:
            params = config['params']
            
            # æ£€æŸ¥åŸºæœ¬å‚æ•°
            basic_params = ['in_channels', 'out_channels']
            for param in basic_params:
                if param not in params:
                    validation_result['warnings'].append(f"å»ºè®®æ·»åŠ å‚æ•°: {param}")
            
            # æ£€æŸ¥å›¾åƒå°ºå¯¸å‚æ•°
            if 'img_size' not in params and 'image_size' not in params:
                validation_result['warnings'].append("å»ºè®®æ·»åŠ å›¾åƒå°ºå¯¸å‚æ•°: img_size æˆ– image_size")
        
        return validation_result
    
    def validate_parameter_consistency(self, model_configs: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯æ¨¡å‹é—´å‚æ•°ä¸€è‡´æ€§"""
        consistency_result = {
            'consistent': True,
            'inconsistencies': [],
            'recommendations': []
        }
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„åŸºæœ¬å‚æ•°
        common_params = {}
        
        for model_name, config in model_configs.items():
            if 'params' in config:
                params = config['params']
                
                # æ£€æŸ¥è¾“å…¥è¾“å‡ºé€šé“æ•°
                if 'in_channels' in params:
                    if 'in_channels' not in common_params:
                        common_params['in_channels'] = {}
                    common_params['in_channels'][model_name] = params['in_channels']
                
                if 'out_channels' in params:
                    if 'out_channels' not in common_params:
                        common_params['out_channels'] = {}
                    common_params['out_channels'][model_name] = params['out_channels']
                
                # æ£€æŸ¥å›¾åƒå°ºå¯¸
                img_size = params.get('img_size') or params.get('image_size')
                if img_size:
                    if 'img_size' not in common_params:
                        common_params['img_size'] = {}
                    common_params['img_size'][model_name] = img_size
        
        # æ£€æŸ¥å‚æ•°ä¸€è‡´æ€§
        for param_name, param_values in common_params.items():
            unique_values = set(param_values.values())
            
            if len(unique_values) > 1:
                consistency_result['consistent'] = False
                inconsistency = {
                    'parameter': param_name,
                    'values': param_values,
                    'unique_values': list(unique_values)
                }
                consistency_result['inconsistencies'].append(inconsistency)
                
                # ç”Ÿæˆå»ºè®®
                most_common_value = max(unique_values, key=lambda x: list(param_values.values()).count(x))
                consistency_result['recommendations'].append(
                    f"å»ºè®®å°†æ‰€æœ‰æ¨¡å‹çš„ {param_name} ç»Ÿä¸€ä¸º: {most_common_value}"
                )
        
        return consistency_result
    
    def validate_training_compatibility(self, base_config: Dict[str, Any], model_configs: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯ä¸è®­ç»ƒé…ç½®çš„å…¼å®¹æ€§"""
        compatibility_result = {
            'compatible': True,
            'issues': [],
            'warnings': []
        }
        
        # æ£€æŸ¥æ•°æ®é…ç½®å…¼å®¹æ€§
        if 'data' in base_config:
            data_config = base_config['data']
            expected_channels = data_config.get('channels', 1)
            expected_img_size = data_config.get('img_size', 128)
            
            for model_name, model_config in model_configs.items():
                if 'params' in model_config:
                    params = model_config['params']
                    
                    # æ£€æŸ¥è¾“å…¥é€šé“æ•°
                    model_in_channels = params.get('in_channels')
                    if model_in_channels and model_in_channels != expected_channels:
                        compatibility_result['compatible'] = False
                        compatibility_result['issues'].append(
                            f"æ¨¡å‹ {model_name} çš„è¾“å…¥é€šé“æ•° ({model_in_channels}) "
                            f"ä¸æ•°æ®é…ç½®ä¸åŒ¹é… ({expected_channels})"
                        )
                    
                    # æ£€æŸ¥å›¾åƒå°ºå¯¸
                    model_img_size = params.get('img_size') or params.get('image_size')
                    if model_img_size and model_img_size != expected_img_size:
                        compatibility_result['warnings'].append(
                            f"æ¨¡å‹ {model_name} çš„å›¾åƒå°ºå¯¸ ({model_img_size}) "
                            f"ä¸æ•°æ®é…ç½®ä¸åŒ ({expected_img_size})"
                        )
        
        return compatibility_result
    
    def check_model_implementation(self, model_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹å®ç°æ˜¯å¦å­˜åœ¨"""
        implementation_result = {
            'model_name': model_name,
            'implementation_exists': False,
            'model_file': None,
            'class_found': False,
            'import_path': None
        }
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        models_dir = self.project_root / "models"
        possible_files = [
            models_dir / f"{model_name}.py",
            models_dir / f"{model_name.replace('_', '')}.py",
            models_dir / "architectures" / f"{model_name}.py"
        ]
        
        for model_file in possible_files:
            if model_file.exists():
                implementation_result['implementation_exists'] = True
                implementation_result['model_file'] = str(model_file)
                break
        
        # æ£€æŸ¥ __init__.py ä¸­çš„å¯¼å…¥
        init_file = models_dir / "__init__.py"
        if init_file.exists():
            try:
                with open(init_file, 'r', encoding='utf-8') as f:
                    init_content = f.read()
                
                # æŸ¥æ‰¾æ¨¡å‹ç±»æˆ–å‡½æ•°
                model_variations = [
                    model_name,
                    model_name.upper(),
                    ''.join(word.capitalize() for word in model_name.split('_')),
                    f"create_{model_name}",
                    f"get_{model_name}"
                ]
                
                for variation in model_variations:
                    if variation in init_content:
                        implementation_result['class_found'] = True
                        implementation_result['import_path'] = f"models.{variation}"
                        break
                        
            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ¨¡å‹å¯¼å…¥å¤±è´¥ {model_name}: {e}")
        
        return implementation_result
    
    def run_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        logger.info("å¼€å§‹éªŒè¯æ¨¡å‹é…ç½®...")
        
        # åŠ è½½åŸºç¡€é…ç½®
        base_config = self.load_base_config()
        
        # éªŒè¯ç»“æœ
        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'total_models': len(self.available_models),
            'models': {},
            'consistency_check': {},
            'compatibility_check': {},
            'implementation_check': {},
            'summary': {
                'config_files_found': 0,
                'valid_configs': 0,
                'missing_configs': 0,
                'implementation_found': 0,
                'overall_status': 'unknown'
            }
        }
        
        # éªŒè¯æ¯ä¸ªæ¨¡å‹é…ç½®
        model_configs = {}
        
        for model_name in self.available_models:
            logger.info(f"éªŒè¯æ¨¡å‹: {model_name}")
            
            # åŠ è½½é…ç½®
            config, exists = self.load_model_config(model_name)
            
            if exists:
                validation_results['summary']['config_files_found'] += 1
                model_configs[model_name] = config
                
                # éªŒè¯é…ç½®ç»“æ„
                structure_result = self.validate_model_config_structure(model_name, config)
                validation_results['models'][model_name] = structure_result
                
                if structure_result['structure_valid']:
                    validation_results['summary']['valid_configs'] += 1
            else:
                validation_results['summary']['missing_configs'] += 1
                validation_results['models'][model_name] = {
                    'model_name': model_name,
                    'structure_valid': False,
                    'missing_fields': [],
                    'warnings': [],
                    'errors': ['é…ç½®æ–‡ä»¶ä¸å­˜åœ¨']
                }
            
            # æ£€æŸ¥æ¨¡å‹å®ç°
            impl_result = self.check_model_implementation(model_name)
            validation_results['implementation_check'][model_name] = impl_result
            
            if impl_result['implementation_exists']:
                validation_results['summary']['implementation_found'] += 1
        
        # å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥
        if model_configs:
            consistency_result = self.validate_parameter_consistency(model_configs)
            validation_results['consistency_check'] = consistency_result
        
        # è®­ç»ƒå…¼å®¹æ€§æ£€æŸ¥
        if model_configs:
            compatibility_result = self.validate_training_compatibility(base_config, model_configs)
            validation_results['compatibility_check'] = compatibility_result
        
        # è®¡ç®—æ€»ä½“çŠ¶æ€
        total_models = len(self.available_models)
        valid_configs = validation_results['summary']['valid_configs']
        impl_found = validation_results['summary']['implementation_found']
        
        if valid_configs == total_models and impl_found == total_models:
            validation_results['summary']['overall_status'] = 'excellent'
        elif valid_configs >= total_models * 0.8 and impl_found >= total_models * 0.8:
            validation_results['summary']['overall_status'] = 'good'
        elif valid_configs >= total_models * 0.5:
            validation_results['summary']['overall_status'] = 'fair'
        else:
            validation_results['summary']['overall_status'] = 'poor'
        
        self.validation_results = validation_results
        return validation_results
    
    def save_validation_report(self, results: Dict[str, Any]):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        # ä¿å­˜JSONæ ¼å¼
        json_file = self.project_root / "runs" / "batch_training_results" / "model_config_validation.json"
        json_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"ä¿å­˜éªŒè¯ç»“æœ: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_file = json_file.with_suffix('.md')
        self.generate_markdown_report(results, md_file)
    
    def generate_markdown_report(self, results: Dict[str, Any], output_file: Path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„éªŒè¯æŠ¥å‘Š"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("# æ¨¡å‹é…ç½®éªŒè¯æŠ¥å‘Š\n\n")
                f.write(f"**ç”Ÿæˆæ—¶é—´**: {results['timestamp']}\n")
                f.write(f"**éªŒè¯æ¨¡å‹æ•°é‡**: {len(self.available_models)}\n\n")
                
                # æ€»ä½“çŠ¶æ€
                summary = results['summary']
                status_emoji = {
                    'excellent': 'ğŸŸ¢',
                    'good': 'ğŸŸ¡', 
                    'fair': 'ğŸŸ ',
                    'poor': 'ğŸ”´'
                }
                
                f.write("## æ€»ä½“çŠ¶æ€\n\n")
                f.write(f"{status_emoji.get(summary['overall_status'], 'âšª')} **{summary['overall_status'].upper()}**\n\n")
                
                # ç»Ÿè®¡ä¿¡æ¯
                f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
                total_models = len(self.available_models)
                f.write(f"- ğŸ“ **é…ç½®æ–‡ä»¶**: {summary['config_files_found']}/{total_models} æ‰¾åˆ°\n")
                f.write(f"- âœ… **æœ‰æ•ˆé…ç½®**: {summary['valid_configs']}/{total_models}\n")
                f.write(f"- âŒ **ç¼ºå¤±é…ç½®**: {summary['missing_configs']}/{total_models}\n")
                f.write(f"- ğŸ”§ **å®ç°æ–‡ä»¶**: {summary['implementation_found']}/{total_models} æ‰¾åˆ°\n\n")
                
                # æ¨¡å‹è¯¦æƒ…
                f.write("## æ¨¡å‹é…ç½®è¯¦æƒ…\n\n")
                f.write("| æ¨¡å‹åç§° | é…ç½®çŠ¶æ€ | å®ç°çŠ¶æ€ | é—®é¢˜æ•°é‡ | è­¦å‘Šæ•°é‡ |\n")
                f.write("|---------|---------|---------|---------|----------|\n")
                
                for model_name in self.available_models:
                    model_result = results['models'].get(model_name, {})
                    impl_result = results['implementation_check'].get(model_name, {})
                    
                    config_status = "âœ…" if model_result.get('structure_valid', False) else "âŒ"
                    impl_status = "âœ…" if impl_result.get('implementation_exists', False) else "âŒ"
                    error_count = len(model_result.get('errors', []))
                    warning_count = len(model_result.get('warnings', []))
                    
                    f.write(f"| {model_name} | {config_status} | {impl_status} | {error_count} | {warning_count} |\n")
                
                f.write("\n")
                
                # ä¸€è‡´æ€§æ£€æŸ¥
                if 'consistency_check' in results:
                    consistency = results['consistency_check']
                    f.write("## å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥\n\n")
                    
                    if consistency.get('consistent', True):
                        f.write("âœ… **æ‰€æœ‰æ¨¡å‹å‚æ•°ä¸€è‡´**\n\n")
                    else:
                        f.write("âš ï¸ **å‘ç°å‚æ•°ä¸ä¸€è‡´**\n\n")
                        
                        for inconsistency in consistency.get('inconsistencies', []):
                            param_name = inconsistency['parameter']
                            f.write(f"### {param_name}\n\n")
                            
                            for model, value in inconsistency['values'].items():
                                f.write(f"- {model}: {value}\n")
                            f.write("\n")
                        
                        # å»ºè®®
                        if consistency.get('recommendations'):
                            f.write("### å»ºè®®\n\n")
                            for rec in consistency['recommendations']:
                                f.write(f"- {rec}\n")
                            f.write("\n")
                
                # å…¼å®¹æ€§æ£€æŸ¥
                if 'compatibility_check' in results:
                    compatibility = results['compatibility_check']
                    f.write("## è®­ç»ƒå…¼å®¹æ€§æ£€æŸ¥\n\n")
                    
                    if compatibility.get('compatible', True):
                        f.write("âœ… **æ‰€æœ‰æ¨¡å‹ä¸è®­ç»ƒé…ç½®å…¼å®¹**\n\n")
                    else:
                        f.write("âŒ **å‘ç°å…¼å®¹æ€§é—®é¢˜**\n\n")
                        
                        for issue in compatibility.get('issues', []):
                            f.write(f"- âŒ {issue}\n")
                        
                        for warning in compatibility.get('warnings', []):
                            f.write(f"- âš ï¸ {warning}\n")
                        
                        f.write("\n")
                
                # è¯¦ç»†é”™è¯¯å’Œè­¦å‘Š
                f.write("## è¯¦ç»†é—®é¢˜åˆ—è¡¨\n\n")
                
                for model_name in self.available_models:
                    model_result = results['models'].get(model_name, {})
                    errors = model_result.get('errors', [])
                    warnings = model_result.get('warnings', [])
                    
                    if errors or warnings:
                        f.write(f"### {model_name}\n\n")
                        
                        if errors:
                            f.write("**é”™è¯¯**:\n")
                            for error in errors:
                                f.write(f"- âŒ {error}\n")
                            f.write("\n")
                        
                        if warnings:
                            f.write("**è­¦å‘Š**:\n")
                            for warning in warnings:
                                f.write(f"- âš ï¸ {warning}\n")
                            f.write("\n")
                
                # å»ºè®®æ“ä½œ
                f.write("## å»ºè®®æ“ä½œ\n\n")
                
                if summary['missing_configs'] > 0:
                    f.write("### ç¼ºå¤±é…ç½®æ–‡ä»¶\n\n")
                    f.write("ä¸ºä»¥ä¸‹æ¨¡å‹åˆ›å»ºé…ç½®æ–‡ä»¶:\n")
                    for model_name in self.available_models:
                        if not results['models'].get(model_name, {}).get('structure_valid', False):
                            f.write(f"- `configs/model/{model_name}.yaml`\n")
                    f.write("\n")
                
                if summary['implementation_found'] < len(self.available_models):
                    f.write("### ç¼ºå¤±å®ç°æ–‡ä»¶\n\n")
                    f.write("ä¸ºä»¥ä¸‹æ¨¡å‹æ·»åŠ å®ç°:\n")
                    for model_name in self.available_models:
                        impl_result = results['implementation_check'].get(model_name, {})
                        if not impl_result.get('implementation_exists', False):
                            f.write(f"- `models/{model_name}.py`\n")
                    f.write("\n")
                
            logger.info(f"ç”ŸæˆMarkdownæŠ¥å‘Š: {output_file}")
            
        except Exception as e:
            logger.error(f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    validator = ModelConfigValidator()
    
    try:
        # è¿è¡ŒéªŒè¯
        results = validator.run_validation()
        
        # ä¿å­˜æŠ¥å‘Š
        validator.save_validation_report(results)
        
        # è¾“å‡ºæ‘˜è¦
        summary = results['summary']
        total_models = len(validator.available_models)
        print(f"\nğŸ“‹ æ¨¡å‹é…ç½®éªŒè¯å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“çŠ¶æ€: {summary['overall_status'].upper()}")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {summary['config_files_found']}/{total_models}")
        print(f"âœ… æœ‰æ•ˆé…ç½®: {summary['valid_configs']}/{total_models}")
        print(f"ğŸ”§ å®ç°æ–‡ä»¶: {summary['implementation_found']}/{total_models}")
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­è®­ç»ƒ
        if summary['valid_configs'] >= total_models * 0.8:
            print("âœ… é…ç½®éªŒè¯é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹æ‰¹é‡è®­ç»ƒ!")
        else:
            print("âš ï¸ é…ç½®é—®é¢˜è¾ƒå¤šï¼Œå»ºè®®ä¿®å¤åå†å¼€å§‹è®­ç»ƒ")
            
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: runs/batch_training_results/model_config_validation.md")
        
    except Exception as e:
        logger.error(f"éªŒè¯å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()