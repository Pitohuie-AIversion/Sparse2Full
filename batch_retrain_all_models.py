#!/usr/bin/env python3
"""
æ‰¹é‡é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹è„šæœ¬
ä½¿ç”¨ç¨³å®šçš„è®­ç»ƒé…ç½®å‚æ•°é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹
"""

import os
import sys
import subprocess
import time
import json
from pathlib import Path
from datetime import datetime
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

# ç¨³å®šçš„è®­ç»ƒé…ç½®å‚æ•°
STABLE_CONFIG = {
    "loss": {
        "rec_weight": 1.0,
        "spec_weight": 0.0,
        "dc_weight": 0.0,
        "low_freq_modes": 4,
        "mirror_padding": False
    },
    "training": {
        "batch_size": 4,
        "lr": 1e-4,
        "use_amp": True,
        "epochs": 200,
        "grad_clip_norm": 1.0
    },
    "optimizer": {
        "name": "AdamW",
        "weight_decay": 1e-4
    },
    "scheduler": {
        "name": "cosine_warmup",
        "warmup_steps": 1000
    }
}

# æ‰€æœ‰éœ€è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
MODELS_TO_TRAIN = [
    "SwinUNet",
    "Hybrid", 
    "MLP",
    "UNet",
    "FNO2D",
    "UNetPlusPlus",
    "UFNO_UNet",
    "MLP_Mixer",
    "SegFormer",
    "SegFormer_UNetFormer"
]

# æ¨¡å‹é…ç½®æ˜ å°„
MODEL_CONFIGS = {
    "SwinUNet": {
        "patch_size": 4,
        "window_size": 8,
        "depths": [2, 2, 6, 2],
        "num_heads": [3, 6, 12, 24],
        "embed_dim": 96,
        "mlp_ratio": 4.0,
        "drop_rate": 0.0,
        "attn_drop_rate": 0.0,
        "drop_path_rate": 0.1
    },
    "Hybrid": {
        "patch_size": 4,
        "window_size": 8,
        "depths": [2, 2, 6, 2],
        "num_heads": [3, 6, 12, 24],
        "embed_dim": 96,
        "mlp_ratio": 4.0,
        "fno_modes": 16,
        "fno_layers": 4
    },
    "MLP": {
        "patch_size": 8,
        "hidden_dim": 512,
        "num_layers": 6,
        "dropout": 0.1
    },
    "UNet": {
        "base_channels": 64,
        "num_levels": 4,
        "dropout": 0.1
    },
    "FNO2D": {
        "modes": 16,
        "width": 64,
        "num_layers": 4
    },
    "UNetPlusPlus": {
        "base_channels": 64,
        "num_levels": 4,
        "deep_supervision": True
    },
    "UFNO_UNet": {
        "base_channels": 64,
        "fno_modes": 16,
        "fno_layers": 2
    },
    "MLP_Mixer": {
        "patch_size": 8,
        "hidden_dim": 512,
        "num_layers": 8,
        "tokens_mlp_dim": 256,
        "channels_mlp_dim": 2048
    },
    "SegFormer": {
        "embed_dims": [64, 128, 256, 512],
        "num_heads": [1, 2, 4, 8],
        "mlp_ratios": [4, 4, 4, 4],
        "depths": [3, 4, 6, 3]
    },
    "SegFormer_UNetFormer": {
        "embed_dims": [64, 128, 256, 512],
        "num_heads": [1, 2, 4, 8],
        "mlp_ratios": [4, 4, 4, 4],
        "depths": [3, 4, 6, 3],
        "use_unet_decoder": True
    }
}

class BatchTrainer:
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.config_dir = self.project_root / "configs"
        self.runs_dir = self.project_root / "runs"
        self.batch_runs_dir = self.runs_dir / f"batch_retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆ›å»ºæ‰¹é‡è®­ç»ƒç›®å½•
        self.batch_runs_dir.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒæ—¥å¿—
        self.training_log = []
        self.failed_models = []
        self.successful_models = []
        
    def create_model_config(self, model_name: str) -> Path:
        """ä¸ºæŒ‡å®šæ¨¡å‹åˆ›å»ºé…ç½®æ–‡ä»¶"""
        config_path = self.batch_runs_dir / f"train_{model_name.lower()}.yaml"
        
        # è¯»å–åŸºç¡€é…ç½®
        base_config_path = self.config_dir / "train.yaml"
        with open(base_config_path, 'r', encoding='utf-8') as f:
            config_content = f.read()
        
        # ä¿®æ”¹æ¨¡å‹åç§°å’Œå‚æ•°
        config_lines = config_content.split('\n')
        new_config_lines = []
        
        in_model_section = False
        in_kwargs_section = False
        indent_level = 0
        
        for line in config_lines:
            if line.strip().startswith('model:'):
                in_model_section = True
                new_config_lines.append(line)
            elif in_model_section and line.strip().startswith('name:'):
                new_config_lines.append(f'  name: "{model_name}"')
            elif in_model_section and line.strip().startswith('kwargs:'):
                in_kwargs_section = True
                new_config_lines.append(line)
                # æ·»åŠ æ¨¡å‹ç‰¹å®šé…ç½®
                if model_name in MODEL_CONFIGS:
                    for key, value in MODEL_CONFIGS[model_name].items():
                        if isinstance(value, list):
                            new_config_lines.append(f'      {key}: {value}')
                        elif isinstance(value, str):
                            new_config_lines.append(f'      {key}: "{value}"')
                        else:
                            new_config_lines.append(f'      {key}: {value}')
            elif in_kwargs_section and line.strip() and not line.startswith('      '):
                # é€€å‡ºkwargséƒ¨åˆ†
                in_kwargs_section = False
                new_config_lines.append(line)
            elif in_model_section and line.strip() and not line.startswith('  ') and not line.startswith('model:'):
                # é€€å‡ºmodeléƒ¨åˆ†
                in_model_section = False
                new_config_lines.append(line)
            elif not in_kwargs_section:
                new_config_lines.append(line)
        
        # å†™å…¥æ–°é…ç½®æ–‡ä»¶
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(new_config_lines))
        
        return config_path
    
    def train_model(self, model_name: str) -> bool:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"{'='*60}")
        
        try:
            # åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶
            config_path = self.create_model_config(model_name)
            print(f"é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
            
            # åˆ›å»ºæ¨¡å‹ä¸“ç”¨è¾“å‡ºç›®å½•
            model_output_dir = self.batch_runs_dir / model_name.lower()
            model_output_dir.mkdir(exist_ok=True)
            
            # æ„å»ºè®­ç»ƒå‘½ä»¤ - ä½¿ç”¨Hydraé…ç½®è¦†ç›–
            python_exe = r"F:\ProgramData\anaconda3\python.exe"
            train_cmd = [
                python_exe, "train.py",
                f"model.name={model_name}",
                f"experiment.output_dir={model_output_dir}",
                f"experiment.name={model_name}_stable_retrain"
            ]
            
            print(f"æ‰§è¡Œè®­ç»ƒå‘½ä»¤: {' '.join(train_cmd)}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰§è¡Œè®­ç»ƒ
            with open(model_output_dir / "training.log", "w") as log_file:
                process = subprocess.Popen(
                    train_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    universal_newlines=True,
                    cwd=self.project_root
                )
                
                # å®æ—¶è¾“å‡ºè®­ç»ƒæ—¥å¿—
                for line in process.stdout:
                    print(line.rstrip())
                    log_file.write(line)
                    log_file.flush()
                
                process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            training_time = end_time - start_time
            
            if process.returncode == 0:
                print(f"âœ… æ¨¡å‹ {model_name} è®­ç»ƒæˆåŠŸ! è€—æ—¶: {training_time:.2f}ç§’")
                self.successful_models.append(model_name)
                
                # è®°å½•è®­ç»ƒä¿¡æ¯
                self.training_log.append({
                    "model": model_name,
                    "status": "success",
                    "training_time": training_time,
                    "config_path": str(config_path),
                    "output_dir": str(model_output_dir),
                    "timestamp": datetime.now().isoformat()
                })
                return True
            else:
                print(f"âŒ æ¨¡å‹ {model_name} è®­ç»ƒå¤±è´¥! è¿”å›ç : {process.returncode}")
                self.failed_models.append(model_name)
                
                # è®°å½•å¤±è´¥ä¿¡æ¯
                self.training_log.append({
                    "model": model_name,
                    "status": "failed",
                    "training_time": training_time,
                    "return_code": process.returncode,
                    "config_path": str(config_path),
                    "output_dir": str(model_output_dir),
                    "timestamp": datetime.now().isoformat()
                })
                return False
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            self.failed_models.append(model_name)
            
            # è®°å½•å¼‚å¸¸ä¿¡æ¯
            self.training_log.append({
                "model": model_name,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            return False
    
    def save_training_summary(self):
        """ä¿å­˜è®­ç»ƒæ€»ç»“"""
        summary = {
            "batch_training_info": {
                "start_time": datetime.now().isoformat(),
                "stable_config": STABLE_CONFIG,
                "total_models": len(MODELS_TO_TRAIN),
                "successful_models": len(self.successful_models),
                "failed_models": len(self.failed_models)
            },
            "successful_models": self.successful_models,
            "failed_models": self.failed_models,
            "detailed_log": self.training_log
        }
        
        summary_path = self.batch_runs_dir / "training_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nè®­ç»ƒæ€»ç»“å·²ä¿å­˜åˆ°: {summary_path}")
        return summary_path
    
    def run_batch_training(self):
        """æ‰§è¡Œæ‰¹é‡è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
        print(f"ä½¿ç”¨ç¨³å®šé…ç½®: {STABLE_CONFIG}")
        print(f"è®­ç»ƒæ¨¡å‹åˆ—è¡¨: {MODELS_TO_TRAIN}")
        print(f"è¾“å‡ºç›®å½•: {self.batch_runs_dir}")
        
        total_start_time = time.time()
        
        # é€ä¸ªè®­ç»ƒæ¨¡å‹
        for i, model_name in enumerate(MODELS_TO_TRAIN, 1):
            print(f"\nè¿›åº¦: {i}/{len(MODELS_TO_TRAIN)}")
            self.train_model(model_name)
        
        total_end_time = time.time()
        total_time = total_end_time - total_start_time
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\n{'='*80}")
        print("ğŸ‰ æ‰¹é‡è®­ç»ƒå®Œæˆ!")
        print(f"{'='*80}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/3600:.2f}å°æ—¶)")
        print(f"æˆåŠŸè®­ç»ƒ: {len(self.successful_models)}/{len(MODELS_TO_TRAIN)} ä¸ªæ¨¡å‹")
        print(f"æˆåŠŸæ¨¡å‹: {', '.join(self.successful_models)}")
        if self.failed_models:
            print(f"å¤±è´¥æ¨¡å‹: {', '.join(self.failed_models)}")
        
        # ä¿å­˜è®­ç»ƒæ€»ç»“
        summary_path = self.save_training_summary()
        
        return {
            "success": len(self.failed_models) == 0,
            "successful_models": self.successful_models,
            "failed_models": self.failed_models,
            "summary_path": summary_path,
            "total_time": total_time
        }

def main():
    """ä¸»å‡½æ•°"""
    trainer = BatchTrainer()
    result = trainer.run_batch_training()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    if result["success"]:
        print("\nâœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒæˆåŠŸ!")
        sys.exit(0)
    else:
        print(f"\nâŒ æœ‰ {len(result['failed_models'])} ä¸ªæ¨¡å‹è®­ç»ƒå¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()