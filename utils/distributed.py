#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒæ¨¡å—

å®ç°å¤šGPUè®­ç»ƒå’Œåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼Œæ”¯æŒï¼š
1. åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å’Œæ¸…ç†
2. æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹åŒæ­¥
3. æ¢¯åº¦èšåˆå’Œé€šä¿¡ä¼˜åŒ–
4. æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½
5. æ—¥å¿—å’Œç›‘æ§

Author: PDEBench Team
Date: 2025-01-11
"""

import os
import sys
import time
import socket
import logging
from typing import Optional, Dict, Any, Tuple
from pathlib import Path

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler


def setup_logger(name: str, rank: int = 0) -> logging.Logger:
    """è®¾ç½®åˆ†å¸ƒå¼æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(name)
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­è¾“å‡ºæ—¥å¿—
    if rank == 0:
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [Rank %(rank)d] %(levelname)s: %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
    else:
        logger.setLevel(logging.WARNING)
    
    return logger


class DistributedManager:
    """åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨
    
    è´Ÿè´£åˆ†å¸ƒå¼ç¯å¢ƒçš„åˆå§‹åŒ–ã€æ¨¡å‹åŒ…è£…ã€æ•°æ®åˆ†å‘ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚
    æ”¯æŒå¤šç§åç«¯ï¼ˆNCCLã€Glooï¼‰å’Œå¯åŠ¨æ–¹å¼ã€‚
    """
    
    def __init__(self, 
                 backend: str = "nccl",
                 init_method: str = "env://",
                 timeout_minutes: int = 30):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç®¡ç†å™¨
        
        Args:
            backend: é€šä¿¡åç«¯ ("nccl", "gloo", "mpi")
            init_method: åˆå§‹åŒ–æ–¹æ³•
            timeout_minutes: è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        """
        self.backend = backend
        self.init_method = init_method
        self.timeout = timeout_minutes * 60
        
        # åˆ†å¸ƒå¼çŠ¶æ€
        self.is_distributed = False
        self.rank = 0
        self.local_rank = 0
        self.world_size = 1
        self.device = None
        
        # æ—¥å¿—è®°å½•å™¨
        self.logger = setup_logger("DistributedManager", 0)  # åˆå§‹åŒ–æ—¶ä½¿ç”¨rank 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self.comm_stats = {
            'allreduce_time': 0.0,
            'allreduce_count': 0,
            'broadcast_time': 0.0,
            'broadcast_count': 0
        }
    
    def setup(self) -> bool:
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
        try:
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
                self.rank = int(os.environ['RANK'])
                self.world_size = int(os.environ['WORLD_SIZE'])
                self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
                self.is_distributed = self.world_size > 1
            else:
                print("æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å•GPUæ¨¡å¼")
                self.is_distributed = False
                self.rank = 0
                self.local_rank = 0
                self.world_size = 1
            
            # è®¾ç½®è®¾å¤‡
            if torch.cuda.is_available():
                if self.is_distributed:
                    torch.cuda.set_device(self.local_rank)
                    self.device = torch.device(f"cuda:{self.local_rank}")
                else:
                    self.device = torch.device("cuda:0")
            else:
                self.device = torch.device("cpu")
                if self.is_distributed:
                    self.backend = "gloo"  # CPUåªæ”¯æŒgloo
            
            # æ›´æ–°æ—¥å¿—è®°å½•å™¨çš„rank
            self.logger = setup_logger("DistributedManager", self.rank)
            
            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            if self.is_distributed:
                dist.init_process_group(
                    backend=self.backend,
                    init_method=self.init_method,
                    world_size=self.world_size,
                    rank=self.rank,
                    timeout=torch.distributed.default_pg_timeout
                )
                
                # éªŒè¯åˆå§‹åŒ–
                if not dist.is_initialized():
                    raise RuntimeError("åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥")
                
                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                dist.barrier()
            

            
            if self.is_distributed:
                self.logger.info(f"åˆ†å¸ƒå¼è®­ç»ƒå·²åˆå§‹åŒ–")
                self.logger.info(f"  åç«¯: {self.backend}")
                self.logger.info(f"  ä¸–ç•Œå¤§å°: {self.world_size}")
                self.logger.info(f"  å½“å‰æ’å: {self.rank}")
                self.logger.info(f"  æœ¬åœ°æ’å: {self.local_rank}")
                self.logger.info(f"  è®¾å¤‡: {self.device}")
            else:
                self.logger.info(f"å•GPUè®­ç»ƒæ¨¡å¼ï¼Œè®¾å¤‡: {self.device}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def cleanup(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        if self.is_distributed and dist.is_initialized():
            self.logger.info("æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ...")
            
            # æ‰“å°é€šä¿¡ç»Ÿè®¡
            if self.rank == 0:
                self.print_communication_stats()
            
            dist.destroy_process_group()
            self.logger.info("åˆ†å¸ƒå¼ç¯å¢ƒå·²æ¸…ç†")
    
    def wrap_model(self, model: nn.Module, 
                   find_unused_parameters: bool = False,
                   gradient_as_bucket_view: bool = True) -> nn.Module:
        """åŒ…è£…æ¨¡å‹ä¸ºåˆ†å¸ƒå¼æ¨¡å‹"""
        model = model.to(self.device)
        
        if self.is_distributed:
            # ä½¿ç”¨DistributedDataParallelåŒ…è£…
            model = DDP(
                model,
                device_ids=[self.local_rank] if torch.cuda.is_available() else None,
                output_device=self.local_rank if torch.cuda.is_available() else None,
                find_unused_parameters=find_unused_parameters,
                gradient_as_bucket_view=gradient_as_bucket_view
            )
            
            self.logger.info(f"æ¨¡å‹å·²åŒ…è£…ä¸ºDDPï¼Œè®¾å¤‡: {self.device}")
        else:
            self.logger.info(f"å•GPUæ¨¡å¼ï¼Œæ¨¡å‹å·²ç§»è‡³è®¾å¤‡: {self.device}")
        
        return model
    
    def create_dataloader(self, dataset, 
                         batch_size: int,
                         shuffle: bool = True,
                         num_workers: int = 4,
                         pin_memory: bool = True,
                         drop_last: bool = True,
                         **kwargs) -> Tuple[DataLoader, Optional[DistributedSampler]]:
        """åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨"""
        sampler = None
        
        if self.is_distributed:
            # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
            sampler = DistributedSampler(
                dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=shuffle,
                drop_last=drop_last
            )
            shuffle = False  # ä½¿ç”¨sampleræ—¶ä¸èƒ½shuffle
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            sampler=sampler,
            num_workers=num_workers,
            pin_memory=pin_memory and torch.cuda.is_available(),
            drop_last=drop_last,
            **kwargs
        )
        
        self.logger.info(f"æ•°æ®åŠ è½½å™¨å·²åˆ›å»ºï¼Œæ‰¹å¤§å°: {batch_size}, å·¥ä½œè¿›ç¨‹: {num_workers}")
        
        return dataloader, sampler
    
    def all_reduce(self, tensor: torch.Tensor, op=dist.ReduceOp.SUM) -> torch.Tensor:
        """å…¨å±€å½’çº¦æ“ä½œ"""
        if not self.is_distributed:
            return tensor
        
        start_time = time.time()
        
        # ç¡®ä¿tensoråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        tensor = tensor.to(self.device)
        
        # æ‰§è¡Œå…¨å±€å½’çº¦
        dist.all_reduce(tensor, op=op)
        
        # ç»Ÿè®¡é€šä¿¡æ—¶é—´
        self.comm_stats['allreduce_time'] += time.time() - start_time
        self.comm_stats['allreduce_count'] += 1
        
        return tensor
    
    def all_gather(self, tensor: torch.Tensor) -> torch.Tensor:
        """å…¨å±€æ”¶é›†æ“ä½œ"""
        if not self.is_distributed:
            return tensor.unsqueeze(0)
        
        # å‡†å¤‡è¾“å‡ºå¼ é‡åˆ—è¡¨
        tensor_list = [torch.zeros_like(tensor) for _ in range(self.world_size)]
        
        # æ‰§è¡Œå…¨å±€æ”¶é›†
        dist.all_gather(tensor_list, tensor)
        
        # æ‹¼æ¥ç»“æœ
        return torch.stack(tensor_list, dim=0)
    
    def broadcast(self, tensor: torch.Tensor, src: int = 0) -> torch.Tensor:
        """å¹¿æ’­æ“ä½œ"""
        if not self.is_distributed:
            return tensor
        
        start_time = time.time()
        
        # ç¡®ä¿tensoråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        tensor = tensor.to(self.device)
        
        # æ‰§è¡Œå¹¿æ’­
        dist.broadcast(tensor, src=src)
        
        # ç»Ÿè®¡é€šä¿¡æ—¶é—´
        self.comm_stats['broadcast_time'] += time.time() - start_time
        self.comm_stats['broadcast_count'] += 1
        
        return tensor
    
    def reduce_dict(self, input_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """å½’çº¦å­—å…¸ä¸­çš„æ‰€æœ‰å¼ é‡"""
        if not self.is_distributed:
            return input_dict
        
        reduced_dict = {}
        for key, tensor in input_dict.items():
            if isinstance(tensor, torch.Tensor):
                reduced_tensor = self.all_reduce(tensor.clone())
                reduced_dict[key] = reduced_tensor / self.world_size
            else:
                reduced_dict[key] = tensor
        
        return reduced_dict
    
    def average_gradients(self, model: nn.Module):
        """å¹³å‡æ¢¯åº¦ï¼ˆæ‰‹åŠ¨å®ç°ï¼Œé€šå¸¸DDPä¼šè‡ªåŠ¨å¤„ç†ï¼‰"""
        if not self.is_distributed:
            return
        
        for param in model.parameters():
            if param.grad is not None:
                self.all_reduce(param.grad.data)
                param.grad.data /= self.world_size
    
    def save_checkpoint(self, state_dict: Dict[str, Any], 
                       filepath: str,
                       is_best: bool = False,
                       only_main_process: bool = True):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if only_main_process and self.rank != 0:
            return
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # æ·»åŠ åˆ†å¸ƒå¼ä¿¡æ¯
        state_dict['distributed'] = {
            'world_size': self.world_size,
            'rank': self.rank,
            'backend': self.backend
        }
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        torch.save(state_dict, filepath)
        
        if is_best:
            best_path = str(Path(filepath).parent / "best_model.pth")
            torch.save(state_dict, best_path)
        
        self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str, 
                       model: nn.Module,
                       optimizer: Optional[torch.optim.Optimizer] = None,
                       map_location: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if map_location is None:
            map_location = str(self.device)
        
        checkpoint = torch.load(filepath, map_location=map_location)
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        if hasattr(model, 'module'):
            # DDPæ¨¡å‹
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        self.logger.info(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
        
        return checkpoint
    
    def barrier(self):
        """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
        if self.is_distributed:
            dist.barrier()
    
    def print_communication_stats(self):
        """æ‰“å°é€šä¿¡ç»Ÿè®¡ä¿¡æ¯"""
        if self.rank != 0:
            return
        
        stats = self.comm_stats
        
        print("\nğŸ“Š åˆ†å¸ƒå¼é€šä¿¡ç»Ÿè®¡:")
        print(f"  AllReduceè°ƒç”¨æ¬¡æ•°: {stats['allreduce_count']}")
        print(f"  AllReduceæ€»æ—¶é—´: {stats['allreduce_time']:.3f}s")
        if stats['allreduce_count'] > 0:
            print(f"  AllReduceå¹³å‡æ—¶é—´: {stats['allreduce_time']/stats['allreduce_count']*1000:.2f}ms")
        
        print(f"  Broadcastè°ƒç”¨æ¬¡æ•°: {stats['broadcast_count']}")
        print(f"  Broadcastæ€»æ—¶é—´: {stats['broadcast_time']:.3f}s")
        if stats['broadcast_count'] > 0:
            print(f"  Broadcastå¹³å‡æ—¶é—´: {stats['broadcast_time']/stats['broadcast_count']*1000:.2f}ms")
    
    @property
    def is_main_process(self) -> bool:
        """æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
        return self.rank == 0
    
    def get_world_size(self) -> int:
        """è·å–ä¸–ç•Œå¤§å°"""
        return self.world_size
    
    def get_rank(self) -> int:
        """è·å–å½“å‰æ’å"""
        return self.rank


def get_world_size() -> int:
    """è·å–ä¸–ç•Œå¤§å°ï¼ˆå…¨å±€å‡½æ•°ï¼‰"""
    if dist.is_available() and dist.is_initialized():
        return dist.get_world_size()
    return 1


def get_rank() -> int:
    """è·å–å½“å‰æ’åï¼ˆå…¨å±€å‡½æ•°ï¼‰"""
    if dist.is_available() and dist.is_initialized():
        return dist.get_rank()
    return 0


def find_free_port() -> int:
    """æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port


def setup_distributed_environment(master_addr: str = "localhost",
                                 master_port: Optional[int] = None,
                                 backend: str = "nccl") -> DistributedManager:
    """è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒçš„ä¾¿æ·å‡½æ•°"""
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if master_port is None:
        master_port = find_free_port()
    
    os.environ.setdefault('MASTER_ADDR', master_addr)
    os.environ.setdefault('MASTER_PORT', str(master_port))
    
    # åˆ›å»ºåˆ†å¸ƒå¼ç®¡ç†å™¨
    dist_manager = DistributedManager(backend=backend)
    
    # åˆå§‹åŒ–
    if dist_manager.setup():
        return dist_manager
    else:
        raise RuntimeError("åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")


class DistributedMetrics:
    """åˆ†å¸ƒå¼æŒ‡æ ‡èšåˆå™¨"""
    
    def __init__(self, dist_manager: DistributedManager):
        self.dist_manager = dist_manager
        self.metrics_buffer = {}
    
    def update(self, metrics: Dict[str, float]):
        """æ›´æ–°æŒ‡æ ‡"""
        for key, value in metrics.items():
            if key not in self.metrics_buffer:
                self.metrics_buffer[key] = []
            self.metrics_buffer[key].append(value)
    
    def compute_and_reduce(self) -> Dict[str, float]:
        """è®¡ç®—å¹¶å½’çº¦æŒ‡æ ‡"""
        reduced_metrics = {}
        
        for key, values in self.metrics_buffer.items():
            if values:
                # è®¡ç®—æœ¬åœ°å¹³å‡å€¼
                local_avg = sum(values) / len(values)
                local_tensor = torch.tensor(local_avg, device=self.dist_manager.device)
                
                # å…¨å±€å½’çº¦
                global_tensor = self.dist_manager.all_reduce(local_tensor)
                global_avg = global_tensor.item() / self.dist_manager.world_size
                
                reduced_metrics[key] = global_avg
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.metrics_buffer.clear()
        
        return reduced_metrics


def launch_distributed_training(train_fn, 
                               world_size: int,
                               master_addr: str = "localhost",
                               master_port: Optional[int] = None,
                               backend: str = "nccl",
                               **kwargs):
    """å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒçš„ä¾¿æ·å‡½æ•°"""
    
    if master_port is None:
        master_port = find_free_port()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['MASTER_ADDR'] = master_addr
    os.environ['MASTER_PORT'] = str(master_port)
    os.environ['WORLD_SIZE'] = str(world_size)
    
    # ä½¿ç”¨torch.multiprocessingå¯åŠ¨
    import torch.multiprocessing as mp
    
    def worker(rank, world_size, train_fn, **kwargs):
        os.environ['RANK'] = str(rank)
        os.environ['LOCAL_RANK'] = str(rank)
        
        # åˆ›å»ºåˆ†å¸ƒå¼ç®¡ç†å™¨
        dist_manager = DistributedManager(backend=backend)
        
        try:
            if dist_manager.setup():
                # è°ƒç”¨è®­ç»ƒå‡½æ•°
                train_fn(dist_manager, **kwargs)
            else:
                print(f"Rank {rank}: åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥")
        finally:
            dist_manager.cleanup()
    
    # å¯åŠ¨å¤šè¿›ç¨‹
    mp.spawn(worker, args=(world_size, train_fn), kwargs=kwargs, nprocs=world_size, join=True)


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_distributed_setup():
    """æµ‹è¯•åˆ†å¸ƒå¼è®¾ç½®"""
    print("ğŸ§ª æµ‹è¯•åˆ†å¸ƒå¼ç¯å¢ƒè®¾ç½®...")
    
    # åˆ›å»ºåˆ†å¸ƒå¼ç®¡ç†å™¨
    dist_manager = DistributedManager()
    
    try:
        # åˆå§‹åŒ–
        if dist_manager.setup():
            print("âœ“ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            test_tensor = torch.randn(10, device=dist_manager.device)
            
            # æµ‹è¯•å…¨å±€å½’çº¦
            reduced_tensor = dist_manager.all_reduce(test_tensor.clone())
            print(f"âœ“ AllReduceæµ‹è¯•å®Œæˆï¼Œå¼ é‡å½¢çŠ¶: {reduced_tensor.shape}")
            
            # æµ‹è¯•å¹¿æ’­
            broadcast_tensor = dist_manager.broadcast(test_tensor.clone())
            print(f"âœ“ Broadcastæµ‹è¯•å®Œæˆï¼Œå¼ é‡å½¢çŠ¶: {broadcast_tensor.shape}")
            
            # æµ‹è¯•æŒ‡æ ‡å½’çº¦
            metrics = {'loss': 1.5, 'accuracy': 0.85}
            tensor_metrics = {k: torch.tensor(v, device=dist_manager.device) for k, v in metrics.items()}
            reduced_metrics = dist_manager.reduce_dict(tensor_metrics)
            print(f"âœ“ æŒ‡æ ‡å½’çº¦æµ‹è¯•å®Œæˆ: {reduced_metrics}")
            
            print("âœ… æ‰€æœ‰åˆ†å¸ƒå¼åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        else:
            print("âŒ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")
            
    finally:
        dist_manager.cleanup()


if __name__ == "__main__":
    test_distributed_setup()