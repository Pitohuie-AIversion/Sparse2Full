#!/usr/bin/env python3
"""
æå–æ‰¹é‡è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡

ä»è®­ç»ƒæ—¥å¿—ä¸­æå–æ¯ä¸ªæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
"""

import os
import sys
import json
import re
import torch
from pathlib import Path
from typing import Dict, List, Optional, Any
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

def extract_best_metrics_from_log(log_path: Path) -> Dict[str, Any]:
    """ä»è®­ç»ƒæ—¥å¿—ä¸­æå–æœ€ä½³æŒ‡æ ‡"""
    metrics = {
        'best_val_loss': None,
        'rel_l2': None,
        'mae': None,
        'psnr': None,
        'ssim': None,
        'frmse_low': None,
        'frmse_mid': None,
        'frmse_high': None,
        'brmse': None,
        'crmse': None
    }
    
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾æœ€ä½³éªŒè¯æŸå¤±
        best_loss_match = re.search(r'Best validation loss: ([\d.]+)', content)
        if best_loss_match:
            metrics['best_val_loss'] = float(best_loss_match.group(1))
        
        # æŸ¥æ‰¾æœ€ä½³éªŒè¯æŒ‡æ ‡éƒ¨åˆ† - å¤„ç†å¤šè¡Œæ ¼å¼
        best_metrics_pattern = r'Best validation metrics: \{(.*?)\}'
        best_metrics_match = re.search(best_metrics_pattern, content, re.DOTALL)
        if best_metrics_match:
            metrics_text = best_metrics_match.group(1)
            
            # æå–å„ç§æŒ‡æ ‡çš„å¹³å‡å€¼
            def extract_tensor_mean(pattern, text):
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    tensor_content = match.group(1)
                    # æå–æ•°å€¼ï¼ŒåŒ…æ‹¬ç§‘å­¦è®¡æ•°æ³•
                    numbers = re.findall(r'[\d.]+(?:e[+-]?\d+)?', tensor_content)
                    if numbers:
                        values = [float(n) for n in numbers]
                        return sum(values) / len(values)  # å¹³å‡å€¼
                return None
            
            # æ›´æ–°æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…å¤šè¡Œtensoræ ¼å¼
            metrics['rel_l2'] = extract_tensor_mean(r"'rel_l2': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['mae'] = extract_tensor_mean(r"'mae': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['psnr'] = extract_tensor_mean(r"'psnr': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['ssim'] = extract_tensor_mean(r"'ssim': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['frmse_low'] = extract_tensor_mean(r"'frmse_low': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['frmse_mid'] = extract_tensor_mean(r"'frmse_mid': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['frmse_high'] = extract_tensor_mean(r"'frmse_high': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['brmse'] = extract_tensor_mean(r"'brmse': tensor\(\[\[(.*?)\]\]", metrics_text)
            metrics['crmse'] = extract_tensor_mean(r"'crmse': tensor\(\[\[(.*?)\]\]", metrics_text)
    
    except Exception as e:
        print(f"è§£æè®­ç»ƒæ—¥å¿—å¤±è´¥ {log_path}: {e}")
    
    return metrics

def get_model_params_from_checkpoint(checkpoint_path: Path) -> int:
    """ä»æ£€æŸ¥ç‚¹è·å–æ¨¡å‹å‚æ•°æ•°é‡"""
    try:
        if checkpoint_path.exists():
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            if 'model_params' in checkpoint:
                return checkpoint['model_params']
            elif 'model' in checkpoint:
                # è®¡ç®—å‚æ•°æ•°é‡
                model_state = checkpoint['model']
                total_params = sum(p.numel() for p in model_state.values())
                return total_params
    except Exception as e:
        print(f"è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥ {checkpoint_path}: {e}")
    
    return 0

def analyze_batch_results(batch_dir: str) -> Dict[str, Any]:
    """åˆ†ææ‰¹é‡è®­ç»ƒç»“æœ"""
    batch_path = Path(batch_dir)
    
    # åŠ è½½è®­ç»ƒæ±‡æ€»
    summary_path = batch_path / "training_summary.json"
    with open(summary_path, 'r', encoding='utf-8') as f:
        summary = json.load(f)
    
    results = {}
    
    # åˆ†ææ¯ä¸ªæˆåŠŸçš„æ¨¡å‹
    for model_info in summary['detailed_log']:
        model_name = model_info['model']
        
        if model_info['status'] == 'success':
            print(f"ğŸ”„ åˆ†ææ¨¡å‹: {model_name}")
            
            # æ¨¡å‹ç›®å½•
            model_dir = batch_path / model_name.lower()
            
            # æå–æŒ‡æ ‡
            log_path = model_dir / "train.log"
            metrics = extract_best_metrics_from_log(log_path)
            
            # è·å–æ¨¡å‹å‚æ•°æ•°é‡
            checkpoint_path = model_dir / "checkpoints" / "best.pth"
            model_params = get_model_params_from_checkpoint(checkpoint_path)
            
            results[model_name] = {
                'status': 'success',
                'training_time': model_info['training_time'],
                'metrics': metrics,
                'model_params': model_params,
                'model_dir': str(model_dir)
            }
        else:
            results[model_name] = {
                'status': 'failed',
                'training_time': model_info['training_time'],
                'return_code': model_info.get('return_code', -1)
            }
    
    return {
        'summary': summary,
        'results': results
    }

def create_performance_comparison(analysis_data: Dict[str, Any], output_dir: Path):
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    results = analysis_data['results']
    
    # å‡†å¤‡æ•°æ®
    models = []
    rel_l2_values = []
    mae_values = []
    psnr_values = []
    ssim_values = []
    val_loss_values = []
    training_times = []
    param_counts = []
    
    for model_name, data in results.items():
        if data['status'] == 'success' and data.get('metrics'):
            metrics = data['metrics']
            if metrics['rel_l2'] is not None:  # ç¡®ä¿æœ‰æœ‰æ•ˆæŒ‡æ ‡
                models.append(model_name)
                rel_l2_values.append(metrics['rel_l2'])
                mae_values.append(metrics['mae'] or 0)
                psnr_values.append(metrics['psnr'] or 0)
                ssim_values.append(metrics['ssim'] or 0)
                val_loss_values.append(metrics['best_val_loss'] or 0)
                training_times.append(data['training_time'] / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿ
                param_counts.append(data['model_params'] / 1e6)  # è½¬æ¢ä¸ºç™¾ä¸‡å‚æ•°
    
    if not models:
        print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹æ•°æ®ç”¨äºå¯è§†åŒ–")
        return
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    fig.suptitle('æ‰¹é‡è®­ç»ƒæ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # Rel-L2è¯¯å·®å¯¹æ¯”
    axes[0, 0].bar(models, rel_l2_values, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('ç›¸å¯¹L2è¯¯å·® (è¶Šå°è¶Šå¥½)')
    axes[0, 0].set_ylabel('Rel-L2')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # MAEå¯¹æ¯”
    axes[0, 1].bar(models, mae_values, color='lightgreen', alpha=0.7)
    axes[0, 1].set_title('å¹³å‡ç»å¯¹è¯¯å·® (è¶Šå°è¶Šå¥½)')
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # PSNRå¯¹æ¯”
    axes[0, 2].bar(models, psnr_values, color='orange', alpha=0.7)
    axes[0, 2].set_title('å³°å€¼ä¿¡å™ªæ¯” (è¶Šå¤§è¶Šå¥½)')
    axes[0, 2].set_ylabel('PSNR (dB)')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    # SSIMå¯¹æ¯”
    axes[0, 3].bar(models, ssim_values, color='pink', alpha=0.7)
    axes[0, 3].set_title('ç»“æ„ç›¸ä¼¼æ€§ (è¶Šå¤§è¶Šå¥½)')
    axes[0, 3].set_ylabel('SSIM')
    axes[0, 3].tick_params(axis='x', rotation=45)
    
    # éªŒè¯æŸå¤±å¯¹æ¯”
    axes[1, 0].bar(models, val_loss_values, color='lightcoral', alpha=0.7)
    axes[1, 0].set_title('æœ€ä½³éªŒè¯æŸå¤± (è¶Šå°è¶Šå¥½)')
    axes[1, 0].set_ylabel('Validation Loss')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # è®­ç»ƒæ—¶é—´å¯¹æ¯”
    axes[1, 1].bar(models, training_times, color='gold', alpha=0.7)
    axes[1, 1].set_title('è®­ç»ƒæ—¶é—´ (åˆ†é’Ÿ)')
    axes[1, 1].set_ylabel('æ—¶é—´ (åˆ†é’Ÿ)')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # å‚æ•°æ•°é‡å¯¹æ¯”
    axes[1, 2].bar(models, param_counts, color='lightblue', alpha=0.7)
    axes[1, 2].set_title('æ¨¡å‹å‚æ•°æ•°é‡ (ç™¾ä¸‡)')
    axes[1, 2].set_ylabel('å‚æ•°æ•°é‡ (M)')
    axes[1, 2].tick_params(axis='x', rotation=45)
    
    # æ•ˆç‡å¯¹æ¯” (PSNR/è®­ç»ƒæ—¶é—´)
    efficiency = [p/t if t > 0 else 0 for p, t in zip(psnr_values, training_times)]
    axes[1, 3].bar(models, efficiency, color='lightsteelblue', alpha=0.7)
    axes[1, 3].set_title('è®­ç»ƒæ•ˆç‡ (PSNR/åˆ†é’Ÿ)')
    axes[1, 3].set_ylabel('PSNR/åˆ†é’Ÿ')
    axes[1, 3].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ€§èƒ½å¯¹æ¯”å›¾ä¿å­˜åˆ°: {output_dir / 'performance_comparison.png'}")

def create_ranking_table(analysis_data: Dict[str, Any], output_dir: Path) -> pd.DataFrame:
    """åˆ›å»ºæ¨¡å‹æ’åè¡¨"""
    results = analysis_data['results']
    
    # å‡†å¤‡æ•°æ®
    data = []
    for model_name, result in results.items():
        if result['status'] == 'success' and result.get('metrics'):
            metrics = result['metrics']
            if metrics['rel_l2'] is not None:  # ç¡®ä¿æœ‰æœ‰æ•ˆæŒ‡æ ‡
                data.append({
                    'æ¨¡å‹': model_name,
                    'Rel-L2': metrics['rel_l2'],
                    'MAE': metrics['mae'] or 0,
                    'PSNR': metrics['psnr'] or 0,
                    'SSIM': metrics['ssim'] or 0,
                    'æœ€ä½³éªŒè¯æŸå¤±': metrics['best_val_loss'] or 0,
                    'è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)': round(result['training_time'] / 60, 2),
                    'å‚æ•°é‡(M)': round(result['model_params'] / 1e6, 2),
                    'BRMSE': metrics['brmse'] or 0,
                    'CRMSE': metrics['crmse'] or 0
                })
    
    if not data:
        print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹æ•°æ®ç”¨äºæ’å")
        return pd.DataFrame()
    
    df = pd.DataFrame(data)
    
    # æŒ‰Rel-L2æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    df_sorted = df.sort_values('Rel-L2')
    
    # ä¿å­˜ä¸ºCSV
    df_sorted.to_csv(output_dir / 'model_ranking.csv', index=False, encoding='utf-8-sig')
    
    print(f"âœ… æ’åè¡¨ä¿å­˜åˆ°: {output_dir / 'model_ranking.csv'}")
    return df_sorted

def generate_analysis_report(analysis_data: Dict[str, Any], ranking_df: pd.DataFrame, output_dir: Path):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    summary = analysis_data['summary']
    results = analysis_data['results']
    
    report_path = output_dir / 'batch_training_analysis_report.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ‰¹é‡è®­ç»ƒç»“æœåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # è®­ç»ƒæ¦‚è§ˆ
        f.write("## ğŸ“Š è®­ç»ƒæ¦‚è§ˆ\n\n")
        batch_info = summary['batch_training_info']
        f.write(f"- **æ€»æ¨¡å‹æ•°**: {batch_info['total_models']}\n")
        f.write(f"- **æˆåŠŸè®­ç»ƒ**: {batch_info['successful_models']}\n")
        f.write(f"- **è®­ç»ƒå¤±è´¥**: {batch_info['failed_models']}\n")
        f.write(f"- **æˆåŠŸç‡**: {batch_info['successful_models'] / batch_info['total_models'] * 100:.1f}%\n\n")
        
        # æˆåŠŸæ¨¡å‹åˆ—è¡¨
        f.write("### âœ… æˆåŠŸè®­ç»ƒçš„æ¨¡å‹\n\n")
        for model in summary['successful_models']:
            f.write(f"- {model}\n")
        f.write("\n")
        
        # å¤±è´¥æ¨¡å‹åˆ—è¡¨
        if summary['failed_models']:
            f.write("### âŒ è®­ç»ƒå¤±è´¥çš„æ¨¡å‹\n\n")
            for model in summary['failed_models']:
                f.write(f"- {model}\n")
            f.write("\n")
        
        # æ€§èƒ½æ’å
        f.write("## ğŸ† æ€§èƒ½æ’å (æŒ‰Rel-L2æ’åº)\n\n")
        if not ranking_df.empty:
            f.write("| æ’å | æ¨¡å‹ | Rel-L2 | MAE | PSNR | SSIM | æœ€ä½³éªŒè¯æŸå¤± | è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ) | å‚æ•°é‡(M) |\n")
            f.write("|------|------|--------|-----|------|------|-------------|----------------|----------|\n")
            
            for idx, (_, row) in enumerate(ranking_df.iterrows()):
                rank = idx + 1
                f.write(f"| {rank} | {row['æ¨¡å‹']} | {row['Rel-L2']:.4f} | {row['MAE']:.4f} | "
                       f"{row['PSNR']:.2f} | {row['SSIM']:.4f} | {row['æœ€ä½³éªŒè¯æŸå¤±']:.4f} | "
                       f"{row['è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)']} | {row['å‚æ•°é‡(M)']} |\n")
        f.write("\n")
        
        # å…³é”®å‘ç°
        f.write("## ğŸ” å…³é”®å‘ç°\n\n")
        if not ranking_df.empty:
            best_model = ranking_df.iloc[0]
            f.write(f"### ğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model['æ¨¡å‹']}\n\n")
            f.write(f"- **Rel-L2è¯¯å·®**: {best_model['Rel-L2']:.4f}\n")
            f.write(f"- **PSNR**: {best_model['PSNR']:.2f} dB\n")
            f.write(f"- **SSIM**: {best_model['SSIM']:.4f}\n")
            f.write(f"- **è®­ç»ƒæ—¶é—´**: {best_model['è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)']} åˆ†é’Ÿ\n")
            f.write(f"- **å‚æ•°é‡**: {best_model['å‚æ•°é‡(M)']} M\n\n")
            
            # æ€§èƒ½åˆ†æ
            f.write("### ğŸ“ˆ æ€§èƒ½åˆ†æ\n\n")
            f.write("**Top 3 æ¨¡å‹å¯¹æ¯”:**\n\n")
            for idx, (_, row) in enumerate(ranking_df.head(3).iterrows()):
                rank = idx + 1
                f.write(f"{rank}. **{row['æ¨¡å‹']}**: Rel-L2={row['Rel-L2']:.4f}, "
                       f"PSNR={row['PSNR']:.2f}dB, è®­ç»ƒæ—¶é—´={row['è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)']}åˆ†é’Ÿ\n")
            f.write("\n")
        
        # è®­ç»ƒé…ç½®
        f.write("## âš™ï¸ è®­ç»ƒé…ç½®\n\n")
        stable_config = batch_info['stable_config']
        f.write("### æŸå¤±å‡½æ•°é…ç½® (ç¨³å®šé…ç½®)\n")
        f.write(f"- rec_weight: {stable_config['loss']['rec_weight']}\n")
        f.write(f"- spec_weight: {stable_config['loss']['spec_weight']}\n")
        f.write(f"- dc_weight: {stable_config['loss']['dc_weight']}\n\n")
        
        f.write("### è®­ç»ƒå‚æ•°\n")
        f.write(f"- batch_size: {stable_config['training']['batch_size']}\n")
        f.write(f"- learning_rate: {stable_config['training']['lr']}\n")
        f.write(f"- epochs: {stable_config['training']['epochs']}\n")
        f.write(f"- use_amp: {stable_config['training']['use_amp']}\n\n")
        
        # æ–‡ä»¶è¯´æ˜
        f.write("## ğŸ“ ç”Ÿæˆæ–‡ä»¶è¯´æ˜\n\n")
        f.write("- `performance_comparison.png`: æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾è¡¨\n")
        f.write("- `model_ranking.csv`: è¯¦ç»†æ’åæ•°æ®\n")
        f.write("- `batch_training_analysis_report.md`: æœ¬åˆ†ææŠ¥å‘Š\n")
        f.write("- `analysis_results.json`: å®Œæ•´åˆ†ææ•°æ®\n\n")
    
    print(f"âœ… åˆ†ææŠ¥å‘Šä¿å­˜åˆ°: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æå–æ‰¹é‡è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡')
    parser.add_argument('--batch_dir', type=str, required=True, help='æ‰¹é‡è®­ç»ƒç»“æœç›®å½•')
    parser.add_argument('--output_dir', type=str, help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºbatch_dir/analysisï¼‰')
    
    args = parser.parse_args()
    
    batch_dir = Path(args.batch_dir)
    if not batch_dir.exists():
        print(f"âŒ é”™è¯¯: æ‰¹é‡è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {batch_dir}")
        return
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = batch_dir / 'analysis'
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ”„ å¼€å§‹åˆ†ææ‰¹é‡è®­ç»ƒç»“æœ...")
    print(f"ğŸ“ æ‰¹é‡è®­ç»ƒç›®å½•: {batch_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # åˆ†ææ‰¹é‡è®­ç»ƒç»“æœ
        print("ğŸ”„ æå–æ¨¡å‹æŒ‡æ ‡...")
        analysis_data = analyze_batch_results(batch_dir)
        
        # ä¿å­˜å®Œæ•´åˆ†ææ•°æ®
        with open(output_dir / 'analysis_results.json', 'w', encoding='utf-8') as f:
            # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            serializable_data = {}
            for key, value in analysis_data.items():
                if key == 'results':
                    serializable_results = {}
                    for model_name, model_data in value.items():
                        serializable_results[model_name] = {
                            k: (v if not isinstance(v, torch.Tensor) else v.tolist() if hasattr(v, 'tolist') else str(v))
                            for k, v in model_data.items()
                        }
                    serializable_data[key] = serializable_results
                else:
                    serializable_data[key] = value
            
            json.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
        print("ğŸ”„ åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾...")
        create_performance_comparison(analysis_data, output_dir)
        
        # åˆ›å»ºæ’åè¡¨
        print("ğŸ”„ åˆ›å»ºæ’åè¡¨...")
        ranking_df = create_ranking_table(analysis_data, output_dir)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("ğŸ”„ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        generate_analysis_report(analysis_data, ranking_df, output_dir)
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_path in sorted(output_dir.glob("*")):
            print(f"  - {file_path.name}")
        
        # æ˜¾ç¤ºæ’åæ‘˜è¦
        if not ranking_df.empty:
            print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’å (Top 3):")
            for idx, (_, row) in enumerate(ranking_df.head(3).iterrows()):
                rank = idx + 1
                print(f"  {rank}. {row['æ¨¡å‹']}: Rel-L2={row['Rel-L2']:.4f}, PSNR={row['PSNR']:.2f}dB")
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()