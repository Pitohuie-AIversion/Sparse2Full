#!/usr/bin/env python3
"""
è®­ç»ƒç»“æœå¯è§†åŒ–è„šæœ¬
åˆ†æè®­ç»ƒæ—¥å¿—å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import re
import numpy as np
import pandas as pd
from pathlib import Path
import json
from datetime import datetime
# ä½¿ç”¨ç»Ÿä¸€çš„å¯è§†åŒ–å·¥å…·ï¼Œä¸ç›´æ¥å¯¼å…¥matplotlib
from utils.visualization import PDEBenchVisualizer

def parse_training_log(log_path):
    """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–æŸå¤±å’ŒæŒ‡æ ‡æ•°æ®"""
    epochs = []
    train_losses = []
    val_losses = []
    val_rel_l2 = []
    
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è®­ç»ƒæ•°æ®
    pattern = r'Epoch\s+(\d+)\s+-\s+Train Loss:\s+([\d.]+)\s+Val Loss:\s+([\d.]+)\s+Val Rel-L2:\s+([\d.]+)'
    matches = re.findall(pattern, content)
    
    for match in matches:
        epoch, train_loss, val_loss, rel_l2 = match
        epochs.append(int(epoch))
        train_losses.append(float(train_loss))
        val_losses.append(float(val_loss))
        val_rel_l2.append(float(rel_l2))
    
    return epochs, train_losses, val_losses, val_rel_l2

def extract_best_metrics(log_path):
    """æå–æœ€ä½³éªŒè¯æŒ‡æ ‡"""
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–æœ€ä½³éªŒè¯æŸå¤±
    best_val_loss_match = re.search(r'Best validation loss:\s+([\d.]+)', content)
    best_val_loss = float(best_val_loss_match.group(1)) if best_val_loss_match else None
    
    # æå–æœ€ä½³éªŒè¯æŒ‡æ ‡
    metrics_pattern = r"'rel_l2': tensor\(\[\[([\d.]+)\],\s*\[([\d.]+)\]\].*?'mae': tensor\(\[\[([\d.]+)\],\s*\[([\d.]+)\]\].*?'psnr': tensor\(\[\[([\d.]+)\],\s*\[([\d.]+)\]\].*?'ssim': tensor\(\[\[([\d.]+)\],\s*\[([\d.]+)\]\]"
    metrics_match = re.search(metrics_pattern, content, re.DOTALL)
    
    best_metrics = {}
    if metrics_match:
        rel_l2_1, rel_l2_2, mae_1, mae_2, psnr_1, psnr_2, ssim_1, ssim_2 = metrics_match.groups()
        best_metrics = {
            'rel_l2': [float(rel_l2_1), float(rel_l2_2)],
            'mae': [float(mae_1), float(mae_2)],
            'psnr': [float(psnr_1), float(psnr_2)],
            'ssim': [float(ssim_1), float(ssim_2)]
        }
    
    # æå–è®­ç»ƒæ—¶é—´
    train_time_match = re.search(r'Total training time:\s+([\d.]+)s', content)
    val_time_match = re.search(r'Total validation time:\s+([\d.]+)s', content)
    
    train_time = float(train_time_match.group(1)) if train_time_match else None
    val_time = float(val_time_match.group(1)) if val_time_match else None
    
    return best_val_loss, best_metrics, train_time, val_time

def create_loss_curves(epochs, train_losses, val_losses, val_rel_l2, output_dir):
    """åˆ›å»ºæŸå¤±æ›²çº¿å›¾"""
    visualizer = PDEBenchVisualizer(str(output_dir))
    
    # å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ—¥å¿—
    train_logs = {
        'loss': train_losses,
        'rel_l2': [0] * len(epochs)  # è®­ç»ƒæ—¶æ²¡æœ‰rel_l2ï¼Œç”¨0å¡«å……
    }
    
    val_logs = {
        'loss': val_losses,
        'rel_l2': val_rel_l2
    }
    
    # ä½¿ç”¨ç»Ÿä¸€çš„å¯è§†åŒ–æ¥å£
    visualizer.plot_training_curves(train_logs, val_logs, "loss_curves")

def create_metrics_visualization(best_metrics, output_dir):
    """åˆ›å»ºæœ€ä½³æŒ‡æ ‡å¯è§†åŒ–"""
    if not best_metrics:
        return
    
    visualizer = PDEBenchVisualizer(str(output_dir))
    
    # å‡†å¤‡æ¨¡å‹æ¯”è¾ƒæ•°æ®
    model_results = {}
    for metric, values in best_metrics.items():
        if isinstance(values, list) and len(values) >= 2:
            model_results[f'Channel1_{metric}'] = values[0]
            model_results[f'Channel2_{metric}'] = values[1]
        else:
            model_results[metric] = values if not isinstance(values, list) else values[0]
    
    # ä½¿ç”¨æ¨¡å‹æ¯”è¾ƒåŠŸèƒ½æ¥æ˜¾ç¤ºæŒ‡æ ‡
    visualizer.plot_model_comparison(
        {'Best_Metrics': model_results}, 
        save_name="best_metrics"
    )

def create_convergence_analysis(epochs, train_losses, val_losses, val_rel_l2, output_dir):
    """åˆ›å»ºæ”¶æ•›åˆ†æå›¾"""
    # æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±çš„epoch
    best_val_idx = np.argmin(val_losses)
    best_epoch = epochs[best_val_idx]
    best_val_loss = val_losses[best_val_idx]
    
    # ç›¸å¯¹L2è¯¯å·®æ”¶æ•›åˆ†æ
    best_rel_l2_idx = np.argmin(val_rel_l2)
    best_rel_l2_epoch = epochs[best_rel_l2_idx]
    best_rel_l2_value = val_rel_l2[best_rel_l2_idx]
    
    # ä½¿ç”¨ç»Ÿä¸€çš„å¯è§†åŒ–æ¥å£åˆ›å»ºè®­ç»ƒæ›²çº¿
    visualizer = PDEBenchVisualizer(str(output_dir))
    
    train_logs = {
        'loss': train_losses,
        'rel_l2': [0] * len(epochs)  # è®­ç»ƒæ—¶æ²¡æœ‰rel_l2
    }
    
    val_logs = {
        'loss': val_losses,
        'rel_l2': val_rel_l2
    }
    
    visualizer.plot_training_curves(train_logs, val_logs, "convergence_analysis")
    
    return best_epoch, best_val_loss, best_rel_l2_epoch, best_rel_l2_value

def generate_training_report(epochs, train_losses, val_losses, val_rel_l2, 
                           best_val_loss, best_metrics, train_time, val_time,
                           best_epoch, best_rel_l2_epoch, output_dir):
    """ç”Ÿæˆè®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    final_train_loss = train_losses[-1] if train_losses else 0
    final_val_loss = val_losses[-1] if val_losses else 0
    final_rel_l2 = val_rel_l2[-1] if val_rel_l2 else 0
    
    min_train_loss = min(train_losses) if train_losses else 0
    min_val_loss = min(val_losses) if val_losses else 0
    min_rel_l2 = min(val_rel_l2) if val_rel_l2 else 0
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""
# è®­ç»ƒç»“æœæ€»ç»“æŠ¥å‘Š

## è®­ç»ƒé…ç½®
- **æ¨¡å‹**: SwinUNet
- **ä»»åŠ¡**: SR x4 (è¶…åˆ†è¾¨ç‡4å€)
- **æ•°æ®é›†**: PDEBench
- **è®­ç»ƒæ ·æœ¬**: 1000
- **éªŒè¯æ ·æœ¬**: 100
- **æ‰¹æ¬¡å¤§å°**: 4
- **æ€»epochæ•°**: {len(epochs)}

## è®­ç»ƒæ€§èƒ½
- **æ€»è®­ç»ƒæ—¶é—´**: {train_time:.2f}ç§’
- **æ€»éªŒè¯æ—¶é—´**: {val_time:.2f}ç§’
- **å¹³å‡æ¯epochæ—¶é—´**: {(train_time + val_time) / len(epochs):.2f}ç§’

## æŸå¤±æ”¶æ•›æƒ…å†µ
- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {final_train_loss:.6f}
- **æœ€ç»ˆéªŒè¯æŸå¤±**: {final_val_loss:.6f}
- **æœ€ç»ˆç›¸å¯¹L2è¯¯å·®**: {final_rel_l2:.6f}

## æœ€ä½³æ€§èƒ½æŒ‡æ ‡
- **æœ€ä½³éªŒè¯æŸå¤±**: {best_val_loss:.6f} (Epoch {best_epoch})
- **æœ€ä½³ç›¸å¯¹L2è¯¯å·®**: {min_rel_l2:.6f} (Epoch {best_rel_l2_epoch})

### æœ€ä½³éªŒè¯æŒ‡æ ‡è¯¦æƒ…
"""
    
    if best_metrics:
        for metric, values in best_metrics.items():
            avg_value = np.mean(values)
            report += f"- **{metric.upper()}**: {avg_value:.6f} (é€šé“1: {values[0]:.6f}, é€šé“2: {values[1]:.6f})\n"
    
    report += f"""
## æ”¶æ•›åˆ†æ
- **è®­ç»ƒæŸå¤±å‡å°‘**: {train_losses[0]:.6f} â†’ {final_train_loss:.6f} ({((train_losses[0] - final_train_loss) / train_losses[0] * 100):.1f}%å‡å°‘)
- **éªŒè¯æŸå¤±å‡å°‘**: {val_losses[0]:.6f} â†’ {final_val_loss:.6f} ({((val_losses[0] - final_val_loss) / val_losses[0] * 100):.1f}%å‡å°‘)
- **ç›¸å¯¹L2è¯¯å·®å‡å°‘**: {val_rel_l2[0]:.6f} â†’ {final_rel_l2:.6f} ({((val_rel_l2[0] - final_rel_l2) / val_rel_l2[0] * 100):.1f}%å‡å°‘)

## æ¨¡å‹æ€§èƒ½è¯„ä¼°
- **PSNR**: {np.mean(best_metrics['psnr']) if best_metrics and 'psnr' in best_metrics else 'N/A':.2f} dB
- **SSIM**: {np.mean(best_metrics['ssim']) if best_metrics and 'ssim' in best_metrics else 'N/A':.4f}
- **MAE**: {np.mean(best_metrics['mae']) if best_metrics and 'mae' in best_metrics else 'N/A':.6f}

## è®­ç»ƒç¨³å®šæ€§
- **è®­ç»ƒæŸå¤±æ ‡å‡†å·®**: {np.std(train_losses):.6f}
- **éªŒè¯æŸå¤±æ ‡å‡†å·®**: {np.std(val_losses):.6f}
- **ç›¸å¯¹L2è¯¯å·®æ ‡å‡†å·®**: {np.std(val_rel_l2):.6f}

## ç»“è®º
è®­ç»ƒæˆåŠŸå®Œæˆï¼Œæ¨¡å‹åœ¨{len(epochs)}ä¸ªepochåè¾¾åˆ°è‰¯å¥½çš„æ”¶æ•›çŠ¶æ€ã€‚
æœ€ä½³éªŒè¯æŸå¤±ä¸º{best_val_loss:.6f}ï¼Œç›¸å¯¹L2è¯¯å·®ä¸º{min_rel_l2:.6f}ï¼Œ
PSNRè¾¾åˆ°{np.mean(best_metrics['psnr']) if best_metrics and 'psnr' in best_metrics else 'N/A':.2f}dBï¼Œ
SSIMä¸º{np.mean(best_metrics['ssim']) if best_metrics and 'ssim' in best_metrics else 'N/A':.4f}ï¼Œ
è¡¨æ˜æ¨¡å‹å…·æœ‰è‰¯å¥½çš„è¶…åˆ†è¾¨ç‡é‡å»ºæ€§èƒ½ã€‚

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_dir / 'training_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ä¿å­˜JSONæ ¼å¼çš„æ•°æ®
    data = {
        'epochs': epochs,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_rel_l2': val_rel_l2,
        'best_val_loss': best_val_loss,
        'best_metrics': best_metrics,
        'train_time': train_time,
        'val_time': val_time,
        'best_epoch': best_epoch,
        'best_rel_l2_epoch': best_rel_l2_epoch
    }
    
    with open(output_dir / 'training_data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    log_path = Path('runs/train.log')
    output_dir = Path('runs/visualization')
    output_dir.mkdir(exist_ok=True)
    
    print("ğŸ” åˆ†æè®­ç»ƒæ—¥å¿—...")
    
    # è§£æè®­ç»ƒæ—¥å¿—
    epochs, train_losses, val_losses, val_rel_l2 = parse_training_log(log_path)
    
    if not epochs:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
        return
    
    print(f"âœ… æˆåŠŸè§£æ {len(epochs)} ä¸ªepochçš„è®­ç»ƒæ•°æ®")
    
    # æå–æœ€ä½³æŒ‡æ ‡
    best_val_loss, best_metrics, train_time, val_time = extract_best_metrics(log_path)
    
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # åˆ›å»ºæŸå¤±æ›²çº¿å›¾
    create_loss_curves(epochs, train_losses, val_losses, val_rel_l2, output_dir)
    print("âœ… æŸå¤±æ›²çº¿å›¾å·²ç”Ÿæˆ")
    
    # åˆ›å»ºæŒ‡æ ‡å¯è§†åŒ–
    create_metrics_visualization(best_metrics, output_dir)
    print("âœ… æœ€ä½³æŒ‡æ ‡å›¾å·²ç”Ÿæˆ")
    
    # åˆ›å»ºæ”¶æ•›åˆ†æå›¾
    best_epoch, best_val_loss_found, best_rel_l2_epoch, best_rel_l2_value = create_convergence_analysis(
        epochs, train_losses, val_losses, val_rel_l2, output_dir)
    print("âœ… æ”¶æ•›åˆ†æå›¾å·²ç”Ÿæˆ")
    
    # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
    generate_training_report(epochs, train_losses, val_losses, val_rel_l2,
                           best_val_loss, best_metrics, train_time, val_time,
                           best_epoch, best_rel_l2_epoch, output_dir)
    print("âœ… è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ")
    
    print(f"\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“ˆ æŸå¤±æ›²çº¿å›¾: {output_dir}/loss_curves.png")
    print(f"ğŸ“Š æœ€ä½³æŒ‡æ ‡å›¾: {output_dir}/best_metrics.png")
    print(f"ğŸ“‰ æ”¶æ•›åˆ†æå›¾: {output_dir}/convergence_analysis.png")
    print(f"ğŸ“ è®­ç»ƒæŠ¥å‘Š: {output_dir}/training_report.md")
    print(f"ğŸ’¾ è®­ç»ƒæ•°æ®: {output_dir}/training_data.json")
    
    # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“‹ å…³é”®ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
    print(f"   æœ€ä½³ç›¸å¯¹L2è¯¯å·®: {best_rel_l2_value:.6f} (Epoch {best_rel_l2_epoch})")
    if best_metrics:
        print(f"   å¹³å‡PSNR: {np.mean(best_metrics['psnr']):.2f} dB")
        print(f"   å¹³å‡SSIM: {np.mean(best_metrics['ssim']):.4f}")
        print(f"   å¹³å‡MAE: {np.mean(best_metrics['mae']):.6f}")
    print(f"   æ€»è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")

if __name__ == "__main__":
    main()