#!/usr/bin/env python3
"""
ä½¿ç”¨å·¥ä½œç©ºé—´çš„å¯è§†åŒ–å·¥å…·åˆ›å»ºé¢„æµ‹ç»“æœå¯¹æ¯”å›¾
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from utils.visualization import PDEBenchVisualizer, create_field_comparison
from datasets.pde_bench import PDEBenchDataset
from models.swin_unet import SwinUNet
from ops.degradation import SuperResolutionOperator


def load_model_and_data(run_dir: Path) -> Tuple[torch.nn.Module, torch.utils.data.Dataset, Dict]:
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®é›†"""
    print(f"åŠ è½½æ¨¡å‹å’Œæ•°æ®: {run_dir}")
    
    # åŠ è½½é…ç½®
    config_path = run_dir / "config_merged.yaml"
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    import yaml
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_model_path = run_dir / "best.pth"
    if not best_model_path.exists():
        # å°è¯•ä»checkpointsç›®å½•åŠ è½½
        best_model_path = run_dir / "checkpoints" / "best.pth"
        if not best_model_path.exists():
            raise FileNotFoundError(f"æœ€ä½³æ¨¡å‹ä¸å­˜åœ¨: {best_model_path}")
    
    # åˆ›å»ºæ¨¡å‹
    model_config = config['model']
    if 'MLP' in str(run_dir):
        # ç®€å•MLPæ¨¡å‹
        from models.mlp import MLPModel
        model = MLPModel(
            in_channels=model_config.get('in_channels', 1),
            out_channels=model_config.get('out_channels', 1),
            img_size=model_config.get('img_size', 128),
            hidden_dims=model_config.get('hidden_dims', [128, 256, 128]),
            mode=model_config.get('mode', 'coord'),
            coord_encoding=model_config.get('coord_encoding', 'positional'),
            coord_encoding_dim=model_config.get('coord_encoding_dim', 32),
            max_freq=model_config.get('max_freq', 10.0),
            activation=model_config.get('activation', 'relu'),
            dropout=model_config.get('dropout', 0.1)
        )
    else:
        # SwinUNetæ¨¡å‹
        model = SwinUNet(
            in_channels=model_config.get('in_channels', 1),
            out_channels=model_config.get('out_channels', 1),
            img_size=model_config.get('img_size', 128)
        )
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model.eval()
    
    # åˆ›å»ºæ•°æ®é›†
    data_config = config['data']# åˆ›å»ºæ•°æ®é›†
    from datasets.pdebench import PDEBenchSR
    dataset = PDEBenchSR(
        data_path=data_config['data_path'],
        keys=data_config.get('keys', ['tensor']),
        split='test',
        scale=data_config['observation']['sr']['scale_factor'],
        sigma=data_config['observation']['sr']['blur_sigma'],
        blur_kernel=data_config['observation']['sr']['blur_kernel_size'],
        boundary=data_config['observation']['sr']['boundary_mode'],
        normalize=data_config.get('normalize', True),
        image_size=data_config.get('image_size', 128)
    )
    
    return model, dataset, config


def generate_predictions(model: torch.nn.Module, 
                        dataset: torch.utils.data.Dataset,
                        config: Dict,
                        num_samples: int = 5) -> List[Dict[str, np.ndarray]]:
    """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
    print(f"ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ...")
    
    # åˆ›å»ºé™è´¨ç®—å­
    obs_config = config['data']['observation']
    if obs_config['mode'] == 'SR':
        sr_config = obs_config['sr']
        degradation_op = SuperResolutionOperator(
            scale=sr_config['scale_factor'],
            sigma=sr_config.get('blur_sigma', 1.0),
            kernel_size=sr_config.get('blur_kernel_size', 5),
            boundary=sr_config.get('boundary_mode', 'mirror')
        )
    else:
        raise NotImplementedError(f"è§‚æµ‹æ¨¡å¼ {obs_config['mode']} æœªå®ç°")
    
    results = []
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # é€‰æ‹©æ ·æœ¬ç´¢å¼•
    indices = np.linspace(0, len(dataset)-1, num_samples, dtype=int)
    
    with torch.no_grad():
        for i, idx in enumerate(indices):
            print(f"  å¤„ç†æ ·æœ¬ {i+1}/{num_samples} (ç´¢å¼• {idx})")
            
            # è·å–æ•°æ®
            data_dict = dataset[idx]
            gt = data_dict['target'].unsqueeze(0).to(device)  # [1, C, H, W]
            
            # ç”Ÿæˆè§‚æµ‹
            observed = degradation_op(gt)
            
            # é¢„æµ‹
            pred = model(observed)
            
            # ç¡®ä¿predå’Œgtå°ºå¯¸ä¸€è‡´ç”¨äºæŒ‡æ ‡è®¡ç®—
        if pred.shape != gt.shape:
            # ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·pred
            if pred.dim() == 3:  # [C, H, W]
                pred = torch.nn.functional.interpolate(
                    pred.unsqueeze(0), 
                    size=gt.shape[-2:], 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)
            elif pred.dim() == 4:  # [B, C, H, W]
                pred = torch.nn.functional.interpolate(
                    pred, 
                    size=gt.shape[-2:], 
                    mode='bilinear', 
                    align_corners=False
                )
        
        # è®¡ç®—æŒ‡æ ‡
        rel_l2 = torch.norm(pred - gt) / torch.norm(gt)
        mae = torch.mean(torch.abs(pred - gt))
        
        # è½¬æ¢ä¸ºnumpyç”¨äºå¯è§†åŒ–
        gt_np = gt.squeeze().cpu().numpy()
        pred_np = pred.squeeze().cpu().numpy()
        obs_np = observed.squeeze().cpu().numpy()
        
        # è®¡ç®—è¯¯å·®
        error_np = np.abs(gt_np - pred_np)
        
        results.append({
            'observed': obs_np,
            'ground_truth': gt_np,
            'prediction': pred_np,
            'error': error_np,
            'rel_l2': rel_l2.item(),
            'mae': mae.item(),
            'sample_idx': idx
        })
        
        print(f"    Rel-L2: {rel_l2.item():.4f}, MAE: {mae.item():.6f}")
    
    return results


def create_comparison_visualizations(results: List[Dict[str, np.ndarray]], 
                                   output_dir: Path):
    """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–"""
    print(f"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ï¼Œä¿å­˜åˆ°: {output_dir}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = PDEBenchVisualizer(str(output_dir))
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºå¯¹æ¯”å›¾
    for i, result in enumerate(results):
        sample_idx = result['sample_idx']
        rel_l2 = result['rel_l2']
        mae = result['mae']
        
        # åˆ›å»ºå››è”å›¾ (Observed, GT, Pred, Error)
        observed_tensor = torch.from_numpy(result['observed']).unsqueeze(0).unsqueeze(0)
        gt_tensor = torch.from_numpy(result['ground_truth']).unsqueeze(0).unsqueeze(0)
        pred_tensor = torch.from_numpy(result['prediction']).unsqueeze(0).unsqueeze(0)
        
        save_name = f"sample_{sample_idx:03d}_comparison"
        title = f"Sample {sample_idx} - Rel-L2: {rel_l2:.4f}, MAE: {mae:.6f}"
        
        visualizer.create_quadruplet_visualization(
            observed_tensor, gt_tensor, pred_tensor,
            save_name=save_name,
            title=title
        )
        
        print(f"  âœ“ æ ·æœ¬ {sample_idx} å¯¹æ¯”å›¾å·²ä¿å­˜")
    
    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å›¾
    create_summary_plot(results, output_dir)


def create_summary_plot(results: List[Dict[str, np.ndarray]], output_dir: Path):
    """åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å›¾"""
    print("åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å›¾...")
    
    # æå–æŒ‡æ ‡
    rel_l2_values = [r['rel_l2'] for r in results]
    mae_values = [r['mae'] for r in results]
    sample_indices = [r['sample_idx'] for r in results]
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Rel-L2 åˆ†å¸ƒ
    axes[0].bar(range(len(rel_l2_values)), rel_l2_values, color='steelblue', alpha=0.7)
    axes[0].set_title('Relative L2 Error by Sample', fontweight='bold')
    axes[0].set_xlabel('Sample Index')
    axes[0].set_ylabel('Rel-L2')
    axes[0].set_xticks(range(len(sample_indices)))
    axes[0].set_xticklabels([f'{idx}' for idx in sample_indices])
    axes[0].grid(True, alpha=0.3)
    
    # MAE åˆ†å¸ƒ
    axes[1].bar(range(len(mae_values)), mae_values, color='darkorange', alpha=0.7)
    axes[1].set_title('Mean Absolute Error by Sample', fontweight='bold')
    axes[1].set_xlabel('Sample Index')
    axes[1].set_ylabel('MAE')
    axes[1].set_xticks(range(len(sample_indices)))
    axes[1].set_xticklabels([f'{idx}' for idx in sample_indices])
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    summary_path = output_dir / "metrics_summary.png"
    plt.savefig(summary_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
    print(f"  å¹³å‡ Rel-L2: {np.mean(rel_l2_values):.4f} Â± {np.std(rel_l2_values):.4f}")
    print(f"  å¹³å‡ MAE: {np.mean(mae_values):.6f} Â± {np.std(mae_values):.6f}")
    print(f"  æœ€ä½³ Rel-L2: {np.min(rel_l2_values):.4f} (æ ·æœ¬ {sample_indices[np.argmin(rel_l2_values)]})")
    print(f"  æœ€å·® Rel-L2: {np.max(rel_l2_values):.4f} (æ ·æœ¬ {sample_indices[np.argmax(rel_l2_values)]})")
    
    print(f"  âœ“ æ±‡æ€»å›¾å·²ä¿å­˜: {summary_path}")


def main():
    """ä¸»å‡½æ•°"""
    # æŒ‡å®šè¿è¡Œç›®å½•
    run_dir = Path("runs/SRx4-DarcyFlow-128-MLP-quick-s2025-20250111")
    
    if not run_dir.exists():
        print(f"âŒ è¿è¡Œç›®å½•ä¸å­˜åœ¨: {run_dir}")
        return
    
    try:
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, dataset, config = load_model_and_data(run_dir)
        print(f"âœ… æ¨¡å‹å’Œæ•°æ®åŠ è½½æˆåŠŸ")
        
        # ç”Ÿæˆé¢„æµ‹ç»“æœ
        results = generate_predictions(model, dataset, config, num_samples=5)
        print(f"âœ… é¢„æµ‹ç»“æœç”Ÿæˆå®Œæˆ")
        
        # åˆ›å»ºå¯è§†åŒ–
        output_dir = run_dir / "prediction_comparisons"
        create_comparison_visualizations(results, output_dir)
        print(f"âœ… å¯è§†åŒ–åˆ›å»ºå®Œæˆ")
        
        print(f"\nğŸ‰ æ‰€æœ‰å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()