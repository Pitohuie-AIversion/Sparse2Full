"""è®­ç»ƒç¯å¢ƒå®Œæ•´æ€§æµ‹è¯•

éªŒè¯PDEBenchè®­ç»ƒç¯å¢ƒçš„å„ä¸ªç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
åŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–ã€æŸå¤±è®¡ç®—ã€GPUèµ„æºç­‰
"""

import os
import sys
import time
import traceback
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import numpy as np
from omegaconf import OmegaConf
import hydra
from hydra import initialize, compose

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from datasets import PDEBenchDataModule
from models import create_model
from ops.loss import TotalLoss


def test_environment():
    """æµ‹è¯•è®­ç»ƒç¯å¢ƒ"""
    print("=" * 60)
    print("PDEBenchè®­ç»ƒç¯å¢ƒå®Œæ•´æ€§æµ‹è¯•")
    print("=" * 60)
    
    # 1. æµ‹è¯•åŸºç¡€ç¯å¢ƒ
    print("\n1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥:")
    print(f"  - Pythonç‰ˆæœ¬: {sys.version}")
    print(f"  - PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}")
        print(f"  - å½“å‰è®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"  - æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"  - ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 2. æµ‹è¯•æ•°æ®è·¯å¾„
    print("\n2. æ•°æ®è·¯å¾„æ£€æŸ¥:")
    data_path = "E:/2D"
    if os.path.exists(data_path):
        print(f"  âœ“ æ•°æ®è·¯å¾„å­˜åœ¨: {data_path}")
        subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]
        print(f"  - å­ç›®å½•æ•°é‡: {len(subdirs)}")
        print(f"  - å­ç›®å½•: {subdirs[:5]}{'...' if len(subdirs) > 5 else ''}")
    else:
        print(f"  âœ— æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return False
    
    # 3. æµ‹è¯•é…ç½®åŠ è½½
    print("\n3. é…ç½®åŠ è½½æµ‹è¯•:")
    try:
        with initialize(version_base=None, config_path="configs"):
            config = compose(config_name="config")
        print("  âœ“ é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  - å®éªŒåç§°: {config.experiment.name}")
        print(f"  - æ•°æ®é›†: {config.data._target_}")
        print(f"  - æ¨¡å‹: {config.model.name}")
        print(f"  - è®­ç»ƒepochs: {config.train.epochs}")
    except Exception as e:
        print(f"  âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 4. æµ‹è¯•æ•°æ®æ¨¡å—
    print("\n4. æ•°æ®æ¨¡å—æµ‹è¯•:")
    try:
        data_module = PDEBenchDataModule(config.data)
        data_module.setup()
        print("  âœ“ æ•°æ®æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
        
        train_loader = data_module.train_dataloader()
        print(f"  - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {config.data.dataloader.batch_size}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        batch = next(iter(train_loader))
        print("  âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"  - æ‰¹æ¬¡é”®: {list(batch.keys())}")
        
        # æ£€æŸ¥å¼ é‡å½¢çŠ¶
        for key, value in batch.items():
            if torch.is_tensor(value):
                print(f"    {key}: {value.shape} ({value.dtype})")
        
    except Exception as e:
        print(f"  âœ— æ•°æ®æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False
    
    # 5. æµ‹è¯•æ¨¡å‹åˆ›å»º
    print("\n5. æ¨¡å‹åˆ›å»ºæµ‹è¯•:")
    try:
        model = create_model(config)
        model = model.to(device)
        print("  âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"  - æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
    except Exception as e:
        print(f"  âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        traceback.print_exc()
        return False
    
    # 6. æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n6. å‰å‘ä¼ æ’­æµ‹è¯•:")
    try:
        # è·å–è¾“å…¥å¼ é‡
        input_key = None
        for key in ['baseline', 'input', 'lr']:
            if key in batch:
                input_key = key
                break
        
        if input_key is None:
            print(f"  âœ— æœªæ‰¾åˆ°åˆé€‚çš„è¾“å…¥é”®ï¼Œå¯ç”¨é”®: {list(batch.keys())}")
            return False
        
        input_tensor = batch[input_key].to(device)
        print(f"  - ä½¿ç”¨è¾“å…¥é”®: {input_key}")
        print(f"  - è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
        
        with torch.no_grad():
            pred = model(input_tensor)
        
        print("  âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - è¾“å‡ºå½¢çŠ¶: {pred.shape}")
        print(f"  - è¾“å‡ºèŒƒå›´: [{pred.min():.4f}, {pred.max():.4f}]")
        
    except Exception as e:
        print(f"  âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        traceback.print_exc()
        return False
    
    # 7. æµ‹è¯•æŸå¤±è®¡ç®—
    print("\n7. æŸå¤±è®¡ç®—æµ‹è¯•:")
    try:
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_fn = TotalLoss(
            rec_weight=config.train.loss_weights.reconstruction,
            spec_weight=config.train.loss_weights.spectral,
            dc_weight=config.train.loss_weights.data_consistency
        )
        print("  âœ“ æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        
        # è·å–ç›®æ ‡å¼ é‡
        target_key = None
        for key in ['target', 'gt', 'hr']:
            if key in batch:
                target_key = key
                break
        
        if target_key is None:
            print(f"  âœ— æœªæ‰¾åˆ°ç›®æ ‡å¼ é‡é”®ï¼Œå¯ç”¨é”®: {list(batch.keys())}")
            return False
        
        target_tensor = batch[target_key].to(device)
        print(f"  - ä½¿ç”¨ç›®æ ‡é”®: {target_key}")
        
        # è·å–è§‚æµ‹å¼ é‡
        observation_key = None
        for key in ['observation', 'lr_observation', 'baseline']:
            if key in batch:
                observation_key = key
                break
        
        if observation_key is None:
            print(f"  âœ— æœªæ‰¾åˆ°è§‚æµ‹å¼ é‡é”®")
            return False
        
        observation_tensor = batch[observation_key].to(device)
        print(f"  - ä½¿ç”¨è§‚æµ‹é”®: {observation_key}")
        
        # æ„å»ºä»»åŠ¡å‚æ•°
        task_params = batch.get('task_params', {
            'task': 'SR',
            'scale': 4,
            'sigma': 1.0,
            'kernel_size': 5,
            'boundary': 'mirror'
        })
        
        # è®¡ç®—æŸå¤±
        total_loss, loss_dict = loss_fn(pred, target_tensor, observation_tensor, task_params)
        
        print("  âœ“ æŸå¤±è®¡ç®—æˆåŠŸ")
        for loss_name, loss_value in loss_dict.items():
            if torch.is_tensor(loss_value):
                print(f"    {loss_name}: {loss_value.item():.6f}")
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"  âš  è­¦å‘Š: æ€»æŸå¤±ä¸ºNaNæˆ–Inf: {total_loss}")
        else:
            print(f"  âœ“ æŸå¤±å€¼æ­£å¸¸: {total_loss.item():.6f}")
        
    except Exception as e:
        print(f"  âœ— æŸå¤±è®¡ç®—å¤±è´¥: {e}")
        traceback.print_exc()
        return False
    
    # 8. æµ‹è¯•GPUå†…å­˜ä½¿ç”¨
    print("\n8. GPUå†…å­˜ä½¿ç”¨:")
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        print(f"  - å·²åˆ†é…: {allocated:.2f} GB")
        print(f"  - ç¼“å­˜: {cached:.2f} GB")
    else:
        print("  - CPUæ¨¡å¼ï¼Œæ— GPUå†…å­˜ç»Ÿè®¡")
    
    print("\n" + "=" * 60)
    print("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è®­ç»ƒç¯å¢ƒå‡†å¤‡å°±ç»ª")
    print("=" * 60)
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    try:
        success = test_environment()
        if success:
            print("\nğŸ‰ è®­ç»ƒç¯å¢ƒæµ‹è¯•å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
            return 0
        else:
            print("\nâŒ è®­ç»ƒç¯å¢ƒæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return 1
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())