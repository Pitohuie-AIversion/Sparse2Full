"""
æµ‹è¯•é»„é‡‘æ³•åˆ™éµå¾ª

éªŒè¯PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿæ˜¯å¦éµå¾ªé»„é‡‘æ³•åˆ™ï¼š
1. ä¸€è‡´æ€§ä¼˜å…ˆï¼šè§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCå¿…é¡»å¤ç”¨åŒä¸€å®ç°ä¸é…ç½®
2. å¯å¤ç°ï¼šåŒä¸€YAML+ç§å­ï¼ŒéªŒè¯æŒ‡æ ‡æ–¹å·®â‰¤1e-4
3. ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰æ¨¡å‹forward(x[B,C_in,H,W])â†’y[B,C_out,H,W]
4. å¯æ¯”æ€§ï¼šæ¨ªå‘å¯¹æ¯”å¿…é¡»æŠ¥å‘Šå‡å€¼Â±æ ‡å‡†å·®ï¼ˆâ‰¥3ç§å­ï¼‰+èµ„æºæˆæœ¬
5. æ–‡æ¡£å…ˆè¡Œï¼šæ–°å¢ä»»åŠ¡/ç®—å­/æ¨¡å‹å‰ï¼Œå…ˆæäº¤PRD/æŠ€æœ¯æ–‡æ¡£è¡¥ä¸
"""

import torch
import torch.nn as nn
import numpy as np
import yaml
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import hashlib
import time
import random
from dataclasses import dataclass
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

try:
    from ops.degradation import GaussianBlurDownsample, CenterCrop
    from utils.metrics import MetricsCalculator, StatisticalAnalyzer
    from test_resource_monitoring import ResourceMonitor, SimpleTestModel
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥æŸäº›æ¨¡å—: {e}")
    print("å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬è¿›è¡Œæµ‹è¯•")
    
    # å®šä¹‰ç®€åŒ–çš„æµ‹è¯•æ¨¡å‹
    class SimpleTestModel(nn.Module):
        """ç®€å•æµ‹è¯•æ¨¡å‹"""
        
        def __init__(self, in_channels=3, out_channels=3, hidden_dim=64):
            super().__init__()
            self.conv1 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)
            self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)
            self.conv3 = nn.Conv2d(hidden_dim, out_channels, 3, padding=1)
            self.relu = nn.ReLU(inplace=True)
        
        def forward(self, x):
            x = self.relu(self.conv1(x))
            x = self.relu(self.conv2(x))
            x = self.conv3(x)
            return x


@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    task: str
    data: str
    resolution: int
    model: str
    seed: int
    date: str
    
    def to_exp_name(self) -> str:
        """ç”Ÿæˆå®éªŒåç§°"""
        return f"{self.task}-{self.data}-{self.resolution}-{self.model}-s{self.seed}-{self.date}"


class GoldenRulesValidator:
    """é»„é‡‘æ³•åˆ™éªŒè¯å™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = {}
        
    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        random.seed(seed)
        
        # ç¡®ä¿ç¡®å®šæ€§
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
    def test_consistency_rule(self) -> Dict[str, Any]:
        """æµ‹è¯•ä¸€è‡´æ€§ä¼˜å…ˆè§„åˆ™
        
        éªŒè¯è§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCå¿…é¡»å¤ç”¨åŒä¸€å®ç°ä¸é…ç½®
        """
        print("æµ‹è¯•ä¸€è‡´æ€§ä¼˜å…ˆè§„åˆ™...")
        
        results = {
            'passed': True,
            'details': {},
            'errors': []
        }
        
        try:
            # æµ‹è¯•é€€åŒ–ç®—å­ä¸€è‡´æ€§
            if 'GaussianBlurDownsample' in globals():
                # åˆ›å»ºç›¸åŒé…ç½®çš„é€€åŒ–ç®—å­
                config = {'sigma': 1.0, 'scale': 2, 'kernel_size': 5}
                
                degradation1 = GaussianBlurDownsample(**config)
                degradation2 = GaussianBlurDownsample(**config)
                
                # æµ‹è¯•è¾“å…¥
                x = torch.randn(1, 3, 256, 256)
                
                # è®¾ç½®ç›¸åŒç§å­
                self.set_seed(42)
                y1 = degradation1(x)
                
                self.set_seed(42)
                y2 = degradation2(x)
                
                # éªŒè¯ä¸€è‡´æ€§
                mse = torch.mean((y1 - y2) ** 2).item()
                results['details']['degradation_consistency'] = {
                    'mse': mse,
                    'passed': mse < 1e-8
                }
                
                if mse >= 1e-8:
                    results['passed'] = False
                    results['errors'].append(f"é€€åŒ–ç®—å­ä¸ä¸€è‡´ï¼ŒMSE={mse:.2e}")
                
            else:
                results['details']['degradation_consistency'] = {
                    'skipped': True,
                    'reason': 'é€€åŒ–ç®—å­æ¨¡å—æœªå¯¼å…¥'
                }
            
            # æµ‹è¯•æŒ‡æ ‡è®¡ç®—ä¸€è‡´æ€§
            if 'MetricsCalculator' in globals():
                calculator1 = MetricsCalculator(image_size=(128, 128))
                calculator2 = MetricsCalculator(image_size=(128, 128))
                
                pred = torch.randn(2, 3, 128, 128)
                target = torch.randn(2, 3, 128, 128)
                
                metrics1 = calculator1.compute_rel_l2(pred, target)
                metrics2 = calculator2.compute_rel_l2(pred, target)
                
                diff = abs(metrics1 - metrics2)
                results['details']['metrics_consistency'] = {
                    'diff': diff,
                    'passed': diff < 1e-8
                }
                
                if diff >= 1e-8:
                    results['passed'] = False
                    results['errors'].append(f"æŒ‡æ ‡è®¡ç®—ä¸ä¸€è‡´ï¼Œå·®å¼‚={diff:.2e}")
            
            print(f"âœ“ ä¸€è‡´æ€§è§„åˆ™æµ‹è¯•: {'é€šè¿‡' if results['passed'] else 'å¤±è´¥'}")
            
        except Exception as e:
            results['passed'] = False
            results['errors'].append(f"ä¸€è‡´æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}")
            print(f"âŒ ä¸€è‡´æ€§è§„åˆ™æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_reproducibility_rule(self) -> Dict[str, Any]:
        """æµ‹è¯•å¯å¤ç°æ€§è§„åˆ™
        
        éªŒè¯åŒä¸€YAML+ç§å­ï¼ŒéªŒè¯æŒ‡æ ‡æ–¹å·®â‰¤1e-4
        """
        print("æµ‹è¯•å¯å¤ç°æ€§è§„åˆ™...")
        
        results = {
            'passed': True,
            'details': {},
            'errors': []
        }
        
        try:
            # åˆ›å»ºç®€å•æ¨¡å‹è¿›è¡Œæµ‹è¯•
            model = SimpleTestModel(in_channels=3, out_channels=3, hidden_dim=32)
            model = model.to(self.device)
            
            # æµ‹è¯•æ•°æ®
            x = torch.randn(2, 3, 128, 128).to(self.device)
            target = torch.randn(2, 3, 128, 128).to(self.device)
            
            # å¤šæ¬¡è¿è¡Œç›¸åŒé…ç½®
            seed = 42
            num_runs = 5
            metrics_list = []
            
            for run in range(num_runs):
                self.set_seed(seed)
                
                # å‰å‘ä¼ æ’­
                with torch.no_grad():
                    pred = model(x)
                
                # è®¡ç®—æŒ‡æ ‡
                if 'MetricsCalculator' in globals():
                    calculator = MetricsCalculator(image_size=(128, 128))
                    rel_l2 = calculator.compute_rel_l2(pred, target)
                    mae = calculator.compute_mae(pred, target)
                    
                    metrics_list.append({
                        'rel_l2': rel_l2,
                        'mae': mae
                    })
                else:
                    # ç®€åŒ–ç‰ˆæœ¬
                    rel_l2 = torch.mean((pred - target) ** 2).item()
                    mae = torch.mean(torch.abs(pred - target)).item()
                    
                    metrics_list.append({
                        'rel_l2': rel_l2,
                        'mae': mae
                    })
            
            # è®¡ç®—æ–¹å·®
            for metric_name in ['rel_l2', 'mae']:
                values = [m[metric_name] for m in metrics_list]
                variance = np.var(values)
                
                results['details'][f'{metric_name}_variance'] = {
                    'variance': variance,
                    'values': values,
                    'passed': variance <= 1e-4
                }
                
                if variance > 1e-4:
                    results['passed'] = False
                    results['errors'].append(f"{metric_name}æ–¹å·®è¿‡å¤§: {variance:.2e}")
            
            print(f"âœ“ å¯å¤ç°æ€§è§„åˆ™æµ‹è¯•: {'é€šè¿‡' if results['passed'] else 'å¤±è´¥'}")
            
        except Exception as e:
            results['passed'] = False
            results['errors'].append(f"å¯å¤ç°æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}")
            print(f"âŒ å¯å¤ç°æ€§è§„åˆ™æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_unified_interface_rule(self) -> Dict[str, Any]:
        """æµ‹è¯•ç»Ÿä¸€æ¥å£è§„åˆ™
        
        éªŒè¯æ‰€æœ‰æ¨¡å‹forward(x[B,C_in,H,W])â†’y[B,C_out,H,W]
        """
        print("æµ‹è¯•ç»Ÿä¸€æ¥å£è§„åˆ™...")
        
        results = {
            'passed': True,
            'details': {},
            'errors': []
        }
        
        try:
            # æµ‹è¯•ä¸åŒé…ç½®çš„æ¨¡å‹
            test_configs = [
                {'in_channels': 3, 'out_channels': 3, 'hidden_dim': 32},
                {'in_channels': 1, 'out_channels': 1, 'hidden_dim': 64},
                {'in_channels': 4, 'out_channels': 2, 'hidden_dim': 16}
            ]
            
            for i, config in enumerate(test_configs):
                model = SimpleTestModel(**config)
                model = model.to(self.device)
                
                # æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸
                input_shapes = [
                    (1, config['in_channels'], 64, 64),
                    (2, config['in_channels'], 128, 128),
                    (4, config['in_channels'], 256, 256)
                ]
                
                for j, input_shape in enumerate(input_shapes):
                    x = torch.randn(input_shape).to(self.device)
                    
                    try:
                        with torch.no_grad():
                            y = model(x)
                        
                        # éªŒè¯è¾“å‡ºå½¢çŠ¶
                        expected_shape = (input_shape[0], config['out_channels'], 
                                        input_shape[2], input_shape[3])
                        
                        shape_correct = y.shape == expected_shape
                        
                        results['details'][f'model_{i}_input_{j}'] = {
                            'input_shape': input_shape,
                            'output_shape': list(y.shape),
                            'expected_shape': expected_shape,
                            'passed': shape_correct
                        }
                        
                        if not shape_correct:
                            results['passed'] = False
                            results['errors'].append(
                                f"æ¨¡å‹{i}è¾“å…¥{j}å½¢çŠ¶ä¸åŒ¹é…: {y.shape} vs {expected_shape}"
                            )
                    
                    except Exception as e:
                        results['passed'] = False
                        results['errors'].append(
                            f"æ¨¡å‹{i}è¾“å…¥{j}å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}"
                        )
            
            print(f"âœ“ ç»Ÿä¸€æ¥å£è§„åˆ™æµ‹è¯•: {'é€šè¿‡' if results['passed'] else 'å¤±è´¥'}")
            
        except Exception as e:
            results['passed'] = False
            results['errors'].append(f"ç»Ÿä¸€æ¥å£æµ‹è¯•å¼‚å¸¸: {str(e)}")
            print(f"âŒ ç»Ÿä¸€æ¥å£è§„åˆ™æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_comparability_rule(self) -> Dict[str, Any]:
        """æµ‹è¯•å¯æ¯”æ€§è§„åˆ™
        
        éªŒè¯æ¨ªå‘å¯¹æ¯”å¿…é¡»æŠ¥å‘Šå‡å€¼Â±æ ‡å‡†å·®ï¼ˆâ‰¥3ç§å­ï¼‰+èµ„æºæˆæœ¬
        """
        print("æµ‹è¯•å¯æ¯”æ€§è§„åˆ™...")
        
        results = {
            'passed': True,
            'details': {},
            'errors': []
        }
        
        try:
            # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”
            model1 = SimpleTestModel(in_channels=3, out_channels=3, hidden_dim=32)
            model2 = SimpleTestModel(in_channels=3, out_channels=3, hidden_dim=64)
            
            models = {'small': model1, 'large': model2}
            
            # æµ‹è¯•æ•°æ®
            input_shape = (2, 3, 128, 128)
            x = torch.randn(input_shape).to(self.device)
            target = torch.randn(input_shape).to(self.device)
            
            # å¤šç§å­æµ‹è¯•
            seeds = [42, 123, 456]  # â‰¥3ç§å­
            comparison_results = {}
            
            for model_name, model in models.items():
                model = model.to(self.device)
                
                # èµ„æºç›‘æ§
                if 'ResourceMonitor' in globals():
                    monitor = ResourceMonitor()
                    resource_info = monitor.profile_model(model, input_shape)
                else:
                    # ç®€åŒ–ç‰ˆæœ¬
                    total_params = sum(p.numel() for p in model.parameters())
                    resource_info = {
                        'params': {'params_M': total_params / 1e6},
                        'flops': {'flops_G': 0.0},
                        'memory': {'peak_memory_GB': 0.0},
                        'latency': {'mean_latency_ms': 0.0}
                    }
                
                # å¤šç§å­æŒ‡æ ‡æµ‹è¯•
                metrics_per_seed = []
                
                for seed in seeds:
                    self.set_seed(seed)
                    
                    with torch.no_grad():
                        pred = model(x)
                    
                    # è®¡ç®—æŒ‡æ ‡
                    if 'MetricsCalculator' in globals():
                        calculator = MetricsCalculator(image_size=(128, 128))
                        rel_l2 = calculator.compute_rel_l2(pred, target)
                        mae = calculator.compute_mae(pred, target)
                    else:
                        rel_l2 = torch.mean((pred - target) ** 2).item()
                        mae = torch.mean(torch.abs(pred - target)).item()
                    
                    metrics_per_seed.append({
                        'rel_l2': rel_l2,
                        'mae': mae
                    })
                
                # è®¡ç®—ç»Ÿè®¡é‡
                rel_l2_values = [m['rel_l2'] for m in metrics_per_seed]
                mae_values = [m['mae'] for m in metrics_per_seed]
                
                comparison_results[model_name] = {
                    'metrics': {
                        'rel_l2': {
                            'mean': np.mean(rel_l2_values),
                            'std': np.std(rel_l2_values),
                            'values': rel_l2_values
                        },
                        'mae': {
                            'mean': np.mean(mae_values),
                            'std': np.std(mae_values),
                            'values': mae_values
                        }
                    },
                    'resources': resource_info
                }
            
            # éªŒè¯å¯¹æ¯”ç»“æœæ ¼å¼
            for model_name, result in comparison_results.items():
                # æ£€æŸ¥æ˜¯å¦æœ‰å‡å€¼å’Œæ ‡å‡†å·®
                for metric_name in ['rel_l2', 'mae']:
                    metric_data = result['metrics'][metric_name]
                    
                    has_mean = 'mean' in metric_data
                    has_std = 'std' in metric_data
                    has_multiple_seeds = len(metric_data['values']) >= 3
                    
                    results['details'][f'{model_name}_{metric_name}'] = {
                        'has_mean': has_mean,
                        'has_std': has_std,
                        'has_multiple_seeds': has_multiple_seeds,
                        'mean': metric_data['mean'],
                        'std': metric_data['std'],
                        'num_seeds': len(metric_data['values'])
                    }
                    
                    if not (has_mean and has_std and has_multiple_seeds):
                        results['passed'] = False
                        results['errors'].append(
                            f"{model_name}çš„{metric_name}ç¼ºå°‘å®Œæ•´ç»Ÿè®¡ä¿¡æ¯"
                        )
                
                # æ£€æŸ¥èµ„æºä¿¡æ¯
                resource_keys = ['params', 'flops', 'memory', 'latency']
                for key in resource_keys:
                    if key not in result['resources']:
                        results['passed'] = False
                        results['errors'].append(f"{model_name}ç¼ºå°‘{key}èµ„æºä¿¡æ¯")
            
            results['details']['comparison_results'] = comparison_results
            
            print(f"âœ“ å¯æ¯”æ€§è§„åˆ™æµ‹è¯•: {'é€šè¿‡' if results['passed'] else 'å¤±è´¥'}")
            
        except Exception as e:
            results['passed'] = False
            results['errors'].append(f"å¯æ¯”æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}")
            print(f"âŒ å¯æ¯”æ€§è§„åˆ™æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_documentation_first_rule(self) -> Dict[str, Any]:
        """æµ‹è¯•æ–‡æ¡£å…ˆè¡Œè§„åˆ™
        
        éªŒè¯å…³é”®æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        """
        print("æµ‹è¯•æ–‡æ¡£å…ˆè¡Œè§„åˆ™...")
        
        results = {
            'passed': True,
            'details': {},
            'errors': []
        }
        
        try:
            # æ£€æŸ¥å…³é”®æ–‡æ¡£
            doc_paths = [
                '.trae/documents/PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ-äº§å“éœ€æ±‚æ–‡æ¡£.md',
                '.trae/documents/PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ-æŠ€æœ¯æ¶æ„æ–‡æ¡£.md',
                'README.md'
            ]
            
            for doc_path in doc_paths:
                exists = os.path.exists(doc_path)
                results['details'][doc_path] = {
                    'exists': exists,
                    'path': doc_path
                }
                
                if not exists:
                    results['errors'].append(f"ç¼ºå°‘æ–‡æ¡£: {doc_path}")
                    # ä¸è®¾ä¸ºå¤±è´¥ï¼Œå› ä¸ºæŸäº›æ–‡æ¡£å¯èƒ½åœ¨ä¸åŒä½ç½®
            
            # æ£€æŸ¥ä»£ç æ–‡æ¡£
            code_files = [
                'utils/metrics.py',
                'ops/degradation.py',
                'models/__init__.py'
            ]
            
            documented_files = 0
            for code_file in code_files:
                if os.path.exists(code_file):
                    with open(code_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        has_docstring = '"""' in content or "'''" in content
                        
                        results['details'][f'{code_file}_documented'] = {
                            'exists': True,
                            'has_docstring': has_docstring
                        }
                        
                        if has_docstring:
                            documented_files += 1
                else:
                    results['details'][f'{code_file}_documented'] = {
                        'exists': False,
                        'has_docstring': False
                    }
            
            # è‡³å°‘50%çš„ä»£ç æ–‡ä»¶åº”è¯¥æœ‰æ–‡æ¡£
            doc_ratio = documented_files / len(code_files) if code_files else 0
            results['details']['documentation_ratio'] = {
                'ratio': doc_ratio,
                'documented_files': documented_files,
                'total_files': len(code_files),
                'passed': doc_ratio >= 0.5
            }
            
            if doc_ratio < 0.5:
                results['errors'].append(f"ä»£ç æ–‡æ¡£è¦†ç›–ç‡è¿‡ä½: {doc_ratio:.1%}")
            
            print(f"âœ“ æ–‡æ¡£å…ˆè¡Œè§„åˆ™æµ‹è¯•: {'é€šè¿‡' if results['passed'] else 'å¤±è´¥'}")
            
        except Exception as e:
            results['passed'] = False
            results['errors'].append(f"æ–‡æ¡£æ£€æŸ¥å¼‚å¸¸: {str(e)}")
            print(f"âŒ æ–‡æ¡£å…ˆè¡Œè§„åˆ™æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def generate_compliance_report(self, results: Dict[str, Dict]) -> str:
        """ç”Ÿæˆåˆè§„æŠ¥å‘Š"""
        report = []
        report.append("PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ - é»„é‡‘æ³•åˆ™åˆè§„æŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # æ€»ä½“çŠ¶æ€
        all_passed = all(result['passed'] for result in results.values())
        report.append(f"æ€»ä½“çŠ¶æ€: {'âœ“ é€šè¿‡' if all_passed else 'âŒ å¤±è´¥'}")
        report.append("")
        
        # å„è§„åˆ™è¯¦æƒ…
        rule_names = {
            'consistency': '1. ä¸€è‡´æ€§ä¼˜å…ˆ',
            'reproducibility': '2. å¯å¤ç°æ€§',
            'unified_interface': '3. ç»Ÿä¸€æ¥å£',
            'comparability': '4. å¯æ¯”æ€§',
            'documentation_first': '5. æ–‡æ¡£å…ˆè¡Œ'
        }
        
        for rule_key, rule_name in rule_names.items():
            if rule_key in results:
                result = results[rule_key]
                status = 'âœ“ é€šè¿‡' if result['passed'] else 'âŒ å¤±è´¥'
                report.append(f"{rule_name}: {status}")
                
                if result['errors']:
                    for error in result['errors']:
                        report.append(f"  - {error}")
                
                report.append("")
        
        # å»ºè®®
        report.append("æ”¹è¿›å»ºè®®:")
        if not all_passed:
            for rule_key, result in results.items():
                if not result['passed']:
                    rule_name = rule_names.get(rule_key, rule_key)
                    report.append(f"- {rule_name}: éœ€è¦ä¿®å¤ä¸Šè¿°é”™è¯¯")
        else:
            report.append("- æ‰€æœ‰é»„é‡‘æ³•åˆ™å‡å·²éµå¾ªï¼Œç³»ç»Ÿåˆè§„æ€§è‰¯å¥½")
        
        return "\n".join(report)
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰é»„é‡‘æ³•åˆ™æµ‹è¯•"""
        print("PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ - é»„é‡‘æ³•åˆ™åˆè§„æµ‹è¯•")
        print("=" * 60)
        
        results = {}
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        results['consistency'] = self.test_consistency_rule()
        results['reproducibility'] = self.test_reproducibility_rule()
        results['unified_interface'] = self.test_unified_interface_rule()
        results['comparability'] = self.test_comparability_rule()
        results['documentation_first'] = self.test_documentation_first_rule()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_compliance_report(results)
        print("\n" + report)
        
        # ä¿å­˜ç»“æœ
        self.results = results
        
        return results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    validator = GoldenRulesValidator()
    
    try:
        results = validator.run_all_tests()
        
        # æ£€æŸ¥æ€»ä½“ç»“æœ
        all_passed = all(result['passed'] for result in results.values())
        
        if all_passed:
            print("\nğŸ‰ æ‰€æœ‰é»„é‡‘æ³•åˆ™æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿåˆè§„æ€§è‰¯å¥½ã€‚")
            return 0
        else:
            print("\nâš ï¸ éƒ¨åˆ†é»„é‡‘æ³•åˆ™æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ”¹è¿›ã€‚")
            return 1
            
    except Exception as e:
        print(f"\nâŒ é»„é‡‘æ³•åˆ™æµ‹è¯•å¼‚å¸¸: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)