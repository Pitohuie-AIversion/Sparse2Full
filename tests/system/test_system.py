#!/usr/bin/env python3
"""PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿæµ‹è¯•è„šæœ¬

å®Œæ•´çš„ç³»ç»Ÿæµ‹è¯•ï¼ŒéªŒè¯æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚
ä¸¥æ ¼æŒ‰ç…§å¼€å‘æ‰‹å†Œçš„é»„é‡‘æ³•åˆ™è¿›è¡Œæµ‹è¯•ã€‚

é»„é‡‘æ³•åˆ™ï¼š
1. ä¸€è‡´æ€§ä¼˜å…ˆï¼šè§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCå¿…é¡»å¤ç”¨åŒä¸€å®ç°ä¸é…ç½®
2. å¯å¤ç°ï¼šåŒä¸€YAML+ç§å­ï¼ŒéªŒè¯æŒ‡æ ‡æ–¹å·®â‰¤1e-4
3. ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰æ¨¡å‹forward(x[B,C_in,H,W])â†’y[B,C_out,H,W]
4. å¯æ¯”æ€§ï¼šæ¨ªå‘å¯¹æ¯”å¿…é¡»æŠ¥å‘Šå‡å€¼Â±æ ‡å‡†å·®ï¼ˆâ‰¥3ç§å­ï¼‰+èµ„æºæˆæœ¬
5. æ–‡æ¡£å…ˆè¡Œï¼šæ–°å¢ä»»åŠ¡/ç®—å­/æ¨¡å‹å‰ï¼Œå…ˆæäº¤PRD/æŠ€æœ¯æ–‡æ¡£è¡¥ä¸

ä½¿ç”¨æ–¹æ³•:
    python test_system.py
    python test_system.py --quick  # å¿«é€Ÿæµ‹è¯•
    python test_system.py --full   # å®Œæ•´æµ‹è¯•
"""

import os
import sys
import json
import yaml
import tempfile
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import warnings

import numpy as np
import torch
import torch.nn as nn
from omegaconf import OmegaConf, DictConfig

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from models.swin_unet import SwinUNet
from models.hybrid import HybridModel
from models.mlp import MLPModel
from ops.degradation import SuperResolutionOperator, CropOperator
from ops.loss import TotalLoss
from datasets.pdebench import PDEBenchDataModule
from utils.reproducibility import set_seed, get_environment_info


class SystemTester:
    """ç³»ç»Ÿæµ‹è¯•å™¨
    
    æµ‹è¯•æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ¨¡å‹æ¥å£ä¸€è‡´æ€§
    2. è§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCä¸€è‡´æ€§
    3. æŸå¤±å‡½æ•°è®¡ç®—åŸŸæ­£ç¡®æ€§
    4. å¯å¤ç°æ€§éªŒè¯
    5. é…ç½®åŠ è½½å’ŒéªŒè¯
    """
    
    def __init__(self, quick_mode: bool = False):
        """
        Args:
            quick_mode: æ˜¯å¦å¿«é€Ÿæµ‹è¯•æ¨¡å¼
        """
        self.quick_mode = quick_mode
        self.test_results = {}
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            'batch_size': 2 if quick_mode else 4,
            'img_size': 64 if quick_mode else 128,
            'num_samples': 5 if quick_mode else 20,
            'tolerance': 1e-6,
            'seed': 2025
        }
        
        # è®¾ç½®éšæœºç§å­
        set_seed(self.test_config['seed'])
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger.info(f"Using device: {self.device}")
    
    def test_model_interfaces(self) -> bool:
        """æµ‹è¯•æ¨¡å‹æ¥å£ä¸€è‡´æ€§
        
        é»„é‡‘æ³•åˆ™3ï¼šç»Ÿä¸€æ¥å£ - æ‰€æœ‰æ¨¡å‹forward(x[B,C_in,H,W])â†’y[B,C_out,H,W]
        
        Returns:
            success: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        self.logger.info("Testing model interfaces...")
        
        try:
            batch_size = self.test_config['batch_size']
            img_size = self.test_config['img_size']
            in_channels = 3
            out_channels = 3
            
            # æµ‹è¯•è¾“å…¥
            x = torch.randn(batch_size, in_channels, img_size, img_size).to(self.device)
            
            # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
            models_to_test = [
                ('SwinUNet', SwinUNet, {
                    'in_channels': in_channels,
                    'out_channels': out_channels,
                    'img_size': img_size,
                    'patch_size': 4,
                    'window_size': 8,
                    'depths': [2, 2, 2, 2],
                    'num_heads': [3, 6, 12, 24],
                    'embed_dim': 96
                }),
                ('HybridModel', HybridModel, {
                    'in_channels': in_channels,
                    'out_channels': out_channels,
                    'img_size': img_size,
                    'hidden_dim': 128,
                    'num_layers': 4
                }),
                ('MLPModel', MLPModel, {
                    'in_channels': in_channels,
                    'out_channels': out_channels,
                    'img_size': img_size,
                    'hidden_dim': 256,
                    'num_layers': 4
                })
            ]
            
            interface_results = {}
            
            for model_name, model_class, model_kwargs in models_to_test:
                self.logger.info(f"Testing {model_name}...")
                
                try:
                    # åˆ›å»ºæ¨¡å‹
                    model = model_class(**model_kwargs).to(self.device)
                    model.eval()
                    
                    # å‰å‘ä¼ æ’­
                    with torch.no_grad():
                        y = model(x)
                    
                    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
                    expected_shape = (batch_size, out_channels, img_size, img_size)
                    if y.shape != expected_shape:
                        raise ValueError(f"Output shape mismatch: got {y.shape}, expected {expected_shape}")
                    
                    # æ£€æŸ¥è¾“å‡ºå€¼åŸŸï¼ˆåº”è¯¥æ˜¯æœ‰é™çš„ï¼‰
                    if not torch.isfinite(y).all():
                        raise ValueError("Output contains non-finite values")
                    
                    interface_results[model_name] = {
                        'success': True,
                        'input_shape': list(x.shape),
                        'output_shape': list(y.shape),
                        'output_range': [float(y.min()), float(y.max())],
                        'params': sum(p.numel() for p in model.parameters())
                    }
                    
                    self.logger.info(f"âœ… {model_name} interface test passed")
                    
                except Exception as e:
                    interface_results[model_name] = {
                        'success': False,
                        'error': str(e)
                    }
                    self.logger.error(f"âŒ {model_name} interface test failed: {e}")
            
            # æ±‡æ€»ç»“æœ
            success_count = sum(1 for r in interface_results.values() if r['success'])
            total_count = len(interface_results)
            
            self.test_results['model_interfaces'] = {
                'success': success_count == total_count,
                'passed': success_count,
                'total': total_count,
                'details': interface_results
            }
            
            self.logger.info(f"Model interface tests: {success_count}/{total_count} passed")
            return success_count == total_count
            
        except Exception as e:
            self.logger.error(f"Model interface testing failed: {e}")
            self.test_results['model_interfaces'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def test_observation_consistency(self) -> bool:
        """æµ‹è¯•è§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCä¸€è‡´æ€§
        
        é»„é‡‘æ³•åˆ™1ï¼šä¸€è‡´æ€§ä¼˜å…ˆ - è§‚æµ‹ç®—å­Hä¸è®­ç»ƒDCå¿…é¡»å¤ç”¨åŒä¸€å®ç°ä¸é…ç½®
        
        Returns:
            success: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        self.logger.info("Testing observation operator consistency...")
        
        try:
            batch_size = self.test_config['batch_size']
            img_size = self.test_config['img_size']
            channels = 3
            tolerance = self.test_config['tolerance']
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            gt = torch.randn(batch_size, channels, img_size, img_size)
            
            # æµ‹è¯•è¶…åˆ†è¾¨ç‡è§‚æµ‹ç®—å­
            sr_config = {
                'scale_factor': 2,
                'blur_sigma': 1.0,
                'blur_kernel_size': 5,
                'boundary_mode': 'mirror'
            }
            
            sr_degradation = SuperResolutionOperator(
                scale=sr_config['scale_factor'],
                sigma=sr_config['blur_sigma'],
                kernel_size=sr_config['blur_kernel_size'],
                boundary=sr_config['boundary_mode']
            )
            
            # ç›´æ¥åº”ç”¨è§‚æµ‹ç®—å­
            y_direct = sr_degradation(gt)
            
            # é€šè¿‡æŸå¤±å‡½æ•°åº”ç”¨è§‚æµ‹ç®—å­
            loss_fn = TotalLoss(
                rec_weight=1.0,
                spec_weight=0.0,  # å…³é—­é¢‘è°±æŸå¤±
                dc_weight=1.0
            )
            
            # æ¨¡æ‹Ÿé¢„æµ‹å€¼ï¼ˆä¸GTç›¸åŒï¼Œè¿™æ ·DCæŸå¤±åº”è¯¥ä¸º0ï¼‰
            pred = gt.clone()
            
            # è®¡ç®—æŸå¤±
            total_loss, loss_dict = loss_fn(pred, gt, y_direct, sr_config)
            dc_loss = loss_dict['data_consistency']
            
            # æ£€æŸ¥DCæŸå¤±æ˜¯å¦æ¥è¿‘0ï¼ˆè¯´æ˜Hç®—å­ä¸€è‡´ï¼‰
            if dc_loss.item() > tolerance:
                raise ValueError(f"DC loss too high: {dc_loss.item()}, expected < {tolerance}")
            
            # æµ‹è¯•è£å‰ªè§‚æµ‹ç®—å­
            crop_config = {
                'task': 'crop',
                'crop_size': [32, 32],
                'crop_box': None,  # ä½¿ç”¨ä¸­å¿ƒè£å‰ª
                'boundary': 'mirror'
            }
            
            crop_degradation = CropOperator(
                crop_size=crop_config['crop_size'],
                boundary=crop_config['boundary']
            )
            
            # ç›´æ¥åº”ç”¨è§‚æµ‹ç®—å­
            y_crop_direct = crop_degradation(gt)
            
            # é€šè¿‡æŸå¤±å‡½æ•°åº”ç”¨è§‚æµ‹ç®—å­
            loss_fn_crop = TotalLoss(
                rec_weight=1.0,
                spec_weight=0.0,
                dc_weight=1.0
            )
            
            total_loss_crop, loss_dict_crop = loss_fn_crop(pred, gt, y_crop_direct, crop_config)
            dc_loss_crop = loss_dict_crop['data_consistency']
            
            if dc_loss_crop.item() > tolerance:
                raise ValueError(f"Crop DC loss too high: {dc_loss_crop.item()}, expected < {tolerance}")
            
            self.test_results['observation_consistency'] = {
                'success': True,
                'sr_dc_loss': float(dc_loss.item()),
                'crop_dc_loss': float(dc_loss_crop.item()),
                'tolerance': tolerance
            }
            
            self.logger.info("âœ… Observation consistency test passed")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Observation consistency test failed: {e}")
            self.test_results['observation_consistency'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def test_loss_computation_domain(self) -> bool:
        """æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—åŸŸæ­£ç¡®æ€§
        
        éªŒè¯ï¼š
        - æ¨¡å‹è¾“å‡ºåœ¨z-scoreåŸŸ
        - DCä¸é¢‘åŸŸæŸå¤±åœ¨åŸå€¼åŸŸè®¡ç®—
        
        Returns:
            success: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        self.logger.info("Testing loss computation domain...")
        
        try:
            batch_size = self.test_config['batch_size']
            img_size = self.test_config['img_size']
            channels = 3
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆåŸå€¼åŸŸï¼‰
            gt_original = torch.randn(batch_size, channels, img_size, img_size) * 10 + 5
            
            # æ¨¡æ‹Ÿå½’ä¸€åŒ–ç»Ÿè®¡é‡
            mean = torch.tensor([5.0, 5.0, 5.0]).view(1, 3, 1, 1)
            std = torch.tensor([10.0, 10.0, 10.0]).view(1, 3, 1, 1)
            
            # å½’ä¸€åŒ–åˆ°z-scoreåŸŸ
            gt_normalized = (gt_original - mean) / std
            
            # æ¨¡æ‹Ÿæ¨¡å‹é¢„æµ‹ï¼ˆz-scoreåŸŸï¼‰
            pred_normalized = gt_normalized + 0.1 * torch.randn_like(gt_normalized)
            
            # åˆ›å»ºè§‚æµ‹æ•°æ®
            sr_config = {
                'scale_factor': 2,
                'blur_sigma': 1.0,
                'blur_kernel_size': 5,
                'boundary_mode': 'mirror'
            }
            
            sr_degradation = SuperResolutionOperator(
                scale=sr_config['scale_factor'],
                sigma=sr_config['blur_sigma'],
                kernel_size=sr_config['blur_kernel_size'],
                boundary=sr_config['boundary_mode']
            )
            y_observed = sr_degradation(gt_original)  # è§‚æµ‹æ•°æ®åœ¨åŸå€¼åŸŸ
            
            # åˆ›å»ºæŸå¤±å‡½æ•°
            loss_fn = TotalLoss(
                rec_weight=1.0,
                spec_weight=0.5,
                dc_weight=1.0,
                denormalize_fn=lambda x: x * std + mean
            )
            
            # è®¡ç®—æŸå¤±
            total_loss, loss_dict = loss_fn(pred_normalized, gt_normalized, y_observed, sr_config)
            
            # æ£€æŸ¥å„é¡¹æŸå¤±éƒ½æ˜¯æœ‰é™å€¼
            for loss_name, loss_value in loss_dict.items():
                if not torch.isfinite(loss_value):
                    raise ValueError(f"{loss_name} is not finite: {loss_value}")
            
            # æ£€æŸ¥æ€»æŸå¤±åˆç†æ€§
            if total_loss.item() < 0 or total_loss.item() > 1000:
                raise ValueError(f"Total loss out of reasonable range: {total_loss.item()}")
            
            self.test_results['loss_computation_domain'] = {
                'success': True,
                'losses': {k: float(v.item()) if torch.is_tensor(v) else float(v) for k, v in loss_dict.items()},
                'gt_original_range': [float(gt_original.min()), float(gt_original.max())],
                'gt_normalized_range': [float(gt_normalized.min()), float(gt_normalized.max())],
                'pred_normalized_range': [float(pred_normalized.min()), float(pred_normalized.max())]
            }
            
            self.logger.info("âœ… Loss computation domain test passed")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Loss computation domain test failed: {e}")
            self.test_results['loss_computation_domain'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def test_reproducibility(self) -> bool:
        """æµ‹è¯•å¯å¤ç°æ€§
        
        é»„é‡‘æ³•åˆ™2ï¼šå¯å¤ç° - åŒä¸€YAML+ç§å­ï¼ŒéªŒè¯æŒ‡æ ‡æ–¹å·®â‰¤1e-4
        
        Returns:
            success: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        self.logger.info("Testing reproducibility...")
        
        try:
            num_runs = 3 if self.quick_mode else 5
            tolerance = 1e-4
            
            # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
            model_config = {
                'in_channels': 3,
                'out_channels': 3,
                'img_size': 32,
                'hidden_dim': 64,
                'num_layers': 2
            }
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            batch_size = 2
            x = torch.randn(batch_size, 3, 32, 32)
            
            results = []
            
            for run in range(num_runs):
                # è®¾ç½®ç›¸åŒçš„éšæœºç§å­
                set_seed(self.test_config['seed'])
                
                # åˆ›å»ºæ¨¡å‹
                model = MLPModel(**model_config)
                model.eval()
                
                # å‰å‘ä¼ æ’­
                with torch.no_grad():
                    y = model(x)
                
                # è®°å½•ç»“æœ
                results.append(y.clone())
            
            # æ£€æŸ¥ç»“æœä¸€è‡´æ€§
            reference = results[0]
            max_diff = 0.0
            
            for i, result in enumerate(results[1:], 1):
                diff = torch.abs(result - reference).max().item()
                max_diff = max(max_diff, diff)
                
                if diff > tolerance:
                    raise ValueError(f"Run {i} differs from reference by {diff}, expected < {tolerance}")
            
            # è®¡ç®—æ–¹å·®
            stacked_results = torch.stack(results, dim=0)
            variance = torch.var(stacked_results, dim=0).max().item()
            
            if variance > tolerance:
                raise ValueError(f"Variance {variance} exceeds tolerance {tolerance}")
            
            self.test_results['reproducibility'] = {
                'success': True,
                'num_runs': num_runs,
                'max_difference': max_diff,
                'variance': variance,
                'tolerance': tolerance
            }
            
            self.logger.info(f"âœ… Reproducibility test passed (max_diff: {max_diff:.2e}, variance: {variance:.2e})")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Reproducibility test failed: {e}")
            self.test_results['reproducibility'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def test_config_loading(self) -> bool:
        """æµ‹è¯•é…ç½®åŠ è½½å’ŒéªŒè¯
        
        Returns:
            success: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        self.logger.info("Testing configuration loading...")
        
        try:
            config_files = [
                'configs/config.yaml',
                'configs/train.yaml',
                'configs/eval.yaml',
                'configs/model/swin_unet.yaml',
                'configs/data/pdebench.yaml'
            ]
            
            loaded_configs = {}
            
            for config_file in config_files:
                config_path = Path(config_file)
                if config_path.exists():
                    try:
                        config = OmegaConf.load(config_path)
                        loaded_configs[config_file] = {
                            'success': True,
                            'keys': list(config.keys()) if hasattr(config, 'keys') else []
                        }
                        self.logger.info(f"âœ… Loaded {config_file}")
                    except Exception as e:
                        loaded_configs[config_file] = {
                            'success': False,
                            'error': str(e)
                        }
                        self.logger.warning(f"âš ï¸ Failed to load {config_file}: {e}")
                else:
                    loaded_configs[config_file] = {
                        'success': False,
                        'error': 'File not found'
                    }
                    self.logger.warning(f"âš ï¸ Config file not found: {config_file}")
            
            # æ£€æŸ¥è‡³å°‘æœ‰ä¸€äº›é…ç½®æ–‡ä»¶æˆåŠŸåŠ è½½
            success_count = sum(1 for c in loaded_configs.values() if c['success'])
            
            self.test_results['config_loading'] = {
                'success': success_count > 0,
                'loaded_configs': loaded_configs,
                'success_count': success_count,
                'total_count': len(config_files)
            }
            
            self.logger.info(f"Configuration loading: {success_count}/{len(config_files)} files loaded")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ Configuration loading test failed: {e}")
            self.test_results['config_loading'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def test_tools_functionality(self) -> bool:
        """æµ‹è¯•å·¥å…·è„šæœ¬åŠŸèƒ½
        
        Returns:
            success: æµ‹è¯•æ˜¯å¦é€šè¿‡
        """
        self.logger.info("Testing tools functionality...")
        
        try:
            tools_results = {}
            
            # æµ‹è¯•è®­ç»ƒè„šæœ¬å¸®åŠ©ä¿¡æ¯
            try:
                import subprocess
                result = subprocess.run([
                    sys.executable, 'tools/train.py', '--help'
                ], capture_output=True, text=True, timeout=30)
                
                tools_results['train.py'] = {
                    'success': result.returncode == 0,
                    'help_available': '--help' in result.stdout or 'usage:' in result.stdout
                }
            except Exception as e:
                tools_results['train.py'] = {
                    'success': False,
                    'error': str(e)
                }
            
            # æµ‹è¯•è¯„ä¼°è„šæœ¬å¸®åŠ©ä¿¡æ¯
            try:
                result = subprocess.run([
                    sys.executable, 'tools/eval.py', '--help'
                ], capture_output=True, text=True, timeout=30)
                
                tools_results['eval.py'] = {
                    'success': result.returncode == 0,
                    'help_available': '--help' in result.stdout or 'usage:' in result.stdout
                }
            except Exception as e:
                tools_results['eval.py'] = {
                    'success': False,
                    'error': str(e)
                }
            
            # æµ‹è¯•ä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬å¸®åŠ©ä¿¡æ¯
            try:
                result = subprocess.run([
                    sys.executable, 'tools/check_dc_equivalence.py', '--help'
                ], capture_output=True, text=True, timeout=30)
                
                tools_results['check_dc_equivalence.py'] = {
                    'success': result.returncode == 0,
                    'help_available': '--help' in result.stdout or 'usage:' in result.stdout
                }
            except Exception as e:
                tools_results['check_dc_equivalence.py'] = {
                    'success': False,
                    'error': str(e)
                }
            
            # æµ‹è¯•è®ºæ–‡åŒ…ç”Ÿæˆè„šæœ¬å¸®åŠ©ä¿¡æ¯
            try:
                result = subprocess.run([
                    sys.executable, 'tools/generate_paper_package.py', '--help'
                ], capture_output=True, text=True, timeout=30)
                
                tools_results['generate_paper_package.py'] = {
                    'success': result.returncode == 0,
                    'help_available': '--help' in result.stdout or 'usage:' in result.stdout
                }
            except Exception as e:
                tools_results['generate_paper_package.py'] = {
                    'success': False,
                    'error': str(e)
                }
            
            success_count = sum(1 for r in tools_results.values() if r['success'])
            total_count = len(tools_results)
            
            self.test_results['tools_functionality'] = {
                'success': success_count == total_count,
                'tools_results': tools_results,
                'success_count': success_count,
                'total_count': total_count
            }
            
            self.logger.info(f"Tools functionality: {success_count}/{total_count} tools working")
            return success_count == total_count
            
        except Exception as e:
            self.logger.error(f"âŒ Tools functionality test failed: {e}")
            self.test_results['tools_functionality'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•
        
        Returns:
            test_results: æµ‹è¯•ç»“æœæ±‡æ€»
        """
        self.logger.info("="*60)
        self.logger.info("PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ - å®Œæ•´æµ‹è¯•å¥—ä»¶")
        self.logger.info("="*60)
        
        # è®°å½•ç¯å¢ƒä¿¡æ¯
        env_info = get_environment_info()
        self.test_results['environment'] = env_info
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        tests = [
            ('æ¨¡å‹æ¥å£ä¸€è‡´æ€§', self.test_model_interfaces),
            ('è§‚æµ‹ç®—å­Hä¸DCä¸€è‡´æ€§', self.test_observation_consistency),
            ('æŸå¤±å‡½æ•°è®¡ç®—åŸŸ', self.test_loss_computation_domain),
            ('å¯å¤ç°æ€§', self.test_reproducibility),
            ('é…ç½®åŠ è½½', self.test_config_loading),
            ('å·¥å…·è„šæœ¬åŠŸèƒ½', self.test_tools_functionality)
        ]
        
        passed_tests = 0
        total_tests = len(tests)
        
        for test_name, test_func in tests:
            self.logger.info(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
            try:
                success = test_func()
                if success:
                    passed_tests += 1
                    self.logger.info(f"âœ… {test_name} - é€šè¿‡")
                else:
                    self.logger.error(f"âŒ {test_name} - å¤±è´¥")
            except Exception as e:
                self.logger.error(f"âŒ {test_name} - å¼‚å¸¸: {e}")
        
        # æ±‡æ€»ç»“æœ
        overall_success = passed_tests == total_tests
        
        self.test_results['summary'] = {
            'overall_success': overall_success,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'success_rate': passed_tests / total_tests,
            'test_mode': 'quick' if self.quick_mode else 'full'
        }
        
        self.logger.info("\n" + "="*60)
        self.logger.info("æµ‹è¯•ç»“æœæ±‡æ€»")
        self.logger.info("="*60)
        self.logger.info(f"æ€»ä½“ç»“æœ: {'âœ… é€šè¿‡' if overall_success else 'âŒ å¤±è´¥'}")
        self.logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        self.logger.info(f"æˆåŠŸç‡: {passed_tests/total_tests:.1%}")
        self.logger.info(f"æµ‹è¯•æ¨¡å¼: {'å¿«é€Ÿ' if self.quick_mode else 'å®Œæ•´'}")
        
        if overall_success:
            self.logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»ŸåŠŸèƒ½æ­£å¸¸ã€‚")
            self.logger.info("âœ… ç¬¦åˆå¼€å‘æ‰‹å†Œçš„é»„é‡‘æ³•åˆ™è¦æ±‚")
            self.logger.info("âœ… å¯ä»¥è¿›è¡Œå®é™…è®­ç»ƒå’Œè¯„ä¼°")
        else:
            self.logger.error("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
            self.logger.error("âŒ å»ºè®®ä¿®å¤å¤±è´¥çš„æµ‹è¯•åå†è¿›è¡Œè®­ç»ƒ")
        
        return self.test_results
    
    def save_test_report(self, output_path: str = "test_report.json"):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿæµ‹è¯•")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--full", action="store_true", help="å®Œæ•´æµ‹è¯•æ¨¡å¼")
    parser.add_argument("--output", type=str, default="test_report.json", help="æµ‹è¯•æŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    # ç¡®å®šæµ‹è¯•æ¨¡å¼
    quick_mode = args.quick or not args.full
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = SystemTester(quick_mode=quick_mode)
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_all_tests()
    
    # ä¿å­˜æŠ¥å‘Š
    tester.save_test_report(args.output)
    
    # è®¾ç½®é€€å‡ºç 
    exit_code = 0 if results['summary']['overall_success'] else 1
    sys.exit(exit_code)


if __name__ == "__main__":
    main()