#!/usr/bin/env python3
"""
PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ - æµ‹è¯•è¿è¡Œå™¨

ç»Ÿä¸€è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼Œç”Ÿæˆæµ‹è¯•æŠ¥å‘Šå’Œè¦†ç›–ç‡ç»Ÿè®¡ã€‚
éµå¾ªå¼€å‘æ‰‹å†Œçš„æµ‹è¯•è¦æ±‚ï¼Œç¡®ä¿ç³»ç»Ÿè´¨é‡ã€‚
"""

import os
import sys
import subprocess
import argparse
import time
from pathlib import Path
from typing import List, Dict, Any

def run_command(cmd: List[str], cwd: Path = None) -> Dict[str, Any]:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        start_time = time.time()
        result = subprocess.run(
            cmd,
            cwd=cwd,
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        end_time = time.time()
        
        return {
            'success': result.returncode == 0,
            'returncode': result.returncode,
            'stdout': result.stdout,
            'stderr': result.stderr,
            'duration': end_time - start_time
        }
    except subprocess.TimeoutExpired:
        return {
            'success': False,
            'returncode': -1,
            'stdout': '',
            'stderr': 'Command timed out after 300 seconds',
            'duration': 300
        }
    except Exception as e:
        return {
            'success': False,
            'returncode': -1,
            'stdout': '',
            'stderr': str(e),
            'duration': 0
        }

def run_test_suite(test_file: str, python_path: str, project_root: Path) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œæµ‹è¯•: {test_file}")
    print(f"{'='*60}")
    
    cmd = [python_path, "-m", "pytest", f"tests/{test_file}", "-v", "--tb=short"]
    result = run_command(cmd, cwd=project_root)
    
    if result['success']:
        print(f"âœ… {test_file} æµ‹è¯•é€šè¿‡")
    else:
        print(f"âŒ {test_file} æµ‹è¯•å¤±è´¥")
        if result['stderr']:
            print(f"é”™è¯¯ä¿¡æ¯: {result['stderr']}")
    
    print(f"æ‰§è¡Œæ—¶é—´: {result['duration']:.2f}ç§’")
    
    return result

def run_coverage_analysis(python_path: str, project_root: Path) -> Dict[str, Any]:
    """è¿è¡Œè¦†ç›–ç‡åˆ†æ"""
    print(f"\n{'='*60}")
    print("è¿è¡Œè¦†ç›–ç‡åˆ†æ")
    print(f"{'='*60}")
    
    # å®‰è£…coverageåŒ…ï¼ˆå¦‚æœéœ€è¦ï¼‰
    install_cmd = [python_path, "-m", "pip", "install", "coverage", "pytest-cov"]
    install_result = run_command(install_cmd, cwd=project_root)
    
    if not install_result['success']:
        print("âš ï¸  æ— æ³•å®‰è£…coverageåŒ…ï¼Œè·³è¿‡è¦†ç›–ç‡åˆ†æ")
        return {'success': False, 'message': 'Coverage package not available'}
    
    # è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
    coverage_cmd = [
        python_path, "-m", "pytest",
        "--cov=models",
        "--cov=datasets", 
        "--cov=ops",
        "--cov=utils",
        "--cov=tools",
        "--cov-report=term-missing",
        "--cov-report=html:htmlcov",
        "tests/",
        "-v"
    ]
    
    result = run_command(coverage_cmd, cwd=project_root)
    
    if result['success']:
        print("âœ… è¦†ç›–ç‡åˆ†æå®Œæˆ")
        print("ğŸ“Š è¦†ç›–ç‡æŠ¥å‘Šå·²ç”Ÿæˆåˆ° htmlcov/ ç›®å½•")
    else:
        print("âŒ è¦†ç›–ç‡åˆ†æå¤±è´¥")
    
    return result

def generate_test_report(results: Dict[str, Dict[str, Any]], output_file: Path):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    total_tests = len(results)
    passed_tests = sum(1 for r in results.values() if r['success'])
    failed_tests = total_tests - passed_tests
    total_duration = sum(r['duration'] for r in results.values())
    
    report = f"""# PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿ - æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- æ€»æµ‹è¯•å¥—ä»¶æ•°: {total_tests}
- é€šè¿‡æµ‹è¯•: {passed_tests}
- å¤±è´¥æµ‹è¯•: {failed_tests}
- æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%
- æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’

## è¯¦ç»†ç»“æœ

"""
    
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
        report += f"### {test_name}\n"
        report += f"- çŠ¶æ€: {status}\n"
        report += f"- æ‰§è¡Œæ—¶é—´: {result['duration']:.2f}ç§’\n"
        
        if not result['success']:
            report += f"- é”™è¯¯ä¿¡æ¯: {result['stderr'][:200]}...\n"
        
        report += "\n"
    
    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“‹ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PDEBenchæµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--python', type=str, default='python', help='Pythonè§£é‡Šå™¨è·¯å¾„')
    parser.add_argument('--coverage', action='store_true', help='è¿è¡Œè¦†ç›–ç‡åˆ†æ')
    parser.add_argument('--output', type=str, default='test_report.md', help='æµ‹è¯•æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--tests', nargs='*', help='æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    
    # é»˜è®¤æµ‹è¯•å¥—ä»¶
    default_test_suites = [
        'test_e2e_comprehensive.py',
        'test_model_interface_consistency.py', 
        'test_data_pipeline.py',
        'unit/test_models.py',
        'unit/test_datasets.py',
        'unit/test_losses.py',
        'unit/test_metrics.py',
        'unit/test_ops.py'
    ]
    
    # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•
    test_suites = args.tests if args.tests else default_test_suites
    
    print("ğŸš€ å¼€å§‹è¿è¡ŒPDEBenchæµ‹è¯•å¥—ä»¶")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"Pythonè§£é‡Šå™¨: {args.python}")
    print(f"æµ‹è¯•å¥—ä»¶: {test_suites}")
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    for test_file in test_suites:
        test_path = project_root / "tests" / test_file
        if test_path.exists():
            results[test_file] = run_test_suite(test_file, args.python, project_root)
        else:
            print(f"âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            results[test_file] = {
                'success': False,
                'returncode': -1,
                'stdout': '',
                'stderr': f'Test file not found: {test_file}',
                'duration': 0
            }
    
    # è¿è¡Œè¦†ç›–ç‡åˆ†æ
    if args.coverage:
        coverage_result = run_coverage_analysis(args.python, project_root)
        results['coverage_analysis'] = coverage_result
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    output_path = project_root / args.output
    generate_test_report(results, output_path)
    
    # æ‰“å°æ€»ç»“
    total_tests = len([r for k, r in results.items() if k != 'coverage_analysis'])
    passed_tests = sum(1 for k, r in results.items() if k != 'coverage_analysis' and r['success'])
    
    print(f"\n{'='*60}")
    print("æµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"æ€»æµ‹è¯•å¥—ä»¶: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    print(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æµ‹è¯•æŠ¥å‘Š")
        sys.exit(1)

if __name__ == "__main__":
    main()