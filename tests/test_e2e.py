"""ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬

éªŒè¯PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿçš„å®Œæ•´è®­ç»ƒ-è¯„æµ‹æµç¨‹
ç¡®ä¿å„ç»„ä»¶æ­£ç¡®ååŒå·¥ä½œï¼Œæ»¡è¶³æŠ€æœ¯æ¶æ„æ–‡æ¡£çš„è¦æ±‚

æµ‹è¯•å†…å®¹ï¼š
1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
2. æ¨¡å‹åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­
3. æŸå¤±å‡½æ•°è®¡ç®—
4. è®­ç»ƒå¾ªç¯
5. è¯„æµ‹æŒ‡æ ‡è®¡ç®—
6. å¯è§†åŒ–ç”Ÿæˆ
7. ä¸€è‡´æ€§éªŒè¯
"""

import os
import sys
import tempfile
import shutil
import torch
import torch.nn as nn
import numpy as np
import pytest
from pathlib import Path
from typing import Dict, Any, Optional
import yaml
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from datasets.darcy_flow_dataset import DarcyFlowDataset
from models.swin_unet import SwinUNet
from models.hybrid import HybridModel
from models.mlp import MLPModel
from ops.loss import TotalLoss as CombinedLoss
from utils.metrics import MetricsCalculator as PDEBenchMetrics
from utils.visualization import PDEBenchVisualizer
from utils.distributed import setup_distributed, cleanup_distributed
from ops.degradation import apply_degradation_operator
from tools.check_dc_equivalence import DataConsistencyChecker
from tools.eval import Evaluator


class E2ETestSuite:
    """ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, test_dir: Optional[str] = None):
        """
        Args:
            test_dir: æµ‹è¯•ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä¸´æ—¶ç›®å½•
        """
        if test_dir is None:
            self.test_dir = Path(tempfile.mkdtemp(prefix="pde_e2e_test_"))
        else:
            self.test_dir = Path(test_dir)
        
        self.test_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯•å­ç›®å½•
        (self.test_dir / 'data').mkdir(exist_ok=True)
        (self.test_dir / 'configs').mkdir(exist_ok=True)
        (self.test_dir / 'runs').mkdir(exist_ok=True)
        (self.test_dir / 'outputs').mkdir(exist_ok=True)
        
        # æµ‹è¯•é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.batch_size = 2
        self.img_size = 64  # å°å°ºå¯¸ç”¨äºå¿«é€Ÿæµ‹è¯•
        self.num_epochs = 2
        self.num_samples = 4
        
        print(f"E2Eæµ‹è¯•ç›®å½•: {self.test_dir}")
        print(f"æµ‹è¯•è®¾å¤‡: {self.device}")
    
    def setup_test_data(self) -> Dict[str, Any]:
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        print("è®¾ç½®æµ‹è¯•æ•°æ®...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        data_dir = self.test_dir / 'data'
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„Darcy Flowæ•°æ®
        np.random.seed(42)
        torch.manual_seed(42)
        
        for split in ['train', 'val', 'test']:
            split_dir = data_dir / split
            split_dir.mkdir(exist_ok=True)
            
            num_files = 4 if split == 'train' else 2
            
            for i in range(num_files):
                # ç”Ÿæˆæ¨¡æ‹Ÿçš„PDEè§£
                x = np.linspace(0, 1, self.img_size)
                y = np.linspace(0, 1, self.img_size)
                X, Y = np.meshgrid(x, y)
                
                # æ¨¡æ‹ŸDarcy Flowè§£ï¼ˆç®€å•çš„é«˜æ–¯å‡½æ•°ç»„åˆï¼‰
                solution = (np.exp(-((X-0.3)**2 + (Y-0.3)**2) / 0.1) + 
                           0.5 * np.exp(-((X-0.7)**2 + (Y-0.7)**2) / 0.05))
                
                # æ·»åŠ å™ªå£°
                solution += 0.01 * np.random.randn(*solution.shape)
                
                # ä¿å­˜ä¸ºnumpyæ–‡ä»¶
                np.save(split_dir / f'sample_{i:03d}.npy', solution.astype(np.float32))
        
        # åˆ›å»ºæ•°æ®é›†é…ç½®
        dataset_config = {
            'data_dir': str(data_dir),
            'img_size': self.img_size,
            'normalize': True,
            'augment': False
        }
        
        return dataset_config
    
    def setup_test_configs(self) -> Dict[str, Dict[str, Any]]:
        """è®¾ç½®æµ‹è¯•é…ç½®"""
        print("è®¾ç½®æµ‹è¯•é…ç½®...")
        
        configs_dir = self.test_dir / 'configs'
        
        # åŸºç¡€é…ç½®
        base_config = {
            'experiment': {
                'name': 'e2e_test',
                'seed': 42,
                'device': str(self.device)
            },
            'data': {
                'dataset': 'darcy_flow',
                'batch_size': self.batch_size,
                'num_workers': 0,
                'img_size': self.img_size
            },
            'training': {
                'epochs': self.num_epochs,
                'lr': 1e-3,
                'weight_decay': 1e-4,
                'grad_clip': 1.0,
                'amp': False,  # å…³é—­AMPä»¥ç®€åŒ–æµ‹è¯•
                'save_freq': 1
            },
            'loss': {
                'rec_weight': 1.0,
                'spec_weight': 0.5,
                'dc_weight': 1.0
            },
            'eval': {
                'batch_size': self.batch_size,
                'metrics': {
                    'img_size': self.img_size,
                    'boundary_width': 8
                }
            }
        }
        
        # æ¨¡å‹é…ç½®
        model_configs = {
            'swin_unet': {
                **base_config,
                'model': {
                    'name': 'swin_unet',
                    'in_channels': 1,
                    'out_channels': 1,
                    'img_size': self.img_size,
                    'patch_size': 4,
                    'window_size': 4,
                    'depths': [2, 2],
                    'num_heads': [2, 4],
                    'embed_dim': 48
                }
            },
            'hybrid': {
                **base_config,
                'model': {
                    'name': 'hybrid',
                    'in_channels': 1,
                    'out_channels': 1,
                    'img_size': self.img_size,
                    'hidden_dim': 64,
                    'num_layers': 2
                }
            },
            'mlp': {
                **base_config,
                'model': {
                    'name': 'mlp',
                    'in_channels': 1,
                    'out_channels': 1,
                    'img_size': self.img_size,
                    'hidden_dim': 128,
                    'num_layers': 3
                }
            }
        }
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        for name, config in model_configs.items():
            config_path = configs_dir / f'{name}.yaml'
            with open(config_path, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
        
        return model_configs
    
    def test_data_loading(self, dataset_config: Dict[str, Any]) -> bool:
        """æµ‹è¯•æ•°æ®åŠ è½½"""
        print("æµ‹è¯•æ•°æ®åŠ è½½...")
        
        try:
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = DarcyFlowDataset(
                data_dir=dataset_config['data_dir'],
                split='train',
                img_size=dataset_config['img_size'],
                normalize=dataset_config['normalize']
            )
            
            val_dataset = DarcyFlowDataset(
                data_dir=dataset_config['data_dir'],
                split='val',
                img_size=dataset_config['img_size'],
                normalize=dataset_config['normalize']
            )
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = torch.utils.data.DataLoader(
                train_dataset, batch_size=self.batch_size, shuffle=True
            )
            
            val_loader = torch.utils.data.DataLoader(
                val_dataset, batch_size=self.batch_size, shuffle=False
            )
            
            # æµ‹è¯•æ•°æ®åŠ è½½
            train_batch = next(iter(train_loader))
            val_batch = next(iter(val_loader))
            
            # éªŒè¯æ•°æ®å½¢çŠ¶
            assert train_batch['gt'].shape == (self.batch_size, 1, self.img_size, self.img_size)
            assert train_batch['baseline'].shape == (self.batch_size, 1, self.img_size, self.img_size)
            assert val_batch['gt'].shape[0] <= self.batch_size  # å¯èƒ½å°äºbatch_size
            
            print(f"âœ“ æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡")
            print(f"  è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
            print(f"  éªŒè¯é›†å¤§å°: {len(val_dataset)}")
            print(f"  æ•°æ®å½¢çŠ¶: {train_batch['gt'].shape}")
            
            return True
            
        except Exception as e:
            print(f"âœ— æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_model_initialization(self, model_configs: Dict[str, Dict[str, Any]]) -> Dict[str, bool]:
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
        print("æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–...")
        
        results = {}
        
        for model_name, config in model_configs.items():
            try:
                model_config = config['model']
                
                if model_name == 'swin_unet':
                    model = SwinUNet(
                        in_channels=model_config['in_channels'],
                        out_channels=model_config['out_channels'],
                        img_size=model_config['img_size'],
                        patch_size=model_config['patch_size'],
                        window_size=model_config['window_size'],
                        depths=model_config['depths'],
                        num_heads=model_config['num_heads'],
                        embed_dim=model_config['embed_dim']
                    )
                elif model_name == 'hybrid':
                    model = HybridModel(
                        in_channels=model_config['in_channels'],
                        out_channels=model_config['out_channels'],
                        img_size=model_config['img_size'],
                        hidden_dim=model_config['hidden_dim'],
                        num_layers=model_config['num_layers']
                    )
                elif model_name == 'mlp':
                    model = MLPModel(
                        in_channels=model_config['in_channels'],
                        out_channels=model_config['out_channels'],
                        img_size=model_config['img_size'],
                        hidden_dim=model_config['hidden_dim'],
                        num_layers=model_config['num_layers']
                    )
                
                model = model.to(self.device)
                
                # æµ‹è¯•å‰å‘ä¼ æ’­
                dummy_input = torch.randn(
                    self.batch_size, 
                    model_config['in_channels'], 
                    self.img_size, 
                    self.img_size
                ).to(self.device)
                
                with torch.no_grad():
                    output = model(dummy_input)
                
                expected_shape = (
                    self.batch_size, 
                    model_config['out_channels'], 
                    self.img_size, 
                    self.img_size
                )
                
                assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {expected_shape}"
                
                # è®¡ç®—å‚æ•°é‡
                num_params = sum(p.numel() for p in model.parameters())
                
                print(f"âœ“ {model_name}æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
                print(f"  å‚æ•°é‡: {num_params:,}")
                print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
                
                results[model_name] = True
                
            except Exception as e:
                print(f"âœ— {model_name}æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
                results[model_name] = False
        
        return results
    
    def test_loss_computation(self, dataset_config: Dict[str, Any]) -> bool:
        """æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—"""
        print("æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—...")
        
        try:
            # åˆ›å»ºæ•°æ®é›†å’Œæ¨¡å‹
            dataset = DarcyFlowDataset(
                data_dir=dataset_config['data_dir'],
                split='train',
                img_size=dataset_config['img_size']
            )
            
            dataloader = torch.utils.data.DataLoader(
                dataset, batch_size=self.batch_size, shuffle=False
            )
            
            # åˆ›å»ºç®€å•æ¨¡å‹ç”¨äºæµ‹è¯•
            model = nn.Conv2d(1, 1, 3, padding=1).to(self.device)
            
            # åˆ›å»ºæŸå¤±å‡½æ•°
            criterion = CombinedLoss(
                rec_weight=1.0,
                spec_weight=0.5,
                dc_weight=1.0,
                img_size=self.img_size
            )
            
            # è·å–ä¸€ä¸ªbatch
            batch = next(iter(dataloader))
            baseline = batch['baseline'].to(self.device)
            gt = batch['gt'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            pred = model(baseline)
            
            # è®¡ç®—æŸå¤±
            loss_dict = criterion(pred, gt, baseline)
            
            # éªŒè¯æŸå¤±ç»„ä»¶
            required_keys = ['total_loss', 'rec_loss', 'spec_loss', 'dc_loss']
            for key in required_keys:
                assert key in loss_dict, f"ç¼ºå°‘æŸå¤±ç»„ä»¶: {key}"
                assert isinstance(loss_dict[key], torch.Tensor), f"{key}ä¸æ˜¯å¼ é‡"
                assert loss_dict[key].requires_grad, f"{key}ä¸éœ€è¦æ¢¯åº¦"
            
            print(f"âœ“ æŸå¤±å‡½æ•°è®¡ç®—æµ‹è¯•é€šè¿‡")
            print(f"  æ€»æŸå¤±: {loss_dict['total_loss'].item():.6f}")
            print(f"  é‡å»ºæŸå¤±: {loss_dict['rec_loss'].item():.6f}")
            print(f"  é¢‘è°±æŸå¤±: {loss_dict['spec_loss'].item():.6f}")
            print(f"  æ•°æ®ä¸€è‡´æ€§æŸå¤±: {loss_dict['dc_loss'].item():.6f}")
            
            return True
            
        except Exception as e:
            print(f"âœ— æŸå¤±å‡½æ•°è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_training_loop(self, model_configs: Dict[str, Dict[str, Any]], 
                          dataset_config: Dict[str, Any]) -> Dict[str, bool]:
        """æµ‹è¯•è®­ç»ƒå¾ªç¯"""
        print("æµ‹è¯•è®­ç»ƒå¾ªç¯...")
        
        results = {}
        
        for model_name in ['swin_unet']:  # åªæµ‹è¯•ä¸€ä¸ªæ¨¡å‹ä»¥èŠ‚çœæ—¶é—´
            try:
                config = model_configs[model_name]
                
                # åˆ›å»ºæ•°æ®é›†
                train_dataset = DarcyFlowDataset(
                    data_dir=dataset_config['data_dir'],
                    split='train',
                    img_size=dataset_config['img_size']
                )
                
                train_loader = torch.utils.data.DataLoader(
                    train_dataset, batch_size=self.batch_size, shuffle=True
                )
                
                # åˆ›å»ºæ¨¡å‹
                model_config = config['model']
                model = SwinUNet(
                    in_channels=model_config['in_channels'],
                    out_channels=model_config['out_channels'],
                    img_size=model_config['img_size'],
                    patch_size=model_config['patch_size'],
                    window_size=model_config['window_size'],
                    depths=model_config['depths'],
                    num_heads=model_config['num_heads'],
                    embed_dim=model_config['embed_dim']
                ).to(self.device)
                
                # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
                optimizer = torch.optim.AdamW(
                    model.parameters(),
                    lr=config['training']['lr'],
                    weight_decay=config['training']['weight_decay']
                )
                
                criterion = CombinedLoss(
                    rec_weight=config['loss']['rec_weight'],
                    spec_weight=config['loss']['spec_weight'],
                    dc_weight=config['loss']['dc_weight'],
                    img_size=self.img_size
                )
                
                # è®­ç»ƒå¾ªç¯
                model.train()
                epoch_losses = []
                
                for epoch in range(self.num_epochs):
                    epoch_loss = 0.0
                    num_batches = 0
                    
                    for batch in train_loader:
                        baseline = batch['baseline'].to(self.device)
                        gt = batch['gt'].to(self.device)
                        
                        # å‰å‘ä¼ æ’­
                        pred = model(baseline)
                        
                        # è®¡ç®—æŸå¤±
                        loss_dict = criterion(pred, gt, baseline)
                        loss = loss_dict['total_loss']
                        
                        # åå‘ä¼ æ’­
                        optimizer.zero_grad()
                        loss.backward()
                        
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(
                            model.parameters(), 
                            config['training']['grad_clip']
                        )
                        
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                        num_batches += 1
                    
                    avg_loss = epoch_loss / num_batches
                    epoch_losses.append(avg_loss)
                    
                    print(f"  Epoch {epoch+1}/{self.num_epochs}, Loss: {avg_loss:.6f}")
                
                # éªŒè¯æŸå¤±ä¸‹é™
                if len(epoch_losses) > 1:
                    loss_decreased = epoch_losses[-1] < epoch_losses[0]
                    if not loss_decreased:
                        print(f"  è­¦å‘Š: æŸå¤±æœªä¸‹é™ ({epoch_losses[0]:.6f} -> {epoch_losses[-1]:.6f})")
                
                print(f"âœ“ {model_name}è®­ç»ƒå¾ªç¯æµ‹è¯•é€šè¿‡")
                results[model_name] = True
                
            except Exception as e:
                print(f"âœ— {model_name}è®­ç»ƒå¾ªç¯æµ‹è¯•å¤±è´¥: {e}")
                results[model_name] = False
        
        return results
    
    def test_evaluation_metrics(self, dataset_config: Dict[str, Any]) -> bool:
        """æµ‹è¯•è¯„æµ‹æŒ‡æ ‡è®¡ç®—"""
        print("æµ‹è¯•è¯„æµ‹æŒ‡æ ‡è®¡ç®—...")
        
        try:
            # åˆ›å»ºæ•°æ®é›†
            dataset = DarcyFlowDataset(
                data_dir=dataset_config['data_dir'],
                split='test',
                img_size=dataset_config['img_size']
            )
            
            dataloader = torch.utils.data.DataLoader(
                dataset, batch_size=self.batch_size, shuffle=False
            )
            
            # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
            metrics_calculator = PDEBenchMetrics(
                img_size=self.img_size,
                boundary_width=8
            )
            
            # è·å–ä¸€ä¸ªbatch
            batch = next(iter(dataloader))
            baseline = batch['baseline']
            gt = batch['gt']
            
            # åˆ›å»ºæ¨¡æ‹Ÿé¢„æµ‹ï¼ˆæ·»åŠ å°‘é‡å™ªå£°ï¼‰
            pred = gt + 0.01 * torch.randn_like(gt)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = metrics_calculator.compute_all_metrics(pred, gt, baseline)
            
            # éªŒè¯æŒ‡æ ‡
            required_metrics = [
                'rel_l2', 'mae', 'psnr', 'ssim', 
                'dc_error', 'frmse_low', 'frmse_mid', 'frmse_high'
            ]
            
            for metric in required_metrics:
                assert metric in metrics, f"ç¼ºå°‘æŒ‡æ ‡: {metric}"
                assert isinstance(metrics[metric], (float, torch.Tensor)), f"{metric}ç±»å‹é”™è¯¯"
                if isinstance(metrics[metric], torch.Tensor):
                    assert not torch.isnan(metrics[metric]), f"{metric}ä¸ºNaN"
                else:
                    assert not np.isnan(metrics[metric]), f"{metric}ä¸ºNaN"
            
            print(f"âœ“ è¯„æµ‹æŒ‡æ ‡è®¡ç®—æµ‹è¯•é€šè¿‡")
            for metric, value in metrics.items():
                if isinstance(value, torch.Tensor):
                    value = value.item()
                print(f"  {metric}: {value:.6f}")
            
            return True
            
        except Exception as e:
            print(f"âœ— è¯„æµ‹æŒ‡æ ‡è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_visualization(self, dataset_config: Dict[str, Any]) -> bool:
        """æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½"""
        print("æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½...")
        
        try:
            # åˆ›å»ºæ•°æ®é›†
            dataset = DarcyFlowDataset(
                data_dir=dataset_config['data_dir'],
                split='test',
                img_size=dataset_config['img_size']
            )
            
            dataloader = torch.utils.data.DataLoader(
                dataset, batch_size=1, shuffle=False
            )
            
            # åˆ›å»ºå¯è§†åŒ–å™¨
            vis_dir = self.test_dir / 'outputs' / 'visualization'
            visualizer = PDEBenchVisualizer(str(vis_dir))
            
            # è·å–ä¸€ä¸ªæ ·æœ¬
            batch = next(iter(dataloader))
            baseline = batch['baseline'][0]  # [1, H, W]
            gt = batch['gt'][0]  # [1, H, W]
            
            # åˆ›å»ºæ¨¡æ‹Ÿé¢„æµ‹
            pred = gt + 0.05 * torch.randn_like(gt)
            
            # æµ‹è¯•å„ç§å¯è§†åŒ–åŠŸèƒ½
            vis_paths = {}
            
            # åœºå¯¹æ¯”å›¾
            vis_paths['field'] = visualizer.plot_field_comparison(
                gt, pred, baseline, "test_field_comparison"
            )
            
            # åŠŸç‡è°±å¯¹æ¯”
            vis_paths['spectrum'] = visualizer.plot_power_spectrum_comparison(
                gt, pred, "test_power_spectrum"
            )
            
            # è¾¹ç•Œåˆ†æ
            vis_paths['boundary'] = visualizer.plot_boundary_analysis(
                gt, pred, save_name="test_boundary_analysis"
            )
            
            # é¢‘åŸŸåˆ†æ
            vis_paths['frequency'] = visualizer.plot_frequency_band_analysis(
                gt, pred, save_name="test_frequency_analysis"
            )
            
            # éªŒè¯æ–‡ä»¶ç”Ÿæˆ
            for vis_type, path in vis_paths.items():
                assert os.path.exists(path), f"å¯è§†åŒ–æ–‡ä»¶æœªç”Ÿæˆ: {path}"
                print(f"  âœ“ {vis_type}å¯è§†åŒ–: {path}")
            
            print(f"âœ“ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âœ— å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_data_consistency(self, dataset_config: Dict[str, Any]) -> bool:
        """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§éªŒè¯"""
        print("æµ‹è¯•æ•°æ®ä¸€è‡´æ€§éªŒè¯...")
        
        try:
            # åˆ›å»ºä¸€è‡´æ€§æ£€æŸ¥å™¨é…ç½®
            checker_config = {
                'dataset': {
                    'name': 'darcy_flow',
                    'data_path': dataset_config['data_dir'],
                    'keys': ['solution'],
                    'normalize': True
                }
            }
            
            checker = DataConsistencyChecker(checker_config)
            
            # æ£€æŸ¥ä¸€è‡´æ€§
            results = checker.check_multiple_samples(
                num_samples=2,  # å°‘é‡æ ·æœ¬ç”¨äºå¿«é€Ÿæµ‹è¯•
                random_seed=42
            )
            
            # éªŒè¯ç»“æœ
            assert 'statistics' in results
            stats = results['statistics']
            
            print(f"âœ“ æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯•é€šè¿‡")
            print(f"  æ£€æŸ¥æ ·æœ¬æ•°: {stats['total_checked']}")
            print(f"  é€šè¿‡ç‡: {stats['pass_rate']:.2%}")
            print(f"  ä¸€è‡´æ€§æ£€æŸ¥: {'é€šè¿‡' if stats['pass_rate'] >= 0.95 else 'å¤±è´¥'}")
            
            return stats['pass_rate'] >= 0.95
            
        except Exception as e:
            print(f"âœ— æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_evaluator_integration(self, model_configs: Dict[str, Dict[str, Any]], 
                                 dataset_config: Dict[str, Any]) -> bool:
        """æµ‹è¯•è¯„æµ‹å™¨é›†æˆ"""
        print("æµ‹è¯•è¯„æµ‹å™¨é›†æˆ...")
        
        try:
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            eval_config = {
                'data': {
                    'dataset': 'darcy_flow',
                    'data_dir': dataset_config['data_dir'],
                    'split': 'test',
                    'batch_size': self.batch_size,
                    'img_size': self.img_size
                },
                'model': model_configs['swin_unet']['model'],
                'eval': {
                    'batch_size': self.batch_size,
                    'device': str(self.device),
                    'metrics': {
                        'img_size': self.img_size,
                        'boundary_width': 8
                    },
                    'visualization': {
                        'enabled': True,
                        'max_samples': 2
                    },
                    'output': {
                        'save_predictions': True,
                        'save_metrics': True,
                        'formats': ['json', 'csv']
                    }
                }
            }
            
            # ä¿å­˜é…ç½®
            config_path = self.test_dir / 'configs' / 'eval_test.yaml'
            with open(config_path, 'w') as f:
                yaml.dump(eval_config, f)
            
            # åˆ›å»ºè¯„æµ‹å™¨
            evaluator = Evaluator(str(config_path))
            
            # åˆ›å»ºè™šæ‹Ÿæ£€æŸ¥ç‚¹ï¼ˆéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ï¼‰
            checkpoint_dir = self.test_dir / 'runs' / 'test_model'
            checkpoint_dir.mkdir(parents=True, exist_ok=True)
            
            model = SwinUNet(**eval_config['model'])
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'epoch': 1,
                'config': eval_config
            }
            
            checkpoint_path = checkpoint_dir / 'best_model.pth'
            torch.save(checkpoint, checkpoint_path)
            
            # è¿è¡Œè¯„æµ‹
            results = evaluator.evaluate(str(checkpoint_path))
            
            # éªŒè¯ç»“æœ
            assert 'metrics' in results
            assert 'aggregated' in results['metrics']
            
            required_metrics = ['rel_l2', 'mae', 'psnr', 'ssim']
            for metric in required_metrics:
                assert metric in results['metrics']['aggregated'], f"ç¼ºå°‘èšåˆæŒ‡æ ‡: {metric}"
            
            print(f"âœ“ è¯„æµ‹å™¨é›†æˆæµ‹è¯•é€šè¿‡")
            print(f"  è¯„æµ‹æ ·æœ¬æ•°: {len(results['metrics']['per_sample'])}")
            print(f"  Rel-L2: {results['metrics']['aggregated']['rel_l2']:.6f}")
            
            return True
            
        except Exception as e:
            print(f"âœ— è¯„æµ‹å™¨é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def run_all_tests(self) -> Dict[str, bool]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("å¼€å§‹PDEBenchç¨€ç–è§‚æµ‹é‡å»ºç³»ç»Ÿç«¯åˆ°ç«¯æµ‹è¯•")
        print("=" * 60)
        
        results = {}
        
        try:
            # 1. è®¾ç½®æµ‹è¯•ç¯å¢ƒ
            dataset_config = self.setup_test_data()
            model_configs = self.setup_test_configs()
            
            # 2. æ•°æ®åŠ è½½æµ‹è¯•
            results['data_loading'] = self.test_data_loading(dataset_config)
            
            # 3. æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•
            model_init_results = self.test_model_initialization(model_configs)
            results.update({f'model_init_{k}': v for k, v in model_init_results.items()})
            
            # 4. æŸå¤±å‡½æ•°æµ‹è¯•
            results['loss_computation'] = self.test_loss_computation(dataset_config)
            
            # 5. è®­ç»ƒå¾ªç¯æµ‹è¯•
            training_results = self.test_training_loop(model_configs, dataset_config)
            results.update({f'training_{k}': v for k, v in training_results.items()})
            
            # 6. è¯„æµ‹æŒ‡æ ‡æµ‹è¯•
            results['evaluation_metrics'] = self.test_evaluation_metrics(dataset_config)
            
            # 7. å¯è§†åŒ–æµ‹è¯•
            results['visualization'] = self.test_visualization(dataset_config)
            
            # 8. æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
            results['data_consistency'] = self.test_data_consistency(dataset_config)
            
            # 9. è¯„æµ‹å™¨é›†æˆæµ‹è¯•
            results['evaluator_integration'] = self.test_evaluator_integration(
                model_configs, dataset_config
            )
            
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            results['overall'] = False
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "=" * 60)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 60)
        
        passed_tests = sum(results.values())
        total_tests = len(results)
        
        for test_name, passed in results.items():
            status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
            print(f"{test_name:30s} {status}")
        
        print("-" * 60)
        print(f"æ€»è®¡: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        overall_success = passed_tests == total_tests
        results['overall'] = overall_success
        
        if overall_success:
            print("ğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶ã€‚")
        
        return results
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)
            print(f"æ¸…ç†æµ‹è¯•ç›®å½•: {self.test_dir}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PDEBench E2Eæµ‹è¯•')
    parser.add_argument('--test-dir', type=str, help='æµ‹è¯•ç›®å½•')
    parser.add_argument('--keep-files', action='store_true', help='ä¿ç•™æµ‹è¯•æ–‡ä»¶')
    parser.add_argument('--device', type=str, default='auto', help='æµ‹è¯•è®¾å¤‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = E2ETestSuite(args.test_dir)
    
    # è®¾ç½®è®¾å¤‡
    if args.device != 'auto':
        test_suite.device = torch.device(args.device)
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = test_suite.run_all_tests()
        
        # ä¿å­˜ç»“æœ
        results_path = test_suite.test_dir / 'test_results.json'
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # è¿”å›é€€å‡ºç 
        exit_code = 0 if results.get('overall', False) else 1
        
    finally:
        # æ¸…ç†
        if not args.keep_files:
            test_suite.cleanup()
    
    return exit_code


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)